{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisite Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following pre-requisites are performed before the extraction of data from relevant systems.\n",
    "\n",
    "1. **Set Configuration:**\n",
    "    * Set a `CONFIG_ID` for execution.\n",
    "    * Code picks & loads configuration file (yaml) from `config` folder based on the `config_id` maintained in the file(s). \n",
    "2. **Create directories:**\n",
    "    * Pick the extraction directory maintained in configuration for Indicators\n",
    "    * Extraction data such as CSV / parquet files are staged in this directory.\n",
    "    * This data is stored locally.\n",
    "    * A `reports` directory is also created within, where the error reports are stored.\n",
    "3. **Create database tables:**\n",
    "    * The script creates necessary tables in the database specified in the configuration.\n",
    "    * Tables are only created if they **do not** already exist in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 18:37:33,280  [INFO]: ** EXTRACT - INDICATORS **\n",
      "\n",
      "2025-01-25 18:37:33,282  [INFO]: Configuration ID: dev\n",
      "2025-01-25 18:37:33,282  [INFO]: Extraction directory: ../migration-data/indicators/extract\n",
      "2025-01-25 18:37:33,282  [INFO]: Reports directory: ../migration-data/indicators/extract/reports\n",
      "2025-01-25 18:37:33,301  [INFO]: [DB] Tenant ID: dev\n",
      "2025-01-25 18:37:33,302  [INFO]: [DB] Database Connection: sqlite:///../migration-data/dev.db\n",
      "2025-01-25 18:37:33,302  [DEBU]: [DB] SQLAlchemy Echo: False\n",
      "2025-01-25 18:37:33,302  [INFO]: [DB] Drop Reload: True\n",
      "2025-01-25 18:37:33,346  [DEBU]: [DB] Table Created: T_PAI_EXTERNALDATA\n",
      "2025-01-25 18:37:33,346  [DEBU]: [DB] Table Created: T_PAI_EXTERNALDATA_FLOC\n",
      "2025-01-25 18:37:33,347  [DEBU]: [DB] Table Created: T_EIOT_MAPPING\n",
      "2025-01-25 18:37:33,347  [DEBU]: [DB] Table Created: T_EIOT_MAPPING_INDICATORS\n",
      "2025-01-25 18:37:33,347  [DEBU]: [DB] Table Created: T_EIOT_UPLOAD_STATUS\n",
      "2025-01-25 18:37:33,348  [DEBU]: [DB] Table Created: T_APM_INDICATOR_POSITIONS\n",
      "2025-01-25 18:37:33,348  [DEBU]: [DB] Table Created: T_PAI_EQU_HEADER\n",
      "2025-01-25 18:37:33,348  [DEBU]: [DB] Table Created: T_PAI_FLOC_HEADER\n",
      "2025-01-25 18:37:33,348  [DEBU]: [DB] Table Created: T_PAI_EQU_MODEL_TEMPLATES\n",
      "2025-01-25 18:37:33,349  [DEBU]: [DB] Table Created: T_PAI_FLOC_MODEL_TEMPLATES\n",
      "2025-01-25 18:37:33,349  [DEBU]: [DB] Table Created: T_PAI_EQU_TEMPLATE_HEADER\n",
      "2025-01-25 18:37:33,349  [DEBU]: [DB] Table Created: T_PAI_FLOC_TEMPLATE_HEADER\n",
      "2025-01-25 18:37:33,350  [DEBU]: [DB] Table Created: T_PAI_EQU_INDICATOR_GROUPS\n",
      "2025-01-25 18:37:33,350  [DEBU]: [DB] Table Created: T_PAI_FLOC_INDICATOR_GROUPS\n",
      "2025-01-25 18:37:33,350  [DEBU]: [DB] Table Created: T_PAI_EQU_INDICATORS\n",
      "2025-01-25 18:37:33,350  [DEBU]: [DB] Table Created: T_PAI_FLOC_INDICATORS\n",
      "2025-01-25 18:37:33,351  [DEBU]: [DB] Table Created: T_ERP_CHARACTERISTICS\n",
      "2025-01-25 18:37:33,351  [DEBU]: [DB] Table Created: T_PRE_LOAD_INDICATORS\n",
      "2025-01-25 18:37:33,351  [DEBU]: [DB] Table Created: T_LOAD_INDICATORS\n",
      "2025-01-25 18:37:33,351  [DEBU]: [DB] Table Created: T_POST_LOAD_INDICATORS\n",
      "2025-01-25 18:37:33,352  [DEBU]: [DB] Table Created: T_ALERT\n",
      "2025-01-25 18:37:33,352  [DEBU]: [DB] Table Created: T_POST_ALERTS\n",
      "2025-01-25 18:37:33,352  [DEBU]: [DB] Table Created: T_ALERTTYPE\n",
      "2025-01-25 18:37:33,352  [DEBU]: [DB] Table Created: T_POST_ALERTTYPES\n",
      "2025-01-25 18:37:33,353  [DEBU]: [DB] Table Created: T_PAI_ALERTS\n",
      "2025-01-25 18:37:33,353  [DEBU]: [DB] Table Created: T_PAI_ALERTTYPE\n",
      "2025-01-25 18:37:33,353  [DEBU]: [DB] Table Created: T_UDR_APM_INDICATORS\n",
      "2025-01-25 18:37:33,356  [DEBU]: [DB] View Created: V_PAI_EQU_EXTERNAL_DATA\n",
      "2025-01-25 18:37:33,359  [DEBU]: [DB] View Created: V_TRANSFORM_EQU_INDICATORS\n",
      "2025-01-25 18:37:33,362  [DEBU]: [DB] View Created: V_APM_INDICATOR_POSITIONS\n",
      "2025-01-25 18:37:33,364  [DEBU]: [DB] View Created: V_ERP_CHARACTERISTICS\n",
      "2025-01-25 18:37:33,366  [DEBU]: [DB] View Created: V_PAI_FLOC_EXTERNAL_DATA\n",
      "2025-01-25 18:37:33,368  [DEBU]: [DB] View Created: V_TRANSFORM_FLOC_INDICATORS\n",
      "2025-01-25 18:37:33,369  [DEBU]: [DB] View Created: V_TRANSFORM_INDICATORS\n",
      "2025-01-25 18:37:33,371  [DEBU]: [DB] View Created: V_ALERTTYPE_CHECK\n",
      "2025-01-25 18:37:33,373  [DEBU]: [DB] View Created: V_ALERTS_CHECK\n",
      "2025-01-25 18:37:33,375  [DEBU]: [DB] View Created: V_POST_LOAD_INDICATORS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Pre-requisite Configuration\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# For testing\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from modules.util.config import get_config_by_id\n",
    "from modules.util.database import SQLAlchemyClient\n",
    "from modules.util.helpers import Logger\n",
    "import os \n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "CONFIG_ID = 'dev'\n",
    "\n",
    "config_extract = get_config_by_id(CONFIG_ID).get(\"extract\")\n",
    "config_indicators = config_extract['indicator']\n",
    "EXTRACTION_DIR = config_indicators[\"directory\"]\n",
    "REPORTS_DIR = f\"{EXTRACTION_DIR}/reports\"\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Directories\n",
    "# ------------------------------------------------------------------------------ #\n",
    "if not os.path.exists(REPORTS_DIR):\n",
    "    os.makedirs(REPORTS_DIR)\n",
    "\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "log.info(\"** EXTRACT - INDICATORS **\")\n",
    "Logger.blank_line(log)\n",
    "log.info(f\"Configuration ID: {CONFIG_ID}\")\n",
    "log.info(f\"Extraction directory: {EXTRACTION_DIR}\")\n",
    "log.info(f\"Reports directory: {REPORTS_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Database\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "db = SQLAlchemyClient(CONFIG_ID)\n",
    "db.table_create_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Extract: Technical Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Technical Objects consists of Equipments & Functional Locations. For migration we consider only the technical objects which have a valid `templateId` & which are either in the `published` or `in revision` state.\n",
    "* Hence while extracting technical objects (EQUs / FLOCs) from the source system, we apply the following filters :\n",
    "    * `templateId is not blank`\n",
    "    * `status is not 1` (ignore the records with unpublished status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-01-25 18:37:33,461  [INFO]: Extract: Technical Objects [Equipments / Functional Locations]\n",
      "\n",
      "2025-01-25 18:37:34,906  [INFO]: [GET] Equipments: 1688\n",
      "2025-01-25 18:37:34,969  [INFO]: ../migration-data/indicators/extract/1_Equipments.parquet generated.\n",
      "2025-01-25 18:37:35,348  [INFO]: [GET] Functional Locations: 33\n",
      "2025-01-25 18:37:35,362  [INFO]: ../migration-data/indicators/extract/1_Functional_Locations.parquet generated.\n"
     ]
    }
   ],
   "source": [
    "# Technical Objets: Extraction [Equipments / Functional Locations]\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Standard Imports\n",
    "# ------------------------------------------------------------------------------ #\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Custom Imports\n",
    "# ------------------------------------------------------------------------------ #\n",
    "from modules.acf.equ_api import ApiEquipment\n",
    "from modules.acf.floc_api import ApiFloc\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Extraction\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "file_equ = rf'{EXTRACTION_DIR}/1_Equipments.parquet'\n",
    "file_floc = rf'{EXTRACTION_DIR}/1_Functional_Locations.parquet'\n",
    "filter = \"not(templateId eq 'null') and not(status eq '1')\"\n",
    "batch_size = 1000\n",
    "\n",
    "api_equ = ApiEquipment(CONFIG_ID)\n",
    "ApiFloc = ApiFloc(CONFIG_ID)\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: Technical Objects [Equipments / Functional Locations]\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "# Extract & generate data file for Equipments\n",
    "results = api_equ.get_equipments(\n",
    "    filter=filter,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "df_equ = pd.DataFrame(results)\n",
    "df_equ.rename(columns={'class': 'class_'}, inplace=True) #due to constraint in python & SQL\n",
    "\n",
    "# inclusion list for equipments\n",
    "equ_include = config_extract.get('filters').get('equipments_include')\n",
    "if equ_include:\n",
    "    log.warning(f\"[FILTER] Equipments - include: {equ_include}\")\n",
    "    df_equ = df_equ[df_equ['equipmentId'].isin(equ_include)]\n",
    "\n",
    "# exclusion list for equipments\n",
    "equ_exclude = config_extract.get('filters').get('equipments_exclude')\n",
    "if equ_exclude:\n",
    "    log.warning(f\"[FILTER] Equipments - exclude: {equ_exclude}\")\n",
    "    df_equ = df_equ[~df_equ['equipmentId'].isin(equ_exclude)]\n",
    "\n",
    "if equ_include or equ_exclude:\n",
    "    log.info(f\"Total Equipments: {len(df_equ)}\")\n",
    "\n",
    "df_equ.to_parquet(file_equ)\n",
    "log.info(f\"{file_equ} generated.\")\n",
    "\n",
    "# Extract & generate data file for Functional Locations\n",
    "results = ApiFloc.get_flocs(\n",
    "    filter=filter,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "df_floc = pd.DataFrame(results)\n",
    "df_floc.rename(columns={'class': 'class_'}, inplace=True)\n",
    "df_floc.to_parquet(file_floc)\n",
    "log.info(f\"{file_floc} generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data fetched above with the necessary filters are then staged in the corresponding tables in the database.\n",
    "\n",
    "| Data | Table Name |\n",
    "| ---- | ---------- |\n",
    "| Equipments | `T_PAI_EQU_HEADER`\n",
    "| Functional Locations | `T_PAI_FLOC_HEADER`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 18:37:35,375  [WARN]: [DB] TRUNCATE T_PAI_EQU_HEADER: 0\n",
      "2025-01-25 18:37:35,376  [WARN]: [DB] TRUNCATE T_PAI_FLOC_HEADER: 0\n",
      "2025-01-25 18:37:35,498  [INFO]: [DB] INSERT T_PAI_EQU_HEADER: 1688\n",
      "2025-01-25 18:37:35,555  [INFO]: [DB] INSERT T_PAI_FLOC_HEADER: 33\n"
     ]
    }
   ],
   "source": [
    "# Technical Objects: Stage Data in Tables\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "from modules.util.database import SQLAlchemyClient, EquipmentHeader, FlocHeader\n",
    "\n",
    "if db.drop_reload:\n",
    "    db.truncate(EquipmentHeader)\n",
    "    db.truncate(FlocHeader)\n",
    "\n",
    "equipments = SQLAlchemyClient.dataframe_to_object(df_equ, EquipmentHeader)\n",
    "flocs = SQLAlchemyClient.dataframe_to_object(df_floc, FlocHeader)\n",
    "\n",
    "if equipments:\n",
    "    db.insert_batches(data=equipments)\n",
    "\n",
    "if flocs:\n",
    "    db.insert_batches(data=flocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Extract: External Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* During migration, we consider only the Technical Objects which have an **external ID** assigned. \n",
    "* We call the `ExternalID` API to extract all the external IDs maintained in the system for Equipments & Functional Locations. \n",
    "* This data will then be used as a *check table* for migrating only the records which have a valid External ID maintained. \n",
    "* During the external ID extraction, we apply the following filters.\n",
    "    * `systemType is 'ERP'`\n",
    "    * `systemName is <sap-system-id>_<sap-client-number>`. The SAP system ID and client number are provided in the configuration\n",
    "    * `externalId does NOT contain LOCAL_`. *LOCAL_* are technical objects that are local to the source systems\n",
    "    * `objectType is 'EQU' or 'FLOC'`. EQU represents equipments & FLOC represents Functional Locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-01-25 18:37:35,594  [INFO]: Extract: External Data [Equipments / Functional Locations]\n",
      "\n",
      "2025-01-25 18:37:35,930  [INFO]: [GET] External Data systemType eq 'SAP ERP' and systemName eq 'QM7_910' and not substringof('LOCAL_', externalId) and objectType eq 'EQU': 3\n",
      "2025-01-25 18:37:35,940  [INFO]: ../migration-data/indicators/extract/2_External_Data_EQU.parquet generated.\n",
      "2025-01-25 18:37:36,058  [INFO]: [GET] External Data systemType eq 'SAP ERP' and systemName eq 'QM7_910' and not substringof('LOCAL_', externalId) and objectType eq 'FLOC': 1\n",
      "2025-01-25 18:37:36,067  [INFO]: ../migration-data/indicators/extract/2_External_Data_FLOC.parquet generated.\n"
     ]
    }
   ],
   "source": [
    "# External Data: Extraction [Equipments / Functional Locations]\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# Standard Imports\n",
    "import pandas as pd\n",
    "\n",
    "# Custom Imports\n",
    "from modules.acf.external_id_api import ApiExternalId\n",
    "\n",
    "\n",
    "file_equ = rf'{EXTRACTION_DIR}/2_External_Data_EQU.parquet'\n",
    "file_floc = rf'{EXTRACTION_DIR}/2_External_Data_FLOC.parquet'\n",
    "\n",
    "api = ApiExternalId(CONFIG_ID)\n",
    "filter = f\"systemType eq 'SAP ERP' and systemName eq '{api.erp_ssid}' and not substringof('LOCAL_', externalId)\"\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: External Data [Equipments / Functional Locations]\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "def extract_externalData(object_type, filter_base):\n",
    "    filter_str = f\"{filter_base} and objectType eq '{object_type}'\"\n",
    "    results = api.get_external_data(\n",
    "        filter_str=filter_str,\n",
    "        batch_size=20000\n",
    "    )\n",
    "    return results\n",
    "\n",
    "results_equ = extract_externalData('EQU', filter)\n",
    "df_equ_external = pd.json_normalize(results_equ)\n",
    "df_equ_external.to_parquet(file_equ)\n",
    "log.info(f\"{file_equ} generated.\")\n",
    "\n",
    "\n",
    "results_floc = extract_externalData('FLOC', filter)\n",
    "df_floc_external = pd.json_normalize(results_floc)\n",
    "df_floc_external.to_parquet(file_floc)\n",
    "log.info(f\"{file_floc} generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data fetched above with the necessary filters are then staged in the corresponding tables in the database.\n",
    "\n",
    "| Data | Table Name |\n",
    "| ---- | ---------- |\n",
    "| External Data for Equipments | `T_PAI_EXTERNALDATA`\n",
    "| External Data for Functional Locations | `T_PAI_EXTERNALDATA_FLOC`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 18:37:36,080  [WARN]: [DB] TRUNCATE T_PAI_EXTERNALDATA: 0\n",
      "2025-01-25 18:37:36,081  [WARN]: [DB] TRUNCATE T_PAI_EXTERNALDATA_FLOC: 0\n",
      "2025-01-25 18:37:36,082  [INFO]: [DB] INSERT T_PAI_EXTERNALDATA: 3\n",
      "2025-01-25 18:37:36,084  [INFO]: [DB] INSERT T_PAI_EXTERNALDATA_FLOC: 1\n"
     ]
    }
   ],
   "source": [
    "# External Data: Stage Data in Tables\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "from modules.util.database import ExternalData_EQU, ExternalData_FLOC\n",
    "\n",
    "if db.drop_reload:\n",
    "    db.truncate(ExternalData_EQU)\n",
    "    db.truncate(ExternalData_FLOC)\n",
    "\n",
    "external_equipments = SQLAlchemyClient.dataframe_to_object(df_equ_external, ExternalData_EQU)\n",
    "if external_equipments:\n",
    "    db.insert_batches(data=external_equipments)\n",
    "\n",
    "external_flocs = SQLAlchemyClient.dataframe_to_object(df_floc_external, ExternalData_FLOC)\n",
    "if external_flocs:       \n",
    "    db.insert_batches(data=external_flocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once the relevant data is staged, we build views that establish a relationship between the extracted technical objects with the external data that was extracted from the system.\n",
    "- These views help us to filter the data, viz. perform further steps only for the technical objects that have an external ID maintained.\n",
    "- The views join between the tables to establish the relationship.\n",
    "- They have a `valid` column present, which represents if the records are valid for processing or not.\n",
    "    - `valid` = `X` if the technical object has a external ID maintained.\n",
    "    - `valid` = `NULL` if the technical object does NOT have an external ID maintained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Extraction - Generic Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generic implementation to extract model information linked to technical objects. This method will be used in the upcoming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models: Extraction (Generic Method)\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# Standard Imports\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Custom Imports\n",
    "from modules.acf.model_api import ApiModel\n",
    "from modules.util.api import APIException\n",
    "\n",
    "def extract_models(results:list, modelColumn:str = 'modelId')->list:\n",
    "\n",
    "    \"\"\"\n",
    "    Extracts unique model information from the given models and fetches their details using an API.\n",
    "    Args:\n",
    "        results (list): A list of dictionaries containing model IDs.\n",
    "        modelColumn (str, optional): The column name in the results that contains the model IDs. Defaults to 'modelId'.\n",
    "    Returns:\n",
    "        list: A tuple containing two lists:\n",
    "            - response: A list of model details fetched from the API.\n",
    "            - error: A list of errors encountered during the API calls, each error is a dictionary with keys 'id', 'status_code', 'response', and 'endpoint'.\n",
    "    Raises:\n",
    "        APIException: If an error occurs during the API call.\n",
    "    Example:\n",
    "        >>> results = [{'modelId': '123'}, {'modelId': '456'}, {'modelId': '123'}]\n",
    "        >>> response, error = extract_models(results)\n",
    "        >>> print(response)\n",
    "        >>> print(error)\n",
    "    \"\"\"\n",
    "\n",
    "    df_models = pd.DataFrame(results)\n",
    "    df_models.drop_duplicates(subset=[modelColumn], inplace=True)\n",
    "\n",
    "    log.info(f'Unique models linked to TOs: {len(df_models)}')\n",
    "\n",
    "    api = ApiModel(CONFIG_ID)\n",
    "\n",
    "    response = []\n",
    "    error = []\n",
    "\n",
    "    log.info('[GET] Models based on Model ID')\n",
    "\n",
    "    def call_api(id):\n",
    "        return(api.get_model_model_id(id))\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        future_id = {executor.submit(call_api, id): id for id in df_models[modelColumn]}\n",
    "        for future in as_completed(future_id):\n",
    "            id = future_id[future]\n",
    "            try:\n",
    "                res = future.result()\n",
    "                if res:\n",
    "                    response.append(res)\n",
    "            except APIException as api_e:\n",
    "                log.error(f\"API Exception: {id} - {api_e.status_code} - {api_e.response} - {api_e.endpoint}\")\n",
    "                err = {\n",
    "                    'id': id,\n",
    "                    'status_code': api_e.status_code,\n",
    "                    'response':api_e.response,\n",
    "                    'endpoint':api_e.endpoint\n",
    "                }\n",
    "                error.append(err)\n",
    "    log.info(f'Model details fetched: {len(response)}')\n",
    "    if error:\n",
    "        log.error(f'Models (with errors): {len(error)}')\n",
    "    return response, error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Extract Model Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we extract model details for the technical objects (Equipments and Functional Locations) that have been identified as valid for processing. The extraction process involves the following steps:\n",
    "\n",
    "1. **Fetch Model IDs:**\n",
    "    - We fetch unique model IDs linked to the technical objects from the database views `V_PAI_EquipmentExternalData` and `V_PAI_FlocExternalData`.\n",
    "\n",
    "2. **Extract Model Details:**\n",
    "    - Using the fetched model IDs, we call the API to get detailed information about each model.\n",
    "    - The extracted model details are normalized and stored in parquet files for further processing.\n",
    "\n",
    "3. **Handle Errors:**\n",
    "    - Any errors encountered during the API calls are logged and stored in CSV files for review.\n",
    "\n",
    "4. **Stage Data in Tables:**\n",
    "    - The extracted model details are then staged in the corresponding tables in the database.\n",
    "\n",
    "The following table summarizes the data extraction and staging process:\n",
    "\n",
    "| Data | Table Name |\n",
    "| ---- | ---------- |\n",
    "| Model Header for Equipments | `T_PAI_EQU_MODEL_TEMPLATES` |\n",
    "| Model Header for Functional Locations | `T_PAI_FLOC_MODEL_TEMPLATES` |\n",
    "\n",
    "The extracted data is crucial for the subsequent steps, where we will extract and process templates, indicator groups, and indicators linked to these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-01-25 18:37:36,116  [INFO]: Extract: Model Header for Equipments\n",
      "\n",
      "2025-01-25 18:37:36,118  [DEBU]: SELECT DISTINCT \"V_PAI_EQU_EXTERNAL_DATA\".\"modelId\" \n",
      "FROM \"V_PAI_EQU_EXTERNAL_DATA\" \n",
      "WHERE \"V_PAI_EQU_EXTERNAL_DATA\".tenantid = :tenantid_1 AND \"V_PAI_EQU_EXTERNAL_DATA\".valid = :valid_1 ORDER BY \"V_PAI_EQU_EXTERNAL_DATA\".\"modelId\"\n",
      "2025-01-25 18:37:36,131  [INFO]: [DB] SELECT V_PAI_EQU_EXTERNAL_DATA: 3\n",
      "2025-01-25 18:37:36,138  [INFO]: Unique models linked to TOs: 3\n",
      "2025-01-25 18:37:36,166  [INFO]: [GET] Models based on Model ID\n",
      "2025-01-25 18:37:36,588  [INFO]: Model details fetched: 3\n",
      "2025-01-25 18:37:36,607  [INFO]: ../migration-data/indicators/extract/3_Model_Header_EQU.parquet generated.\n",
      "2025-01-25 18:37:36,608  [WARN]: [DB] TRUNCATE T_PAI_EQU_MODEL_TEMPLATES: 0\n",
      "2025-01-25 18:37:36,609  [INFO]: [DB] INSERT T_PAI_EQU_MODEL_TEMPLATES: 4\n",
      "\n",
      "2025-01-25 18:37:36,612  [INFO]: Extract: Model Header for Functional Locations\n",
      "\n",
      "2025-01-25 18:37:36,613  [DEBU]: SELECT DISTINCT \"V_PAI_FLOC_EXTERNAL_DATA\".\"modelId\" \n",
      "FROM \"V_PAI_FLOC_EXTERNAL_DATA\" \n",
      "WHERE \"V_PAI_FLOC_EXTERNAL_DATA\".tenantid = :tenantid_1 AND \"V_PAI_FLOC_EXTERNAL_DATA\".valid = :valid_1 ORDER BY \"V_PAI_FLOC_EXTERNAL_DATA\".\"modelId\"\n",
      "2025-01-25 18:37:36,614  [INFO]: [DB] SELECT V_PAI_FLOC_EXTERNAL_DATA: 1\n",
      "2025-01-25 18:37:36,615  [INFO]: Unique models linked to TOs: 1\n",
      "2025-01-25 18:37:36,630  [INFO]: [GET] Models based on Model ID\n",
      "2025-01-25 18:37:36,974  [INFO]: Model details fetched: 1\n",
      "2025-01-25 18:37:36,995  [INFO]: ../migration-data/indicators/extract/3_Model_Header_FLOC.parquet generated.\n",
      "2025-01-25 18:37:36,996  [WARN]: [DB] TRUNCATE T_PAI_FLOC_MODEL_TEMPLATES: 0\n",
      "2025-01-25 18:37:36,997  [INFO]: [DB] INSERT T_PAI_FLOC_MODEL_TEMPLATES: 1\n"
     ]
    }
   ],
   "source": [
    "# Models: Extraction\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# Standard Imports\n",
    "import pandas as pd\n",
    "\n",
    "# Custom Imports\n",
    "from modules.util.database import V_PAI_EquipmentExternalData, V_PAI_FlocExternalData,EquModelTemplates, FlocModelTemplates, SQLAlchemyClient\n",
    "from modules.util.helpers import convert_dataframe, explode_normalize\n",
    "\n",
    "file_equ_model = rf'{EXTRACTION_DIR}/3_Model_Header_EQU.parquet'\n",
    "file_equ_err = rf'{REPORTS_DIR}/3_Model_EQU_errors.csv'\n",
    "\n",
    "file_floc_model = rf'{EXTRACTION_DIR}/3_Model_Header_FLOC.parquet'\n",
    "file_floc_err = rf'{REPORTS_DIR}/3_Model_FLOC_errors.csv'\n",
    "\n",
    "def process_data(model, table, success_file, error_file):\n",
    "    results = db.select(\n",
    "        model=model,\n",
    "        fields=[\"modelId\"],\n",
    "        distinct=True,\n",
    "        where=[model.valid == \"X\"],\n",
    "        orderby=['modelId']\n",
    "    )\n",
    "\n",
    "    if len(results) > 0:\n",
    "        response,error = extract_models(results)\n",
    "        df_model_normal = pd.json_normalize(response, sep=\"_\")\n",
    "\n",
    "        df_model_explode = explode_normalize(\n",
    "        data=df_model_normal,\n",
    "        id=[col for col in df_model_normal.columns if col != 'templates'],\n",
    "        id_explode='templates')\n",
    "\n",
    "        df_model_explode = convert_dataframe(df_model_explode)\n",
    "        df_model_explode.rename(columns={'class': '_class'}, inplace=True)\n",
    "        \n",
    "        df_model_explode.to_parquet(success_file)\n",
    "        log.info(f\"{success_file} generated.\")\n",
    "        \n",
    "        if db.drop_reload is True:\n",
    "            db.truncate(table)\n",
    "        \n",
    "        models = SQLAlchemyClient.dataframe_to_object(df_model_explode, table)\n",
    "        if models:\n",
    "            db.insert_batches(data=models)\n",
    "\n",
    "        if error:\n",
    "            df_error = pd.DataFrame(error)\n",
    "            df_error.to_csv(error_file, index=False)\n",
    "            log.error(f\"{error_file} generated.\")\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: Model Header for Equipments\")\n",
    "Logger.blank_line(log)\n",
    "process_data(V_PAI_EquipmentExternalData, EquModelTemplates, file_equ_model, file_equ_err)\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: Model Header for Functional Locations\")\n",
    "Logger.blank_line(log)\n",
    "process_data(V_PAI_FlocExternalData, FlocModelTemplates, file_floc_model, file_floc_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Extraction - Generic Method\n",
    "\n",
    "A generic implementation to extract template information linked to technical objects. This method will be used in the upcoming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates: Extraction (Generic Method)\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# Standard Imports\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Custom Imports\n",
    "from modules.acf.template_api import ApiTemplate\n",
    "from modules.util.api import APIException\n",
    "\n",
    "def extract_templates(results:list, templateColumn:str='templateId') -> list:\n",
    "    \"\"\"\n",
    "    Extracts unique templates from the provided results and fetches detailed data for each template using an API.\n",
    "    Args:\n",
    "        results (list): A list of dictionaries containing template data (it should contain an entry with 'templateId' field).\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - response (list): A list of dictionaries with detailed template data.\n",
    "            - error (list): A list of dictionaries with error information for templates that failed to fetch.\n",
    "    Raises:\n",
    "        APIException: If there is an issue with the API call.\n",
    "        Exception: For any other exceptions that occur during the API call.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_template = pd.DataFrame(results)\n",
    "    df_template.drop_duplicates(subset=[templateColumn], inplace=True)\n",
    "\n",
    "    log.info(f'Unique Templates linked to TOs: {len(df_template)}')\n",
    "\n",
    "    api = ApiTemplate(CONFIG_ID)\n",
    "\n",
    "    response = []\n",
    "    error = []\n",
    "\n",
    "    log.info('[GET] Templates based on Template ID')\n",
    "\n",
    "    def call_api(id):\n",
    "        return(api.get_template_template_id(id))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        future_id = {executor.submit(call_api, id): id for id in df_template[templateColumn]}\n",
    "        for future in as_completed(future_id):\n",
    "            id = future_id[future]\n",
    "            try:\n",
    "                res = future.result()\n",
    "                if res:\n",
    "                    #since there can be more than one responses for a templateId, we need to loop through all responses\n",
    "                    for data in res:\n",
    "                        template_with_id = {templateColumn: id, **data}\n",
    "                        response.append(template_with_id)\n",
    "            except APIException as api_e:\n",
    "                log.error(f\"API Exception: {id} - {api_e.status_code} - {api_e.response} - {api_e.endpoint}\")\n",
    "                err = {\n",
    "                    'id': id,\n",
    "                    'status_code': api_e.status_code,\n",
    "                    'response':api_e.response,\n",
    "                    'endpoint':api_e.endpoint\n",
    "                }\n",
    "                error.append(err)\n",
    "    log.info(f'Templates fetched: {len(response)}')\n",
    "    if error:\n",
    "        log.error(f'Templates(with errors): {len(error)}')\n",
    "    return response, error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - EQU Templates (Indicator Groups & Indicators)\n",
    "\n",
    "To extract template information for Equipments, we follow these steps:\n",
    "\n",
    "1. **Fetch Template Data:**\n",
    "    - We fetch template data from the view `V_PAI_EquipmentExternalData` for Equipments that are valid for processing.\n",
    "\n",
    "2. **Extract Templates:**\n",
    "    - Using the fetched template data, we call the API to get detailed information about each template.\n",
    "    - The extracted template details are normalized and stored in parquet files for further processing.\n",
    "\n",
    "3. **Handle Errors:**\n",
    "    - Any errors encountered during the API calls are logged and stored in CSV files for review.\n",
    "\n",
    "4. **Stage Data in Tables:**\n",
    "    - The extracted template details are then staged in the corresponding tables in the database.\n",
    "\n",
    "5. **Extract Indicator Groups and Indicators:**\n",
    "    - For each template, we extract the associated indicator groups and indicators.\n",
    "    - The extracted data is normalized and stored in parquet files.\n",
    "    - The data is then staged in the corresponding tables in the database.\n",
    "\n",
    "The following table summarizes the data extraction and staging process:\n",
    "\n",
    "| Data | Table Name |\n",
    "| ---- | ---------- |\n",
    "| Template Header for Equipments | `T_PAI_EQU_TEMPLATE_HEADER` |\n",
    "| Indicator Groups for Equipments | `T_PAI_EQU_INDICATOR_GROUPS` |\n",
    "| Indicators for Equipments | `T_PAI_EQU_INDICATORS` |\n",
    "\n",
    "The extracted data is crucial for the subsequent steps, where we will process the indicators linked to these templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-01-25 18:37:37,028  [INFO]: Extract: Templates - Equipments [Indicator Groups > Indicators]\n",
      "\n",
      "2025-01-25 18:37:37,030  [DEBU]: SELECT DISTINCT \"V_PAI_EQU_EXTERNAL_DATA\".id, \"V_PAI_EQU_EXTERNAL_DATA\".\"externalId\", \"V_PAI_EQU_EXTERNAL_DATA\".\"templateId\", \"V_PAI_EQU_EXTERNAL_DATA\".valid \n",
      "FROM \"V_PAI_EQU_EXTERNAL_DATA\" \n",
      "WHERE \"V_PAI_EQU_EXTERNAL_DATA\".tenantid = :tenantid_1 AND \"V_PAI_EQU_EXTERNAL_DATA\".valid = :valid_1 ORDER BY \"V_PAI_EQU_EXTERNAL_DATA\".\"externalId\"\n",
      "2025-01-25 18:37:37,032  [INFO]: [DB] SELECT V_PAI_EQU_EXTERNAL_DATA: 4\n",
      "2025-01-25 18:37:37,033  [INFO]: Unique Templates linked to TOs: 3\n",
      "2025-01-25 18:37:37,048  [INFO]: [GET] Templates based on Template ID\n",
      "2025-01-25 18:37:37,469  [DEBU]: [GET] Template for Template ID 05D58DF9673046B487D4105DBE816F5B\n",
      "2025-01-25 18:37:37,490  [DEBU]: [GET] Template for Template ID 9D72A836372246B3B535C5F1EB57AB31\n",
      "2025-01-25 18:37:37,520  [DEBU]: [GET] Template for Template ID 98A93BA307024D0E883289396850EECC\n",
      "2025-01-25 18:37:37,521  [INFO]: Templates fetched: 3\n",
      "2025-01-25 18:37:37,531  [INFO]: ../migration-data/indicators/extract/3_Template_EQU.parquet generated\n",
      "2025-01-25 18:37:37,533  [WARN]: [DB] TRUNCATE T_PAI_EQU_TEMPLATE_HEADER: 0\n",
      "2025-01-25 18:37:37,533  [INFO]: [DB] INSERT T_PAI_EQU_TEMPLATE_HEADER: 3\n"
     ]
    }
   ],
   "source": [
    "# [EQU] Templates: Extraction\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# Standard Imports\n",
    "import pandas as pd\n",
    "\n",
    "# Custom Imports\n",
    "from modules.util.database import SQLAlchemyClient, EquTemplateHeader, V_PAI_EquipmentExternalData\n",
    "# from modules.util.helpers import explode_normalize, convert_dataframe\n",
    "\n",
    "file_tgt = rf'{EXTRACTION_DIR}/3_Template_EQU.parquet'\n",
    "file_err = rf'{REPORTS_DIR}/3_Template_EQU_Errors.csv'\n",
    "df_template_normal = pd.DataFrame()\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: Templates - Equipments [Indicator Groups > Indicators]\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "# fetch template data from the view for external data - equipments\n",
    "results = db.select(\n",
    "    model=V_PAI_EquipmentExternalData,\n",
    "    fields=['id', 'externalId', 'templateId', 'valid'],\n",
    "    distinct=True,\n",
    "    where=[V_PAI_EquipmentExternalData.valid == \"X\"],\n",
    "    orderby=['externalId']\n",
    ")\n",
    "\n",
    "if len(results) > 0:\n",
    "\n",
    "    # extract templates for the identified records\n",
    "    response,error = extract_templates(results)\n",
    "\n",
    "    df_template_normal = pd.json_normalize(response, sep='_')\n",
    "    df_template_normal = convert_dataframe(df_template_normal)\n",
    "    df_template_normal.to_parquet(file_tgt, index=False)\n",
    "    log.info(f\"{file_tgt} generated\")\n",
    "\n",
    "    if error is not None and error != []:\n",
    "        df_err = pd.json_normalize(error)\n",
    "        df_err.to_csv(file_err, index=False)\n",
    "        log.error(f\"{file_err} generated\")\n",
    "\n",
    "    if db.drop_reload:\n",
    "        db.truncate(EquTemplateHeader)  # truncate all existing data (remove if needed)\n",
    "    templates = SQLAlchemyClient.dataframe_to_object(df_template_normal, EquTemplateHeader)\n",
    "    if templates:\n",
    "        db.insert_batches(data=templates)  # insert the new templates\n",
    "    else:\n",
    "        log.warning(\"No valid equipments found for template extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 18:37:37,549  [INFO]: Extract: indicator groups & indicators for templates: 3\n",
      "2025-01-25 18:37:37,551  [INFO]: Records with indicator groups: 3\n",
      "2025-01-25 18:37:37,561  [INFO]: Extract: Indicator Groups for templates: 5\n",
      "2025-01-25 18:37:37,567  [INFO]: ../migration-data/indicators/extract/3_Template_EQU_Indicator_Groups.parquet generated.\n",
      "2025-01-25 18:37:37,568  [INFO]: Records with indicators: 5\n",
      "2025-01-25 18:37:37,579  [INFO]: EXTRACT: Indicators from Indicator Groups: 22\n",
      "2025-01-25 18:37:37,591  [INFO]: ../migration-data/indicators/extract/3_Template_EQU_Indicators.parquet generated.\n",
      "2025-01-25 18:37:37,592  [WARN]: [DB] TRUNCATE T_PAI_EQU_INDICATOR_GROUPS: 0\n",
      "2025-01-25 18:37:37,593  [WARN]: [DB] TRUNCATE T_PAI_EQU_INDICATORS: 0\n",
      "2025-01-25 18:37:37,594  [INFO]: [DB] INSERT T_PAI_EQU_INDICATOR_GROUPS: 5\n",
      "2025-01-25 18:37:37,599  [INFO]: [DB] INSERT T_PAI_EQU_INDICATORS: 22\n"
     ]
    }
   ],
   "source": [
    "# [EQU] Indicators & Indicator Groups Extraction\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# Custom Imports\n",
    "from modules.util.database import SQLAlchemyClient,EquIndicatorGroups, EquIndicators\n",
    "\n",
    "if len(df_template_normal) > 0:\n",
    "    file_ind_groups = rf'{EXTRACTION_DIR}/3_Template_EQU_Indicator_Groups.parquet'\n",
    "    file_ind = rf'{EXTRACTION_DIR}/3_Template_EQU_Indicators.parquet'\n",
    "\n",
    "    log.info(f'Extract: indicator groups & indicators for templates: {len(df_template_normal)}')\n",
    "\n",
    "    ######### indicator groups\n",
    "    cols = ['templateId', 'id', 'indicatorGroups']\n",
    "    df_ind_groups = df_template_normal[cols].copy()\n",
    "    df_ind_groups.dropna(subset=['indicatorGroups'], inplace=True)\n",
    "    df_ind_groups.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    log.info(f\"Records with indicator groups: {len(df_ind_groups)}\")\n",
    "\n",
    "    # Ensure indicatorGroups is a list of dictionaries\n",
    "    df_ind_groups['indicatorGroups'] = df_ind_groups['indicatorGroups'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    df_ind_groups_explode = explode_normalize(\n",
    "        data=df_ind_groups,\n",
    "        id_explode='indicatorGroups',\n",
    "        id=['templateId', 'id']\n",
    "    )\n",
    "\n",
    "    df_ind_groups_explode = convert_dataframe(df_ind_groups_explode)\n",
    "    log.info(f\"Extract: Indicator Groups for templates: {len(df_ind_groups_explode)}\")\n",
    "    \n",
    "    df_ind_groups_explode.to_parquet(file_ind_groups)\n",
    "    log.info(f\"{file_ind_groups} generated.\")\n",
    "\n",
    "\n",
    "    ######### indicators\n",
    "\n",
    "    cols = ['templateId','id','indicatorGroups_id','indicatorGroups_internalId','indicatorGroups_indicators']\n",
    "    df_indicators = df_ind_groups_explode[cols].copy()\n",
    "    df_indicators.rename(columns={'indicatorGroups_indicators':'indicators'}, inplace=True)\n",
    "    df_indicators.dropna(subset=['indicators'], inplace=True)\n",
    "    df_indicators.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    log.info(f\"Records with indicators: {len(df_indicators)}\")\n",
    "\n",
    "    df_indicators['indicators'] = df_indicators['indicators'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    df_indicators_explode = explode_normalize(\n",
    "        data=df_indicators,\n",
    "        id_explode='indicators',\n",
    "        id=['templateId', 'id', 'indicatorGroups_id', 'indicatorGroups_internalId']\n",
    "    )\n",
    "    df_indicators_explode = convert_dataframe(df_indicators_explode)\n",
    "    log.info(f\"EXTRACT: Indicators from Indicator Groups: {len(df_indicators_explode)}\")\n",
    "    \n",
    "    df_indicators_explode.to_parquet(file_ind)\n",
    "    log.info(f\"{file_ind} generated.\")\n",
    "\n",
    "    if db.drop_reload:\n",
    "        db.truncate(EquIndicatorGroups)  # truncate all existing data (remove if needed)\n",
    "        db.truncate(EquIndicators)  # truncate all existing data (remove if needed)\n",
    "\n",
    "    indicator_groups = SQLAlchemyClient.dataframe_to_object(df_ind_groups_explode, EquIndicatorGroups)\n",
    "    if indicator_groups:\n",
    "        db.insert_batches(data=indicator_groups)  # insert the new indicator groups\n",
    "\n",
    "    indicators = SQLAlchemyClient.dataframe_to_object(df_indicators_explode, EquIndicators)\n",
    "    if indicators:\n",
    "        db.insert_batches(data=indicators)  # insert the new indicators\n",
    "else:\n",
    "    log.warning(\"No templates found for indicator extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - FLOC Templates (Indicator Groups & Indicators)\n",
    "\n",
    "To extract template information for Functional Locations, we follow these steps:\n",
    "\n",
    "1. **Fetch Template Data:**\n",
    "    - We fetch template data from the view `V_PAI_FlocExternalData` for Functional Locations that are valid for processing.\n",
    "\n",
    "2. **Extract Templates:**\n",
    "    - Using the fetched template data, we call the API to get detailed information about each template.\n",
    "    - The extracted template details are normalized and stored in parquet files for further processing.\n",
    "\n",
    "3. **Handle Errors:**\n",
    "    - Any errors encountered during the API calls are logged and stored in CSV files for review.\n",
    "\n",
    "4. **Stage Data in Tables:**\n",
    "    - The extracted template details are then staged in the corresponding tables in the database.\n",
    "\n",
    "5. **Extract Indicator Groups and Indicators:**\n",
    "    - For each template, we extract the associated indicator groups and indicators.\n",
    "    - The extracted data is normalized and stored in parquet files.\n",
    "    - The data is then staged in the corresponding tables in the database.\n",
    "\n",
    "The following table summarizes the data extraction and staging process:\n",
    "\n",
    "| Data | Table Name |\n",
    "| ---- | ---------- |\n",
    "| Template Header for Functional Locations | `T_PAI_FLOC_TEMPLATE_HEADER` |\n",
    "| Indicator Groups for Functional Locations | `T_PAI_FLOC_INDICATOR_GROUPS` |\n",
    "| Indicators for Functional Locations | `T_PAI_FLOC_INDICATORS` |\n",
    "\n",
    "The extracted data is crucial for the subsequent steps, where we will process the indicators linked to these templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-01-25 18:37:37,618  [INFO]: Extract: Templates - Functional Locations [Indicator Groups > Indicators]\n",
      "\n",
      "2025-01-25 18:37:37,619  [DEBU]: SELECT DISTINCT \"V_PAI_FLOC_EXTERNAL_DATA\".id, \"V_PAI_FLOC_EXTERNAL_DATA\".\"externalId\", \"V_PAI_FLOC_EXTERNAL_DATA\".\"templateId\", \"V_PAI_FLOC_EXTERNAL_DATA\".valid \n",
      "FROM \"V_PAI_FLOC_EXTERNAL_DATA\" \n",
      "WHERE \"V_PAI_FLOC_EXTERNAL_DATA\".tenantid = :tenantid_1 AND \"V_PAI_FLOC_EXTERNAL_DATA\".valid = :valid_1 ORDER BY \"V_PAI_FLOC_EXTERNAL_DATA\".\"externalId\"\n",
      "2025-01-25 18:37:37,620  [INFO]: [DB] SELECT V_PAI_FLOC_EXTERNAL_DATA: 1\n",
      "2025-01-25 18:37:37,621  [INFO]: Unique Templates linked to TOs: 1\n",
      "2025-01-25 18:37:37,636  [INFO]: [GET] Templates based on Template ID\n",
      "2025-01-25 18:37:37,911  [DEBU]: [GET] Template for Template ID 9D72A836372246B3B535C5F1EB57AB31\n",
      "2025-01-25 18:37:37,912  [INFO]: Templates fetched: 1\n",
      "2025-01-25 18:37:37,922  [INFO]: ../migration-data/indicators/extract/4_Template_FLOC.parquet generated\n",
      "2025-01-25 18:37:37,923  [WARN]: [DB] TRUNCATE T_PAI_FLOC_TEMPLATE_HEADER: 0\n",
      "2025-01-25 18:37:37,923  [INFO]: [DB] INSERT T_PAI_FLOC_TEMPLATE_HEADER: 1\n"
     ]
    }
   ],
   "source": [
    "# [FLOC] Templates: Extraction\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# Standard Imports\n",
    "import pandas as pd\n",
    "\n",
    "# Custom Imports\n",
    "from modules.util.database import SQLAlchemyClient, V_PAI_FlocExternalData, FlocTemplateHeader\n",
    "from modules.util.helpers import convert_dataframe\n",
    "\n",
    "file_tgt = rf'{EXTRACTION_DIR}/4_Template_FLOC.parquet'\n",
    "file_err = rf'{REPORTS_DIR}/4_Template_FLOC_Errors.csv'\n",
    "\n",
    "df_template_normal = pd.DataFrame()\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: Templates - Functional Locations [Indicator Groups > Indicators]\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "# fetch template data from the view for external data - functional locations\n",
    "\n",
    "results = db.select(\n",
    "    model=V_PAI_FlocExternalData,\n",
    "    fields=['id', 'externalId', 'templateId', 'valid'],\n",
    "    distinct=True,\n",
    "    where=[V_PAI_FlocExternalData.valid == \"X\"],\n",
    "    orderby=['externalId']\n",
    ")\n",
    "\n",
    "if len(results) > 0:\n",
    "\n",
    "    # extract templates for the identified records\n",
    "    response, error = extract_templates(results, templateColumn='templateId')\n",
    "\n",
    "    df_template_normal = pd.json_normalize(response, sep='_')\n",
    "    df_template_normal = convert_dataframe(df_template_normal)\n",
    "    df_template_normal.rename(columns={'id': 'flocId'}, inplace=True)\n",
    "    df_template_normal.to_parquet(file_tgt, index=False)\n",
    "    log.info(f\"{file_tgt} generated\")\n",
    "\n",
    "    if error is not None and error != []:\n",
    "        df_err = pd.json_normalize(error)\n",
    "        df_err.to_csv(file_err, index=False)\n",
    "        log.error(f\"{file_err} generated\")\n",
    "\n",
    "    if db.drop_reload:\n",
    "        db.truncate(FlocTemplateHeader)  # truncate all existing data (remove if needed)\n",
    "        \n",
    "    templates = SQLAlchemyClient.dataframe_to_object(df_template_normal, FlocTemplateHeader)\n",
    "    if templates:\n",
    "        db.insert_batches(data=templates)  # insert the new templates\n",
    "else:\n",
    "    log.warning(f\"No valid functional locations found for template extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 18:37:37,938  [INFO]: Extract: indicator groups & indicators for templates: 1\n",
      "2025-01-25 18:37:37,940  [INFO]: Records with indicatorGroups: 1\n",
      "2025-01-25 18:37:37,946  [INFO]: Extract: Indicator Groups for templates: 1\n",
      "2025-01-25 18:37:37,951  [INFO]: ../migration-data/indicators/extract/4_Template_FLOC_Indicator_Groups.parquet generated.\n",
      "2025-01-25 18:37:37,952  [INFO]: Records with indicators: 1\n",
      "2025-01-25 18:37:37,959  [INFO]: EXTRACT: Indicators from Indicator Groups: 8\n",
      "2025-01-25 18:37:37,967  [INFO]: ../migration-data/indicators/extract/4_Template_FLOC_Indicators.parquet generated.\n",
      "2025-01-25 18:37:37,968  [WARN]: [DB] TRUNCATE T_PAI_FLOC_INDICATOR_GROUPS: 0\n",
      "2025-01-25 18:37:37,969  [WARN]: [DB] TRUNCATE T_PAI_FLOC_INDICATORS: 0\n",
      "2025-01-25 18:37:37,970  [INFO]: [DB] INSERT T_PAI_FLOC_INDICATOR_GROUPS: 1\n",
      "2025-01-25 18:37:37,972  [INFO]: [DB] INSERT T_PAI_FLOC_INDICATORS: 8\n"
     ]
    }
   ],
   "source": [
    "# [FLOC] Indicators & Indicator Groups Extraction\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# Custom Imports\n",
    "from modules.util.database import SQLAlchemyClient, FlocIndicatorGroups, FlocIndicators\n",
    "\n",
    "if len(df_template_normal) > 0:\n",
    "\n",
    "    file_ind_groups_floc = rf'{EXTRACTION_DIR}/4_Template_FLOC_Indicator_Groups.parquet'\n",
    "    file_ind_floc = rf'{EXTRACTION_DIR}/4_Template_FLOC_Indicators.parquet'\n",
    "\n",
    "    log.info(f'Extract: indicator groups & indicators for templates: {len(df_template_normal)}')\n",
    "\n",
    "    ######### indicator groups\n",
    "    cols_floc = ['templateId', 'flocId', 'indicatorGroups']\n",
    "    df_ind_groups_floc = df_template_normal[cols_floc].copy()\n",
    "    df_ind_groups_floc.dropna(subset=['indicatorGroups'], inplace=True)\n",
    "    df_ind_groups_floc.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    log.info(f\"Records with indicatorGroups: {len(df_ind_groups_floc)}\")\n",
    "\n",
    "    # Ensure indicatorGroups is a list of dictionaries\n",
    "    df_ind_groups_floc['indicatorGroups'] = df_ind_groups_floc['indicatorGroups'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    df_ind_groups_explode_floc = explode_normalize(\n",
    "        data=df_ind_groups_floc,\n",
    "        id_explode='indicatorGroups',\n",
    "        id=['templateId', 'flocId']\n",
    "    )\n",
    "\n",
    "    df_ind_groups_explode_floc = convert_dataframe(df_ind_groups_explode_floc)\n",
    "    log.info(f\"Extract: Indicator Groups for templates: {len(df_ind_groups_explode_floc)}\")\n",
    "    df_ind_groups_explode_floc.to_parquet(file_ind_groups_floc)\n",
    "    log.info(f\"{file_ind_groups_floc} generated.\")\n",
    "\n",
    "\n",
    "    # ######### indicators\n",
    "\n",
    "    cols_floc = ['templateId', 'flocId', 'indicatorGroups_id', 'indicatorGroups_internalId', 'indicatorGroups_indicators']\n",
    "    df_indicators_floc = df_ind_groups_explode_floc[cols_floc].copy()\n",
    "    df_indicators_floc.rename(columns={'indicatorGroups_indicators': 'indicators'}, inplace=True)\n",
    "    df_indicators_floc.dropna(subset=['indicators'], inplace=True)\n",
    "    df_indicators_floc.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    log.info(f\"Records with indicators: {len(df_indicators_floc)}\")\n",
    "\n",
    "    df_indicators_floc['indicators'] = df_indicators_floc['indicators'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    df_indicators_explode_floc = explode_normalize(\n",
    "        data=df_indicators_floc,\n",
    "        id_explode='indicators',\n",
    "        id=['templateId', 'flocId', 'indicatorGroups_id', 'indicatorGroups_internalId']\n",
    "    )\n",
    "    df_indicators_explode_floc = convert_dataframe(df_indicators_explode_floc)\n",
    "    log.info(f\"EXTRACT: Indicators from Indicator Groups: {len(df_indicators_explode_floc)}\")\n",
    "    df_indicators_explode_floc.to_parquet(file_ind_floc)\n",
    "    log.info(f\"{file_ind_floc} generated.\")\n",
    "\n",
    "    if db.drop_reload:\n",
    "        db.truncate(FlocIndicatorGroups)  # truncate all existing data (remove if needed)\n",
    "        db.truncate(FlocIndicators)  # truncate all existing data (remove if needed)\n",
    "\n",
    "    indicator_groups_floc = SQLAlchemyClient.dataframe_to_object(df_ind_groups_explode_floc, FlocIndicatorGroups)\n",
    "    if indicator_groups_floc:\n",
    "        db.insert_batches(data=indicator_groups_floc)  # insert the new indicator groups\n",
    "\n",
    "    indicators_floc = SQLAlchemyClient.dataframe_to_object(df_indicators_explode_floc, FlocIndicators)\n",
    "    if indicators_floc:\n",
    "        db.insert_batches(data=indicators_floc)  # insert the new indicators\n",
    "else:\n",
    "    print(\"WARN:\\tNo templates found for indicator extraction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

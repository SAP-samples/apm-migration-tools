{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisite Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following pre-requisites are performed before the extraction of data from relevant systems.\n",
    "\n",
    "1. **Set Configuration:**\n",
    "    * Set a `CONFIG_ID` for execution.\n",
    "    * Code picks & loads configuration file (yaml) from `config` folder based on the `config_id` maintained in the file(s). \n",
    "2. **Create directories:**\n",
    "    * Pick the extraction directory maintained in configuration for Indicators\n",
    "    * Extraction data such as CSV / parquet files are staged in this directory.\n",
    "    * This data is local.\n",
    "    * A `reports` directory is also created within, where the error reports are stored.\n",
    "3. **Create database tables:**\n",
    "    * The script creates necessary tables in the database specified in the configuration.\n",
    "    * Tables are only created if they do not already exist in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------ #\n",
    "# For testing\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from modules.util.config import get_config_by_id\n",
    "from modules.util.database import SQLAlchemyClient\n",
    "from modules.util.helpers import Logger\n",
    "import os \n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "CONFIG_ID = 'dca-test'\n",
    "\n",
    "config_extract = get_config_by_id(CONFIG_ID).get(\"extract\")\n",
    "config_indicators = config_extract['indicator']\n",
    "EXTRACTION_DIR = config_indicators[\"directory\"]\n",
    "REPORTS_DIR = f\"{EXTRACTION_DIR}/reports\"\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Directories\n",
    "# ------------------------------------------------------------------------------ #\n",
    "if not os.path.exists(REPORTS_DIR):\n",
    "    os.makedirs(REPORTS_DIR)\n",
    "\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "log.info(\"** EXTRACT - INDICATORS **\")\n",
    "Logger.blank_line(log)\n",
    "log.info(f\"Configuration ID: {CONFIG_ID}\")\n",
    "log.info(f\"Extraction directory: {EXTRACTION_DIR}\")\n",
    "log.info(f\"Reports directory: {REPORTS_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Database\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "db = SQLAlchemyClient(CONFIG_ID)\n",
    "db.table_create_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Extract: Technical Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Technical Objects consists of Equipments & Functional Locations. For migration we consider only the technical objects which have a valid `templateId` & which are either in the `published` or `in revision` state.\n",
    "* Hence while extracting technical objects (EQUs / FLOCs) from the source system, we apply the following filters :\n",
    "    * `templateId is not blank`\n",
    "    * `status is not 1` (ignore the records with unpublished status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "\n",
    "# custom imports\n",
    "from modules.acf.equ_api import ApiEquipment\n",
    "from modules.acf.floc_api import ApiFloc\n",
    "\n",
    "# constants\n",
    "file_equ = rf'{EXTRACTION_DIR}/1_Equipments.parquet'\n",
    "file_floc = rf'{EXTRACTION_DIR}/1_Functional_Locations.parquet'\n",
    "filter = f\"not(templateId eq 'null') and not(status eq '1')\"\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "api_equ = ApiEquipment(CONFIG_ID)\n",
    "ApiFloc = ApiFloc(CONFIG_ID)\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: Technical Objects [Equipments / Functional Locations]\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "# Extract & generate data file for Equipments\n",
    "results = api_equ.get_equipments(\n",
    "    filter=filter,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "df_equ = pd.DataFrame(results)\n",
    "df_equ.rename(columns={'class': 'class_'}, inplace=True) #due to constraint in python & SQL\n",
    "\n",
    "equ_include = config_extract.get('filters').get('equipments_include')\n",
    "equ_exclude = config_extract.get('filters').get('equipments_exclude')\n",
    "\n",
    "if equ_include:\n",
    "    log.warning(f\"[FILTER] Equipments - include: {equ_include}\")\n",
    "    df_equ = df_equ[df_equ['equipmentId'].isin(equ_include)]\n",
    "\n",
    "if equ_exclude:\n",
    "    log.warning(f\"[FILTER] Equipments - exclude: {equ_exclude}\")\n",
    "    df_equ = df_equ[~df_equ['equipmentId'].isin(equ_exclude)]\n",
    "\n",
    "if equ_include or equ_exclude:\n",
    "    log.info(f\"Total Equipments: {len(df_equ)}\")\n",
    "\n",
    "df_equ.to_parquet(file_equ)\n",
    "log.info(f\"{file_equ} generated.\")\n",
    "\n",
    "# Extract & generate data file for Functional Locations\n",
    "results = ApiFloc.get_flocs(\n",
    "    filter=filter,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "df_floc = pd.DataFrame(results)\n",
    "df_floc.rename(columns={'class': 'class_'}, inplace=True)\n",
    "df_floc.to_parquet(file_floc)\n",
    "log.info(f\"{file_floc} generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data fetched above with the necessary filters are then staged in the corresponding tables in the database.\n",
    "\n",
    "| Data | Table Name |\n",
    "| ---- | ---------- |\n",
    "| Equipments | `T_PAI_EQU_HEADER`\n",
    "| Functional Locations | `T_PAI_FLOC_HEADER`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import SQLAlchemyClient, EquipmentHeader, FlocHeader\n",
    "\n",
    "if db.drop_reload:\n",
    "    db.truncate(EquipmentHeader)\n",
    "    db.truncate(FlocHeader)\n",
    "\n",
    "equipments = SQLAlchemyClient.dataframe_to_object(df_equ, EquipmentHeader)\n",
    "flocs = SQLAlchemyClient.dataframe_to_object(df_floc, FlocHeader)\n",
    "\n",
    "if equipments:\n",
    "    db.insert_batches(data=equipments)\n",
    "\n",
    "if flocs:\n",
    "    db.insert_batches(data=flocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Extract: External Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* During migration, we consider only the Technical Objects which have an **external ID** assigned. \n",
    "* We call the `ExternalID` API to extract all the external IDs maintained in the system for Equipments & Functional Locations. \n",
    "* This data will then be used as a *check table* for migrating only the records which have a valid External ID maintained. \n",
    "* During the external ID extraction, we apply the following filters.\n",
    "    * `systemType is 'ERP'`\n",
    "    * `systemName is <sap-system-id>_<sap-client-number>`. The SAP system ID and client number are provided in the configuration\n",
    "    * `externalId does NOT contain LOCAL_`. *LOCAL_* are technical objects that are local to the source systems\n",
    "    * `objectType is 'EQU' or 'FLOC'`. EQU represents equipments & FLOC represents Functional Locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.acf.external_id_api import ApiExternalId\n",
    "import pandas as pd\n",
    "\n",
    "file_equ = rf'{EXTRACTION_DIR}/2_External_Data_EQU.parquet'\n",
    "file_floc = rf'{EXTRACTION_DIR}/2_External_Data_FLOC.parquet'\n",
    "\n",
    "api = ApiExternalId(CONFIG_ID)\n",
    "filter = f\"systemType eq 'SAP ERP' and systemName eq '{api.erp_ssid}' and not substringof('LOCAL_', externalId)\"\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: External Data [Equipments / Functional Locations]\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "def extract_externalData(object_type, filter_base):\n",
    "    filter_str = f\"{filter_base} and objectType eq '{object_type}'\"\n",
    "    results = api.get_external_data(\n",
    "        filter=filter_str,\n",
    "        batch_size=20000\n",
    "    )\n",
    "    return results\n",
    "\n",
    "results_equ = extract_externalData('EQU', filter)\n",
    "df_equ_external = pd.json_normalize(results_equ)\n",
    "df_equ_external.to_parquet(file_equ)\n",
    "log.info(f\"{file_equ} generated.\")\n",
    "\n",
    "\n",
    "results_floc = extract_externalData('FLOC', filter)\n",
    "df_floc_external = pd.json_normalize(results_floc)\n",
    "df_floc_external.to_parquet(file_floc)\n",
    "log.info(f\"{file_floc} generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data fetched above with the necessary filters are then staged in the corresponding tables in the database.\n",
    "\n",
    "| Data | Table Name |\n",
    "| ---- | ---------- |\n",
    "| External Data for Equipments | `T_PAI_EXTERNALDATA`\n",
    "| External Data for Functional Locations | `T_PAI_EXTERNALDATA_FLOC`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import ExternalData_EQU, ExternalData_FLOC\n",
    "\n",
    "if db.drop_reload:\n",
    "    db.truncate(ExternalData_EQU)\n",
    "    db.truncate(ExternalData_FLOC)\n",
    "\n",
    "external_equipments = SQLAlchemyClient.dataframe_to_object(df_equ_external, ExternalData_EQU)\n",
    "if external_equipments:\n",
    "    db.insert_batches(data=external_equipments)\n",
    "\n",
    "external_flocs = SQLAlchemyClient.dataframe_to_object(df_floc_external, ExternalData_FLOC)\n",
    "if external_flocs:       \n",
    "    db.insert_batches(data=external_flocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once the relevant data is staged, we build views that establish a relationship between the extracted technical objects with the external data that was extracted from the system.\n",
    "- These views help us to filter the data, viz. perform further steps only for the technical objects that have an external ID maintained.\n",
    "- The views join between the tables to establish the relationship.\n",
    "- They have a `valid` column present, which represents if the records are valid for processing or not.\n",
    "    - `valid` = `X` if the technical object has a external ID maintained.\n",
    "    - `valid` = `NULL` if the technical object does NOT have an external ID maintained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Extraction - Generic Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.acf.model_api import ApiModel\n",
    "from modules.util.api import APIException\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def extract_models(results:list, modelColumn:str = 'modelId')->list:\n",
    "    df_models = pd.DataFrame(results)\n",
    "    df_models.drop_duplicates(subset=[modelColumn], inplace=True)\n",
    "\n",
    "    log.info(f'Unique models linked to TOs: {len(df_models)}')\n",
    "\n",
    "    api = ApiModel(CONFIG_ID)\n",
    "\n",
    "    response = []\n",
    "    error = []\n",
    "\n",
    "    log.info(f'[GET] Models based on Model ID')\n",
    "\n",
    "    def call_api(id):\n",
    "        return(api.get_model_model_id(id))\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        future_id = {executor.submit(call_api, id): id for id in df_models[modelColumn]}\n",
    "        for future in as_completed(future_id):\n",
    "            id = future_id[future]\n",
    "            try:\n",
    "                res = future.result()\n",
    "                if res:\n",
    "                    response.append(res)\n",
    "            except APIException as api_e:\n",
    "                log.error(f\"API Exception: {id} - {api_e.status_code} - {api_e.response} - {api_e.endpoint}\")\n",
    "                err = {\n",
    "                    'id': id,\n",
    "                    'status_code': api_e.status_code,\n",
    "                    'response':api_e.response,\n",
    "                    'endpoint':api_e.endpoint\n",
    "                }\n",
    "                error.append(err)\n",
    "    log.info(f'Model details fetched: {len(response)}')\n",
    "    if error:\n",
    "       log.error(f'Models (with errors): {len(error)}')\n",
    "    return response, error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Extract Model Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from modules.util.database import V_PAI_EquipmentExternalData, V_PAI_FlocExternalData,EquModelTemplates, FlocModelTemplates, SQLAlchemyClient\n",
    "from modules.util.helpers import convert_dataframe, explode_normalize\n",
    "\n",
    "file_equ_model = rf'{EXTRACTION_DIR}/3_Model_Header_EQU.parquet'\n",
    "file_equ_err = rf'{REPORTS_DIR}/3_Model_EQU_errors.csv'\n",
    "\n",
    "file_floc_model = rf'{EXTRACTION_DIR}/3_Model_Header_FLOC.parquet'\n",
    "file_floc_err = rf'{REPORTS_DIR}/3_Model_FLOC_errors.csv'\n",
    "\n",
    "def process_data(model, table, success_file, error_file):\n",
    "    results = db.select(\n",
    "        model=model,\n",
    "        fields=[\"modelId\"],\n",
    "        distinct=True,\n",
    "        where=[model.valid == \"X\"],\n",
    "        orderby=['modelId']\n",
    "    )\n",
    "\n",
    "    if len(results) > 0:\n",
    "        response,error = extract_models(results)\n",
    "        df_model_normal = pd.json_normalize(response, sep=\"_\")\n",
    "\n",
    "        df_model_explode = explode_normalize(\n",
    "        data=df_model_normal,\n",
    "        id=[col for col in df_model_normal.columns if col != 'templates'],\n",
    "        id_explode='templates')\n",
    "\n",
    "        df_model_explode = convert_dataframe(df_model_explode)\n",
    "        df_model_explode.rename(columns={'class': '_class'}, inplace=True)\n",
    "        \n",
    "        df_model_explode.to_parquet(success_file)\n",
    "        log.info(f\"{success_file} generated.\")\n",
    "        \n",
    "        if db.drop_reload == True:\n",
    "            db.truncate(table)\n",
    "        \n",
    "        models = SQLAlchemyClient.dataframe_to_object(df_model_explode, table)\n",
    "        if models:\n",
    "            db.insert_batches(data=models)\n",
    "\n",
    "        if error:\n",
    "            df_error = pd.DataFrame(error)\n",
    "            df_error.to_csv(error_file, index=False)\n",
    "            log.error(f\"{error_file} generated.\")\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: Model Header for Equipments\")\n",
    "Logger.blank_line(log)\n",
    "process_data(V_PAI_EquipmentExternalData, EquModelTemplates, file_equ_model, file_equ_err)\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: Model Header for Functional Locations\")\n",
    "Logger.blank_line(log)\n",
    "process_data(V_PAI_FlocExternalData, FlocModelTemplates, file_floc_model, file_floc_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Extraction - Generic Method\n",
    "\n",
    "Generic method to extract templates from the template API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a generic method(s) to get indicator groups / indicators from templates\n",
    "\n",
    "from modules.acf.template_api import ApiTemplate\n",
    "from modules.util.api import APIException\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def extract_templates(results:list, templateColumn:str='templateId') -> list:\n",
    "    \"\"\"\n",
    "    Extracts unique templates from the provided results and fetches detailed data for each template using an API.\n",
    "    Args:\n",
    "        results (list): A list of dictionaries containing template data (it should contain an entry with 'templateId' field).\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - response (list): A list of dictionaries with detailed template data.\n",
    "            - error (list): A list of dictionaries with error information for templates that failed to fetch.\n",
    "    Raises:\n",
    "        APIException: If there is an issue with the API call.\n",
    "        Exception: For any other exceptions that occur during the API call.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_template = pd.DataFrame(results)\n",
    "    df_template.drop_duplicates(subset=[templateColumn], inplace=True)\n",
    "\n",
    "    log.info(f'Unique Templates linked to TOs: {len(df_template)}')\n",
    "\n",
    "    api = ApiTemplate(CONFIG_ID)\n",
    "\n",
    "    response = []\n",
    "    error = []\n",
    "\n",
    "    log.info(f'[GET] Templates based on Template ID')\n",
    "\n",
    "    def call_api(id):\n",
    "        return(api.get_template_template_id(id))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        future_id = {executor.submit(call_api, id): id for id in df_template[templateColumn]}\n",
    "        for future in as_completed(future_id):\n",
    "            id = future_id[future]\n",
    "            try:\n",
    "                res = future.result()\n",
    "                if res:\n",
    "                    #since there can be more than one responses for a templateId, we need to loop through all responses\n",
    "                    for data in res:\n",
    "                        template_with_id = {templateColumn: id, **data}\n",
    "                        response.append(template_with_id)\n",
    "            except APIException as api_e:\n",
    "                log.error(f\"API Exception: {id} - {api_e.status_code} - {api_e.response} - {api_e.endpoint}\")\n",
    "                err = {\n",
    "                    'id': id,\n",
    "                    'status_code': api_e.status_code,\n",
    "                    'response':api_e.response,\n",
    "                    'endpoint':api_e.endpoint\n",
    "                }\n",
    "                error.append(err)\n",
    "    log.info(f'Templates fetched: {len(response)}')\n",
    "    if error:\n",
    "        log.error(f'Templates(with errors): {len(error)}')\n",
    "    return response, error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - EQU Templates (Indicator Groups & Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from modules.util.database import SQLAlchemyClient, EquTemplateHeader, V_PAI_EquipmentExternalData\n",
    "from modules.util.helpers import explode_normalize, convert_dataframe\n",
    "\n",
    "\n",
    "file_tgt = rf'{EXTRACTION_DIR}/3_Template_EQU.parquet'\n",
    "file_err = rf'{REPORTS_DIR}/3_Template_EQU_Errors.csv'\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: Templates - Equipments [Indicator Groups > Indicators]\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "# fetch template data from the view for external data - equipments\n",
    "results = db.select(\n",
    "    model=V_PAI_EquipmentExternalData,\n",
    "    fields=['id', 'externalId', 'templateId', 'valid'],\n",
    "    distinct=True,\n",
    "    where=[V_PAI_EquipmentExternalData.valid == \"X\"],\n",
    "    orderby=['externalId']\n",
    ")\n",
    "\n",
    "if len(results) > 0:\n",
    "\n",
    "    # extract templates for the identified records\n",
    "    response,error = extract_templates(results)\n",
    "\n",
    "    df_template_normal = pd.json_normalize(response, sep='_')\n",
    "    df_template_normal = convert_dataframe(df_template_normal)\n",
    "    df_template_normal.to_parquet(file_tgt, index=False)\n",
    "    log.info(f\"{file_tgt} generated\")\n",
    "\n",
    "    if error is not None and error != []:\n",
    "        df_err = pd.json_normalize(error)\n",
    "        df_err.to_csv(file_err, index=False)\n",
    "        log.error(f\"{file_err} generated\")\n",
    "\n",
    "    if db.drop_reload:\n",
    "        db.truncate(EquTemplateHeader)  # truncate all existing data (remove if needed)\n",
    "    templates = SQLAlchemyClient.dataframe_to_object(df_template_normal, EquTemplateHeader)\n",
    "    if templates:\n",
    "        db.insert_batches(data=templates)  # insert the new templates\n",
    "    else:\n",
    "        log.warning(f\"No valid equipments found for template extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import SQLAlchemyClient,EquIndicatorGroups, EquIndicators\n",
    "\n",
    "if len(df_template_normal) > 0:\n",
    "\n",
    "    file_ind_groups = rf'{EXTRACTION_DIR}/3_Template_EQU_Indicator_Groups.parquet'\n",
    "    file_ind = rf'{EXTRACTION_DIR}/3_Template_EQU_Indicators.parquet'\n",
    "\n",
    "    log.info(f'Extract: indicator groups & indicators for templates: {len(df_template_normal)}')\n",
    "\n",
    "    ######### indicator groups\n",
    "    cols = ['templateId', 'id', 'indicatorGroups']\n",
    "    df_ind_groups = df_template_normal[cols].copy()\n",
    "    df_ind_groups.dropna(subset=['indicatorGroups'], inplace=True)\n",
    "    df_ind_groups.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    log.info(f\"Records with indicator groups: {len(df_ind_groups)}\")\n",
    "\n",
    "    # Ensure indicatorGroups is a list of dictionaries\n",
    "    df_ind_groups['indicatorGroups'] = df_ind_groups['indicatorGroups'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    df_ind_groups_explode = explode_normalize(\n",
    "        data=df_ind_groups,\n",
    "        id_explode='indicatorGroups',\n",
    "        id=['templateId', 'id']\n",
    "    )\n",
    "\n",
    "    df_ind_groups_explode = convert_dataframe(df_ind_groups_explode)\n",
    "    log.info(f\"Extract: Indicator Groups for templates: {len(df_ind_groups_explode)}\")\n",
    "    \n",
    "    df_ind_groups_explode.to_parquet(file_ind_groups)\n",
    "    log.info(f\"{file_ind_groups} generated.\")\n",
    "\n",
    "\n",
    "    ######### indicators\n",
    "\n",
    "    cols = ['templateId','id','indicatorGroups_id','indicatorGroups_internalId','indicatorGroups_indicators']\n",
    "    df_indicators = df_ind_groups_explode[cols].copy()\n",
    "    df_indicators.rename(columns={'indicatorGroups_indicators':'indicators'}, inplace=True)\n",
    "    df_indicators.dropna(subset=['indicators'], inplace=True)\n",
    "    df_indicators.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    log.info(f\"Records with indicators: {len(df_indicators)}\")\n",
    "\n",
    "    df_indicators['indicators'] = df_indicators['indicators'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    df_indicators_explode = explode_normalize(\n",
    "        data=df_indicators,\n",
    "        id_explode='indicators',\n",
    "        id=['templateId', 'id', 'indicatorGroups_id', 'indicatorGroups_internalId']\n",
    "    )\n",
    "    df_indicators_explode = convert_dataframe(df_indicators_explode)\n",
    "    log.info(f\"EXTRACT: Indicators from Indicator Groups: {len(df_indicators_explode)}\")\n",
    "    \n",
    "    df_indicators_explode.to_parquet(file_ind)\n",
    "    log.info(f\"{file_ind} generated.\")\n",
    "\n",
    "    if db.drop_reload:\n",
    "        db.truncate(EquIndicatorGroups)  # truncate all existing data (remove if needed)\n",
    "        db.truncate(EquIndicators)  # truncate all existing data (remove if needed)\n",
    "\n",
    "    indicator_groups = SQLAlchemyClient.dataframe_to_object(df_ind_groups_explode, EquIndicatorGroups)\n",
    "    if indicator_groups:\n",
    "        db.insert_batches(data=indicator_groups)  # insert the new indicator groups\n",
    "\n",
    "    indicators = SQLAlchemyClient.dataframe_to_object(df_indicators_explode, EquIndicators)\n",
    "    if indicators:\n",
    "        db.insert_batches(data=indicators)  # insert the new indicators\n",
    "else:\n",
    "    log.warning(f\"No templates found for indicator extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - FLOC Templates (Indicator Groups & Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from modules.util.database import SQLAlchemyClient, V_PAI_FlocExternalData, FlocTemplateHeader\n",
    "from modules.util.helpers import convert_dataframe\n",
    "\n",
    "file_tgt = rf'{EXTRACTION_DIR}/4_Template_FLOC.parquet'\n",
    "file_err = rf'{REPORTS_DIR}/4_Template_FLOC_Errors.csv'\n",
    "\n",
    "df_template_normal = pd.DataFrame()\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Extract: Templates - Functional Locations [Indicator Groups > Indicators]\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "# fetch template data from the view for external data - functional locations\n",
    "\n",
    "results = db.select(\n",
    "    model=V_PAI_FlocExternalData,\n",
    "    fields=['id', 'externalId', 'templateId', 'valid'],\n",
    "    distinct=True,\n",
    "    where=[V_PAI_FlocExternalData.valid == \"X\"],\n",
    "    orderby=['externalId']\n",
    ")\n",
    "\n",
    "if len(results) > 0:\n",
    "\n",
    "    # extract templates for the identified records\n",
    "    response, error = extract_templates(results, templateColumn='templateId')\n",
    "\n",
    "    df_template_normal = pd.json_normalize(response, sep='_')\n",
    "    df_template_normal = convert_dataframe(df_template_normal)\n",
    "    df_template_normal.rename(columns={'id': 'flocId'}, inplace=True)\n",
    "    df_template_normal.to_parquet(file_tgt, index=False)\n",
    "    log.info(f\"{file_tgt} generated\")\n",
    "\n",
    "    if error is not None and error != []:\n",
    "        df_err = pd.json_normalize(error)\n",
    "        df_err.to_csv(file_err, index=False)\n",
    "        log.error(f\"{file_err} generated\")\n",
    "\n",
    "    if db.drop_reload:\n",
    "        db.truncate(FlocTemplateHeader)  # truncate all existing data (remove if needed)\n",
    "        \n",
    "    templates = SQLAlchemyClient.dataframe_to_object(df_template_normal, FlocTemplateHeader)\n",
    "    if templates:\n",
    "        db.insert_batches(data=templates)  # insert the new templates\n",
    "else:\n",
    "    log.warning(f\"No valid functional locations found for template extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import SQLAlchemyClient, FlocIndicatorGroups, FlocIndicators\n",
    "\n",
    "if len(df_template_normal) > 0:\n",
    "\n",
    "    file_ind_groups_floc = rf'{EXTRACTION_DIR}/4_Template_FLOC_Indicator_Groups.parquet'\n",
    "    file_ind_floc = rf'{EXTRACTION_DIR}/4_Template_FLOC_Indicators.parquet'\n",
    "\n",
    "    log.info(f'Extract: indicator groups & indicators for templates: {len(df_template_normal)}')\n",
    "\n",
    "    ######### indicator groups\n",
    "    cols_floc = ['templateId', 'flocId', 'indicatorGroups']\n",
    "    df_ind_groups_floc = df_template_normal[cols_floc].copy()\n",
    "    df_ind_groups_floc.dropna(subset=['indicatorGroups'], inplace=True)\n",
    "    df_ind_groups_floc.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    log.info(f\"Records with indicatorGroups: {len(df_ind_groups_floc)}\")\n",
    "\n",
    "    # Ensure indicatorGroups is a list of dictionaries\n",
    "    df_ind_groups_floc['indicatorGroups'] = df_ind_groups_floc['indicatorGroups'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    df_ind_groups_explode_floc = explode_normalize(\n",
    "        data=df_ind_groups_floc,\n",
    "        id_explode='indicatorGroups',\n",
    "        id=['templateId', 'flocId']\n",
    "    )\n",
    "\n",
    "    df_ind_groups_explode_floc = convert_dataframe(df_ind_groups_explode_floc)\n",
    "    log.info(f\"Extract: Indicator Groups for templates: {len(df_ind_groups_explode_floc)}\")\n",
    "    df_ind_groups_explode_floc.to_parquet(file_ind_groups_floc)\n",
    "    log.info(f\"{file_ind_groups_floc} generated.\")\n",
    "\n",
    "\n",
    "    # ######### indicators\n",
    "\n",
    "    cols_floc = ['templateId', 'flocId', 'indicatorGroups_id', 'indicatorGroups_internalId', 'indicatorGroups_indicators']\n",
    "    df_indicators_floc = df_ind_groups_explode_floc[cols_floc].copy()\n",
    "    df_indicators_floc.rename(columns={'indicatorGroups_indicators': 'indicators'}, inplace=True)\n",
    "    df_indicators_floc.dropna(subset=['indicators'], inplace=True)\n",
    "    df_indicators_floc.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    log.info(f\"Records with indicators: {len(df_indicators_floc)}\")\n",
    "\n",
    "    df_indicators_floc['indicators'] = df_indicators_floc['indicators'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    df_indicators_explode_floc = explode_normalize(\n",
    "        data=df_indicators_floc,\n",
    "        id_explode='indicators',\n",
    "        id=['templateId', 'flocId', 'indicatorGroups_id', 'indicatorGroups_internalId']\n",
    "    )\n",
    "    df_indicators_explode_floc = convert_dataframe(df_indicators_explode_floc)\n",
    "    log.info(f\"EXTRACT: Indicators from Indicator Groups: {len(df_indicators_explode_floc)}\")\n",
    "    df_indicators_explode_floc.to_parquet(file_ind_floc)\n",
    "    log.info(f\"{file_ind_floc} generated.\")\n",
    "\n",
    "    if db.drop_reload:\n",
    "        db.truncate(FlocIndicatorGroups)  # truncate all existing data (remove if needed)\n",
    "        db.truncate(FlocIndicators)  # truncate all existing data (remove if needed)\n",
    "\n",
    "    indicator_groups_floc = SQLAlchemyClient.dataframe_to_object(df_ind_groups_explode_floc, FlocIndicatorGroups)\n",
    "    if indicator_groups_floc:\n",
    "        db.insert_batches(data=indicator_groups_floc)  # insert the new indicator groups\n",
    "\n",
    "    indicators_floc = SQLAlchemyClient.dataframe_to_object(df_indicators_explode_floc, FlocIndicators)\n",
    "    if indicators_floc:\n",
    "        db.insert_batches(data=indicators_floc)  # insert the new indicators\n",
    "else:\n",
    "    print(f\"WARN:\\tNo templates found for indicator extraction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

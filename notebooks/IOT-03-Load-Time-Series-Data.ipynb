{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load parquet files to APM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import SQLAlchemyClient\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "CONFIG_ID = \"CUSTOM_TEST\"\n",
    "\n",
    "# setup database\n",
    "\n",
    "db = SQLAlchemyClient(CONFIG_ID)\n",
    "db.table_create_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine parquet files\n",
    "\n",
    "First we'll read and combine all files within the same folder of the same indicator group.\n",
    "The new dataset will be stored in a subfolder named as `ready` within your folder where\n",
    "the transformed time-series files are stored.\n",
    "Within this combining we will also make sure that each file for the same indicator group\n",
    "has not more than 1.000.000 rows as this is a limitation by file upload api from eIOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from modules.util.helpers import Logger\n",
    "from modules.util.config import get_config_by_id\n",
    "import pandas as pd\n",
    "\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "config = get_config_by_id(CONFIG_ID)\n",
    "\n",
    "# TO-DO: move to configuration and create folders if not exist  (use pathlib)\n",
    "TRANSFORMED_FOLDER = config[\"transform\"][\"time-series\"][\"directory\"]\n",
    "READY_FOLDER = os.path.join(TRANSFORMED_FOLDER, \"ready\")\n",
    "\n",
    "# make sure the pick folder exists\n",
    "if not os.path.exists(READY_FOLDER):\n",
    "    os.makedirs(READY_FOLDER)\n",
    "\n",
    "# we need to iterate over all folders which are within the transformed folder\n",
    "for root, dirs, files in os.walk(TRANSFORMED_FOLDER):\n",
    "    # log the current folder\n",
    "    log.info(f\"Processing folder: {root} with {len(files)} files\")\n",
    "\n",
    "    # List to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # iterate over files and read them into dataframes\n",
    "    # the index is \"managedObjectId\", \"_time\", \"measuringNodeId\"\n",
    "\n",
    "    for f in files:\n",
    "        file_path = os.path.join(root, f)\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Ensure the key columns exist in the DataFrame\n",
    "        if all(\n",
    "            col in df.columns for col in [\"managedObjectId\", \"measuringNodeId\", \"_time\"]\n",
    "        ):\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            log.error(f\"Skipping {f}: Missing key columns.\")\n",
    "\n",
    "    # Merge all DataFrames on the key columns\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "        # Drop duplicate rows (optional, if needed)\n",
    "        combined_df = combined_df.drop_duplicates(\n",
    "            subset=[\"managedObjectId\", \"measuringNodeId\", \"_time\"]\n",
    "        )\n",
    "\n",
    "        # Determine the number of chunks needed based on the maximum lines per file\n",
    "        max_lines_per_file = 1000000\n",
    "        num_chunks = (len(combined_df) // max_lines_per_file) + 1\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * max_lines_per_file\n",
    "            end_idx = (i + 1) * max_lines_per_file\n",
    "            chunk_df = combined_df.iloc[start_idx:end_idx]\n",
    "\n",
    "            # Save each chunk to a new parquet file\n",
    "            chunk_file_path = os.path.join(\n",
    "                READY_FOLDER,\n",
    "                f\"{os.path.basename(root)}_combined_chunk_{i}.parquet\",\n",
    "            )\n",
    "            chunk_df.to_parquet(chunk_file_path, index=False)\n",
    "            log.debug(f\"Chunk {i} saved to {chunk_file_path}.\")\n",
    "    else:\n",
    "        log.info(\"No valid Parquet files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload files to eIOT\n",
    "\n",
    "The next step is to upload the data to eIOT. As the API has some limitations, we need also to\n",
    "check that the file size of each file does not reach the limit of 50 MB. Therefore it might be\n",
    "that we split the files even further (besides the 1 million lines limit).\n",
    "\n",
    "Afterwards the files can be uploaded to eIOT file interface. As a result we'll get a file id\n",
    "back which can be used to check the processing status of the file. We're storing the fild id\n",
    "therefore in the database table T_EIOT_UPLOAD_STATUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from modules.apm.eiot import EIoTApi\n",
    "from modules.util.helpers import Logger\n",
    "from modules.util.database import SQLAlchemyClient, EIotUploadStatus, EIotUploadStatusValues\n",
    "from modules.util.config import get_config_by_id\n",
    "# import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "config = get_config_by_id(CONFIG_ID)\n",
    "api_eiot = EIoTApi(CONFIG_ID)\n",
    "db = SQLAlchemyClient(CONFIG_ID)\n",
    "\n",
    "UPLOADED_FOLDER = config[\"load\"][\"time-series\"][\"directory\"]\n",
    "TRANSFORMED_FOLDER = config[\"transform\"][\"time-series\"][\"directory\"]\n",
    "READY_FOLDER = os.path.join(TRANSFORMED_FOLDER, \"ready\")\n",
    "\n",
    "# make sure the folder exists\n",
    "if not os.path.exists(UPLOADED_FOLDER):\n",
    "    os.makedirs(UPLOADED_FOLDER)\n",
    "\n",
    "# in the previous step we have saved the combined dataframes to parquet files\n",
    "# we now need to upload these files to the EIOT platform\n",
    "# we need to iterate over all folders which are within the transformed folder\n",
    "for root, dirs, files in os.walk(READY_FOLDER):\n",
    "    # filter files to only include parquet files\n",
    "    files = [f for f in files if f.endswith(\".parquet\")]\n",
    "    # iterate over files and upload them to the EIOT platform\n",
    "    for f in files:\n",
    "        # we need to check the size of the file before uploading it\n",
    "        # the maximum file size is 50 MB, so we need to split the file into chunks if it is larger\n",
    "        max_file_size = 30 * 1024 * 1024  # 50 MB in bytes\n",
    "        file_path = os.path.join(root, f)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        # Read the parquet file\n",
    "        table = pq.read_table(file_path)\n",
    "        # get the lines of the file\n",
    "        lines = table.num_rows\n",
    "\n",
    "        if file_size > max_file_size or lines > 1000000:\n",
    "            log.info(f\"File {f} is larger than 50 MB or has more than 1 million lines, splitting into chunks.\")\n",
    "            # Split the file into chunks and upload each chunk\n",
    "            schema = table.schema\n",
    "\n",
    "            # Determine chunk size based on file size and number of lines\n",
    "            chunk_size = min(max_file_size // table.num_columns, 1000000)\n",
    "            chunks = [table.slice(i, chunk_size) for i in range(0, table.num_rows, chunk_size)]\n",
    "\n",
    "            for chunk_number, chunk in enumerate(chunks):\n",
    "                chunk_file_path = file_path.replace('.parquet', f\"_chunk_{chunk_number}.parquet\")\n",
    "                pq.write_table(chunk, chunk_file_path)\n",
    "                # Upload the chunk\n",
    "                file_response = api_eiot.upload_file(parquet_file_and_path=chunk_file_path)\n",
    "                log.info(f\"Uploaded chunk {chunk_number} of file {f} with ID {file_response['fileId']}\")\n",
    "                db.insert_one(\n",
    "                    EIotUploadStatus(\n",
    "                        fileName=file_response[\"fileName\"],\n",
    "                        fileId=file_response[\"fileId\"],\n",
    "                        status=EIotUploadStatusValues.UPLOADED,\n",
    "                        statusDescription=\"upload_initiated\",\n",
    "                        statusTimestamp=file_response[\"uploadedTime\"],\n",
    "                    )\n",
    "                )\n",
    "                # move the chunk file after uploading to the uploaded folder\n",
    "                os.rename(chunk_file_path, os.path.join(UPLOADED_FOLDER, os.path.basename(chunk_file_path)))\n",
    "        else:\n",
    "            # before start uploading the file, we need to check if the file is already uploaded\n",
    "            # if it is uploaded, we should skip it\n",
    "\n",
    "            # check if the file is already uploaded\n",
    "            file_status = db.select_one(model=EIotUploadStatus, where=[EIotUploadStatus.fileName==f])\n",
    "            if file_status and file_status[\"status\"] != EIotUploadStatusValues.FAILURE:\n",
    "                log.info(f\"File {f} is already uploaded, skipping.\")\n",
    "                continue\n",
    "\n",
    "            file_response = api_eiot.upload_file(parquet_file_and_path=file_path)\n",
    "            log.info(f\"Uploaded file {f} with ID {file_response['fileId']}\")\n",
    "            db.insert_one(\n",
    "                EIotUploadStatus(\n",
    "                    fileName=file_response[\"fileName\"],\n",
    "                    fileId=file_response[\"fileId\"],\n",
    "                    status=EIotUploadStatusValues.UPLOADED,\n",
    "                    statusDescription=\"upload_initiated\",\n",
    "                    statusTimestamp=file_response[\"uploadedTime\"],\n",
    "                )\n",
    "            )\n",
    "            # move the chunk file after uploading to the uploaded folder\n",
    "            os.rename(file_path, os.path.join(UPLOADED_FOLDER, os.path.basename(file_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check file upload status\n",
    "\n",
    "The step starts by loading all outstanding file uploads from the internal database. This means all process\n",
    "which are not finished and not failed. Next we check for each entry the current status of processing\n",
    "and update also the databse status if it has changed.\n",
    "\n",
    "After each iteration we'll wait for some time before checking the status again.\n",
    "\n",
    "This step is stopped by the user or when all uploads are done or failed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from modules.apm.eiot import EIoTApi\n",
    "from modules.util.helpers import Logger\n",
    "from modules.util.database import (\n",
    "    SQLAlchemyClient,\n",
    "    EIotUploadStatus,\n",
    "    EIotUploadStatusValues,\n",
    ")\n",
    "from modules.util.config import get_config_by_id\n",
    "\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "config = get_config_by_id(CONFIG_ID)\n",
    "api_eiot = EIoTApi(CONFIG_ID)\n",
    "db = SQLAlchemyClient(CONFIG_ID)\n",
    "\n",
    "uploaded_entries = db.select(\n",
    "    model=EIotUploadStatus,\n",
    "    where=[\n",
    "        EIotUploadStatus.status != EIotUploadStatusValues.PROCESSED\n",
    "        and EIotUploadStatus.status != EIotUploadStatusValues.FAILURE\n",
    "    ],\n",
    ")\n",
    "\n",
    "# iterate over all uploaded entries and check if they are processed\n",
    "all_exports_complete = False\n",
    "\n",
    "count = 1\n",
    "\n",
    "while not all_exports_complete:\n",
    "    all_exports_complete = True\n",
    "\n",
    "    for entry in uploaded_entries:\n",
    "        file_id = entry[\"fileId\"]\n",
    "        status = api_eiot.get_file_status(file_id)\n",
    "        log.info(f\"File {file_id} has status: {status[\"status\"]}.\")\n",
    "\n",
    "        if entry[\"status\"] != status[\"status\"]:\n",
    "            # we have a new status, update the database\n",
    "            entry[\"status\"] = status[\"status\"]\n",
    "            if status[\"status\"] == \"Processed\":\n",
    "                new_status = EIotUploadStatusValues.PROCESSED\n",
    "            elif status[\"status\"] == \"Received\":\n",
    "                log.info(f\"File {file_id} is received.\")\n",
    "                new_status = EIotUploadStatusValues.RECEIVED\n",
    "            elif status[\"status\"] == \"In Process\":\n",
    "                new_status = EIotUploadStatusValues.IN_PROCESS\n",
    "            elif status[\"status\"] == \"Scanned\":\n",
    "                new_status = EIotUploadStatusValues.SCANNED\n",
    "            elif status[\"status\"] == \"Processing Failed\":\n",
    "                new_status = EIotUploadStatusValues.FAILURE\n",
    "                log.error(f\"File {file_id} processing failed: {status[\"description\"]}\")\n",
    "            else:\n",
    "                raise Exception(f\"Unknown status: {status['status']}\")\n",
    "\n",
    "            db.update_one(\n",
    "                model=EIotUploadStatus,\n",
    "                where=[EIotUploadStatus.fileId == file_id],\n",
    "                values={\n",
    "                    \"status\": new_status,\n",
    "                    \"statusDescription\": status[\"description\"],\n",
    "                },\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # check if the status is one of the final statuses\n",
    "        if status[\"status\"] not in [\"Processing Failed\", \"Processed\"]:\n",
    "            all_exports_complete = False\n",
    "\n",
    "    # wait for some time before checking the status again\n",
    "    if not all_exports_complete:\n",
    "        time.sleep(30 * count)\n",
    "        count += 1\n",
    "\n",
    "\n",
    "log.info(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Time Series Data from SAP IOT\n",
    "\n",
    "This notebook will download the time series data from SAP IOT and store the downloaded files in the\n",
    "folder which is configured in the respective configuration file under **[extract]/[time-series]/[directory]**.\n",
    "\n",
    "## Pre-requirements\n",
    "\n",
    "- within the `notebooks` folder there must be a file `.env` which contains the credentials to\n",
    "access SAP PAI and SAP IOT. See file `.env.sample` as a reference for all needed parameters\n",
    "to be maintained. By default you'll get the details of the parameters from the PAI and IOT\n",
    "service key.\n",
    "\n",
    "- the indicator ETL part needs to be done as this notebook requires a mapping from PAI indicators\n",
    "to APM indicators which will be created in the steps when migrating the indicators. As a result\n",
    "the view V_POST_LOAD_INDICATORS will hold all needed information.\n",
    "\n",
    "- you need to know which datamodel your IOT data is using. If you use the IOT subscription\n",
    "which is embedded with SAP APM, it'll propably the `ABSTRACT` model. Otherwise, using a separate\n",
    "SAP IOT subscription will be `THING` model. In the first cell you need to define this configuration.\n",
    "\n",
    "## Steps in the notebook\n",
    "\n",
    "1. Define configuration and data model\n",
    "2. Initialize SQLite database\n",
    "3. Create database table `iot_export_status`\n",
    "4. Initiate download\n",
    "5. Check processing status\n",
    "6. Download the data\n",
    "7. Unzip the downloaded files\n",
    "\n",
    "For detailed description see [documentation](../docs/time-series-migration.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the config you want to use\n",
    "CONFIG_ID = \"dca-test\"\n",
    "# Define the data model you want to use, either \"ABSTRACT\" or \"THING\"\n",
    "DATA_MODEL = \"ABSTRACT\"\n",
    "\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Indicator Groups\n",
    "\n",
    "As first we need to figure out for which indicator groups we want to download the time series data from cold-store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sqlite database\n",
    "from modules.acf.model_api import ApiModel\n",
    "from modules.iot.iot import SAPIoTAPIWrapper\n",
    "from modules.util.helpers import Logger\n",
    "from modules.util.database import (\n",
    "    SQLAlchemyClient,\n",
    "    EquIndicatorGroups,\n",
    "    FlocIndicatorGroups,\n",
    ")\n",
    "\n",
    "\n",
    "extraction_ids = []\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "log.info(\"** EXTRACT - TIME SERIES DATA **\")\n",
    "Logger.blank_line(log)\n",
    "log.info(f\"Configuration ID: {CONFIG_ID} - Data Model: {DATA_MODEL}\")\n",
    "\n",
    "db = SQLAlchemyClient(CONFIG_ID)\n",
    "db.table_create_all()\n",
    "iot_wrapper = SAPIoTAPIWrapper(config_id=CONFIG_ID)\n",
    "model_wrapper = ApiModel(config_id=CONFIG_ID)\n",
    "\n",
    "if DATA_MODEL == \"ABSTRACT\":\n",
    "    # property types are the indicator groups\n",
    "    equi_ind_groups = db.select(EquIndicatorGroups)\n",
    "    floc_ind_groups = db.select(FlocIndicatorGroups)\n",
    "    log.debug(f\"fetched {len(equi_ind_groups)} indicator groups\")\n",
    "    # iterate over all indicator groups\n",
    "    for group in equi_ind_groups:\n",
    "        log.debug(\n",
    "            f\"found indicator group: {group['indicatorGroups_description_short']}\"\n",
    "        )\n",
    "        extraction_ids.append(f\"IG_{group['indicatorGroups_id']}\")\n",
    "\n",
    "    for group in floc_ind_groups:\n",
    "        log.debug(\n",
    "            f\"found indicator group: {group['indicatorGroups_description_short']}\"\n",
    "        )\n",
    "        extraction_ids.append(f\"IG_{group['indicatorGroups_id']}\")\n",
    "\n",
    "elif DATA_MODEL == \"THING\":\n",
    "    thing_types = iot_wrapper.get_thing_types()\n",
    "    equi_model = model_wrapper.get_equipment_models()\n",
    "    floc_model = model_wrapper.get_floc_models()\n",
    "\n",
    "    log.debug(f\"fetched {len(equi_model)} equipment models\")\n",
    "\n",
    "    # we expect that every model has a thing type\n",
    "    # iterate over all equipment models and check if the thing type exists\n",
    "    thing_types_found = []\n",
    "    for model in equi_model:\n",
    "        search_terms = model[\"modelSearchTerms\"].split(\",\")\n",
    "        found = False\n",
    "        for term in search_terms:\n",
    "            if term in thing_types:\n",
    "                thing_types_found.append(term)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            log.debug(f\"thing type {model['modelSearchTerms']} not found\")\n",
    "\n",
    "    log.info(\n",
    "        f\"found {len(thing_types_found)} equipment models with existing thing types\"\n",
    "    )\n",
    "    for thing_type in thing_types_found:\n",
    "        log.debug(f\"found thing type: {thing_type}\")\n",
    "        property_sets = iot_wrapper.get_property_sets_by_thing_type(thing_type)\n",
    "        log.debug(f\"found {len(property_sets)} property sets\")\n",
    "        for property_set in property_sets:\n",
    "            log.debug(f\"found property set: {property_set}\")\n",
    "            if property_set not in extraction_ids:\n",
    "                extraction_ids.append(property_set)\n",
    "\n",
    "log.info(f\"found {len(extraction_ids)} extraction ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Setup\n",
    "\n",
    "We'll create a new database table \"iot_export_status\" where we store the\n",
    "request id from the time series cold store. Later we use the status\n",
    "column to keep track which files can be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from modules.database.tables import meta_obj\n",
    "from modules.util.config import get_config_by_id\n",
    "from modules.util.helpers import Logger\n",
    "\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "config = get_config_by_id(CONFIG_ID)\n",
    "engine = create_engine(config[\"database\"][\"connection\"], echo=False)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    log.info(\"creating iot_export_status table\")\n",
    "    # meta_obj.drop_all(engine)\n",
    "    meta_obj.create_all(engine)\n",
    "    # check that the table is empty\n",
    "    \n",
    "    # conn.execute(iot_export_status_table.delete())\n",
    "    # result = conn.execute(iot_export_status_table.select())\n",
    "    # log.info(result.fetchall())\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate Download\n",
    "\n",
    "For the extracted indicator groups we'll trigger the download. First we start with yearly time-frames.\n",
    "The overall time-frame and the time slices must be defined in the config file under `[extract]-[time-series]`.\n",
    "Here you find the properties: `time_range_from`, `time_range_to`, `time_range_interval`.\n",
    "\n",
    "For each request we'll get a request id back which we save in an internal status table (iot_export_status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.iot.iot import SAPIoTAPIWrapper\n",
    "from modules.util.helpers import generate_yearly_slices\n",
    "from sqlalchemy import create_engine, func, select\n",
    "from modules.database.tables import iot_export_status_table\n",
    "from modules.util.config import get_config_by_id, get_system_by_type\n",
    "from modules.util.helpers import Logger\n",
    "\n",
    "import time\n",
    "\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "config = get_config_by_id(CONFIG_ID)\n",
    "iot_config = get_system_by_type(config, \"IOT\")\n",
    "engine = create_engine(config[\"database\"][\"connection\"], echo=False)\n",
    "\n",
    "iot_wrapper = SAPIoTAPIWrapper(config_id=CONFIG_ID)\n",
    "time_slices = generate_yearly_slices(\n",
    "    config[\"extract\"][\"time-series\"][\"time_range_from\"],\n",
    "    config[\"extract\"][\"time-series\"][\"time_range_to\"],\n",
    ")\n",
    "property_set_type_ignore_list = list()\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # iterate over all thing types\n",
    "\n",
    "    for indicator_group in extraction_ids:\n",
    "        # check if this property set type is in the ignore list\n",
    "        if indicator_group in property_set_type_ignore_list:\n",
    "            # if the property set type is in the ignore list, continue with the next property set type\n",
    "            continue\n",
    "        log.info(f\"start export for {indicator_group}\")\n",
    "        # iterate over all time slices\n",
    "        for time_slice in time_slices:\n",
    "            # get the data for the thing type and the time slice\n",
    "            try: \n",
    "                # check if this property set type is already in the iot_export_status table\n",
    "                # if it is already in the table, we don't need to export it again\n",
    "                query = iot_export_status_table.select().where(\n",
    "                    (iot_export_status_table.c.tenant_id == config[\"config_id\"])\n",
    "                    & (iot_export_status_table.c.indicator_group == indicator_group)\n",
    "                    & (\n",
    "                        iot_export_status_table.c.start_date\n",
    "                        == time_slice[0].strftime(\"%Y-%m-%d\")\n",
    "                    )\n",
    "                    & (\n",
    "                        iot_export_status_table.c.end_date\n",
    "                        == time_slice[1].strftime(\"%Y-%m-%d\")\n",
    "                    )\n",
    "                )\n",
    "                status = conn.execute(query).fetchall()\n",
    "\n",
    "                if status != []:\n",
    "                    # if the status is not None, the export is already done\n",
    "                    continue\n",
    "\n",
    "                # initiate the export\n",
    "                request_id = iot_wrapper.initiate_time_series_export(\n",
    "                    indicator_group=indicator_group,\n",
    "                    start_date=time_slice[0].strftime(\"%Y-%m-%d\"),\n",
    "                    end_date=time_slice[1].strftime(\"%Y-%m-%d\"),\n",
    "                )\n",
    "                # store the response id in the thing_types_status dictionary to keep track of the exports\n",
    "                # with reference to the start and end date of the export\n",
    "                stmt = iot_export_status_table.insert().values(\n",
    "                    tenant_id=config[\"config_id\"],\n",
    "                    indicator_group=indicator_group,\n",
    "                    start_date=time_slice[0].strftime(\"%Y-%m-%d\"),\n",
    "                    end_date=time_slice[1].strftime(\"%Y-%m-%d\"),\n",
    "                    status=\"Initiated\",\n",
    "                    request_id=request_id,\n",
    "                )\n",
    "\n",
    "                conn.execute(stmt)\n",
    "                conn.commit()\n",
    "\n",
    "            except Exception as e:\n",
    "                # dependent on the DATA_MODEL, we need to deal differently with the error\n",
    "                # THING_MODEL\n",
    "                # 400 - bad request: malformed query or errors in the query (property does not exist)\n",
    "                # 404 - not found: there is no data in the provided timerange\n",
    "                # 413 - too large: too much data - choose smaller date interval\n",
    "                if DATA_MODEL == 'THING':\n",
    "                    if e.response.status_code == 400:\n",
    "                        # store the error message in the iot_export_status table\n",
    "                        error_message = e.response.json()\n",
    "                        if \"message\" in error_message:\n",
    "                            message = error_message[\"message\"]\n",
    "                        else:\n",
    "                            message=\"Unknown error\"\n",
    "\n",
    "                        stmt = iot_export_status_table.insert().values(\n",
    "                            tenant_id=config[\"config_id\"],\n",
    "                            indicator_group=indicator_group,\n",
    "                            start_date=time_slice[0].strftime(\"%Y-%m-%d\"),\n",
    "                            end_date=time_slice[1].strftime(\"%Y-%m-%d\"),\n",
    "                            status=\"Error\",\n",
    "                            message=message,\n",
    "                            request_id=\"None\",\n",
    "                        )\n",
    "                    elif e.response.status_code == 404:\n",
    "                        # add this property set type to an ignore list and none of the time slices will be exported\n",
    "                        property_set_type_ignore_list.append(indicator_group)\n",
    "                        continue\n",
    "                    elif e.response.status_code == 413:\n",
    "                        # reduce the time slice to a smaller interval\n",
    "                        continue\n",
    "\n",
    "                # ABSTRACT_MODEL\n",
    "                # Any export script we build for not thing model but model abstraction download from PAI or APM has to deal\n",
    "                # with the message text from a 400 return - 400 and \"no sdata\" is basically a succes checking if there is\n",
    "                # data - 400 and \"too much data\" means the query timeframe has to be reduced.\n",
    "                elif DATA_MODEL == 'ABSTRACT':\n",
    "                    if e.response.status_code == 400:\n",
    "                        error_message = e.response.json()\n",
    "                        # check if response message contains \"no data\"\n",
    "                        if \"message\" in error_message and \"no data\" in error_message[\"message\"]:\n",
    "                            property_set_type_ignore_list.append(indicator_group)\n",
    "                            continue\n",
    "                        elif \"message\" in error_message and \"Data not found for the requested date range\" in error_message[\"message\"]:\n",
    "                            property_set_type_ignore_list.append(indicator_group)\n",
    "                            continue\n",
    "                        elif \"message\" in error_message and \"too much data\" in error_message[\"message\"]:\n",
    "                            # means the query timeframe has to be reduced.\n",
    "                            log.warning(\n",
    "                                \"query timeframe has to be reduced\"\n",
    "                                f\"for TS download for: {indicator_group}\"\n",
    "                                f\"{time_slice[0].strftime(\"%Y-%m-%d\")}\"\n",
    "                                f\"{time_slice[1].strftime(\"%Y-%m-%d\")}\"\n",
    "                            )\n",
    "                            stmt = iot_export_status_table.insert().values(\n",
    "                                tenant_id=config[\"config_id\"],\n",
    "                                indicator_group=indicator_group,\n",
    "                                start_date=time_slice[0].strftime(\"%Y-%m-%d\"),\n",
    "                                end_date=time_slice[1].strftime(\"%Y-%m-%d\"),\n",
    "                                status=\"Timeframe\",\n",
    "                                message=\"query timeframe has to be reduced\",\n",
    "                                request_id=\"None\",\n",
    "                            )\n",
    "                    else:\n",
    "                        error_message = e.response.json()\n",
    "                        if \"message\" in error_message:\n",
    "                            message = error_message[\"message\"]\n",
    "                        else:\n",
    "                            message = \"Unknown error\"\n",
    "\n",
    "                        log.error(\n",
    "                            f\"unknown status code: {e.response.status_code} when \"\n",
    "                            f\"initiate TS download for: {indicator_group}\"\n",
    "                            f\"{time_slice[0].strftime(\"%Y-%m-%d\")}\"\n",
    "                            f\"{time_slice[1].strftime(\"%Y-%m-%d\")}\"\n",
    "                        )\n",
    "                        stmt = iot_export_status_table.insert().values(\n",
    "                            tenant_id=config[\"config_id\"],\n",
    "                            indicator_group=indicator_group,\n",
    "                            start_date=time_slice[0].strftime(\"%Y-%m-%d\"),\n",
    "                            end_date=time_slice[1].strftime(\"%Y-%m-%d\"),\n",
    "                            status=\"Error\",\n",
    "                            message=message,\n",
    "                            request_id=\"None\",\n",
    "                        )\n",
    "\n",
    "                conn.execute(stmt)\n",
    "                conn.commit()\n",
    "\n",
    "                continue\n",
    "\n",
    "    # print status report: how many exports are initiated and how many failed\n",
    "    status_combined = (\n",
    "        select(iot_export_status_table.c.status, func.count().label(\"count\"))\n",
    "        .where(iot_export_status_table.c.status.in_([\"Error\", \"Initiated\"]))\n",
    "        .group_by(iot_export_status_table.c.status)\n",
    "    )\n",
    "\n",
    "    result = conn.execute(status_combined).fetchall()\n",
    "    status_dict = {row[0]: row[1] for row in result}\n",
    "    status_error = status_dict.get(\"Error\", 0)\n",
    "    status_initiated = status_dict.get(\"Initiated\", 0)\n",
    "\n",
    "    log.info(f\"{status_error} entries failed and {status_initiated} entries are initiated\")\n",
    "\n",
    "log.info(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check processing status\n",
    "\n",
    "Next we need to check the processing status of all initiated downloads. Once all\n",
    "downloads are ready to download we can continue with next step.\n",
    "\n",
    "The following are the possible statuses:\n",
    "\n",
    "- Initiated: The request is placed successfully.\n",
    "- Submitted: The request for data export is initiated and the method is retrieving the data and preparing for the export process.\n",
    "- Failed: The request for data export failed due to various reasons. The reasons are listed in the response payload.\n",
    "- Exception: The system retried to initiate the data export but failed.\n",
    "- Ready for Download: The request for data export succeeded and the data is available in a file format for download.\n",
    "- Expired: The data that is ready for download is available only for seven days, beyond which the exported data is not available for download. You should re-initiate the request for data export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.iot.iot import SAPIoTAPIWrapper\n",
    "from sqlalchemy import create_engine, and_\n",
    "from modules.database.tables import iot_export_status_table\n",
    "from modules.util.config import get_config_by_id, get_system_by_type\n",
    "\n",
    "iot_wrapper = SAPIoTAPIWrapper(config_id=CONFIG_ID)\n",
    "\n",
    "config = get_config_by_id(CONFIG_ID)\n",
    "iot_config = get_system_by_type(config, \"IOT\")\n",
    "engine = create_engine(config[\"database\"][\"connection\"], echo=False)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # select all initiated exports from the iot_export_status table\n",
    "    query = iot_export_status_table.select().where(\n",
    "        (iot_export_status_table.c.tenant_id == config[\"config_id\"]) &\n",
    "        (iot_export_status_table.c.status == \"Initiated\")\n",
    "    )\n",
    "\n",
    "    results = conn.execute(query).fetchall()\n",
    "\n",
    "    # iterate over download_data dictionary and check the status of the exports\n",
    "    all_exports_complete = False\n",
    "\n",
    "    count = 1\n",
    "    while not all_exports_complete:\n",
    "        all_exports_complete = True\n",
    "        for export_status in results:\n",
    "            export_status = export_status._asdict()\n",
    "            # get the status of the export\n",
    "            status = iot_wrapper.get_time_series_export_status(\n",
    "                request_id=export_status[\"request_id\"]\n",
    "            )\n",
    "            log.info(\n",
    "                f\"Status for {export_status['indicator_group']} from \"\n",
    "                f\"{export_status['start_date']} to \"\n",
    "                f\"{export_status['end_date']}: {status}\"\n",
    "            )\n",
    "\n",
    "            if export_status[\"status\"] != status:\n",
    "                # update the status in the iot_export_status table\n",
    "                stmt = (\n",
    "                    iot_export_status_table.update()\n",
    "                    .values(status=status)\n",
    "                    .where(\n",
    "                        and_(\n",
    "                            iot_export_status_table.c.tenant_id == config[\"config_id\"],\n",
    "                            iot_export_status_table.c.indicator_group\n",
    "                            == export_status[\"indicator_group\"],\n",
    "                            iot_export_status_table.c.start_date\n",
    "                            == time_slice[0].strftime(\"%Y-%m-%d\"),\n",
    "                            iot_export_status_table.c.end_date\n",
    "                            == time_slice[1].strftime(\"%Y-%m-%d\"),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                res = conn.execute(stmt)\n",
    "                conn.commit()\n",
    "\n",
    "                # update the status in the results list as well\n",
    "\n",
    "                export_status['status'] = status\n",
    "\n",
    "            # check if the status is one of the final statuses\n",
    "            if status not in [\"Failed\", \"Exception\", \"Ready for Download\", \"Expired\"]:\n",
    "                all_exports_complete = False\n",
    "\n",
    "        # wait for some time before checking the status again\n",
    "        if not all_exports_complete:\n",
    "            time.sleep(30*count)\n",
    "            count += 1\n",
    "\n",
    "log.info(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Time-Series Data to Disk\n",
    "\n",
    "The next step is to download the cold store data to the disk for further processing.\n",
    "The folder where you want to save the time series data can be defined in the config\n",
    "section at `[\"extract\"][\"time-series\"][\"directory\"]`. As the file size can be large\n",
    "the download will be done in chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from modules.iot.iot import SAPIoTAPIWrapper\n",
    "from sqlalchemy import create_engine, and_\n",
    "from modules.database.tables import iot_export_status_table\n",
    "from modules.util.config import get_config_by_id\n",
    "\n",
    "config = get_config_by_id(CONFIG_ID)\n",
    "iot_wrapper = SAPIoTAPIWrapper(config_id=CONFIG_ID)\n",
    "engine = create_engine(config[\"database\"][\"connection\"], echo=False)\n",
    "\n",
    "DOWNLOAD_FOLDER = config[\"extract\"][\"time-series\"][\"directory\"]\n",
    "\n",
    "if not os.path.exists(DOWNLOAD_FOLDER):\n",
    "    os.makedirs(DOWNLOAD_FOLDER)\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # select all exports that are ready for download\n",
    "    query = iot_export_status_table.select().where(\n",
    "        and_(\n",
    "            iot_export_status_table.c.tenant_id == config[\"config_id\"],\n",
    "            iot_export_status_table.c.status == \"Ready for Download\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results = conn.execute(query).fetchall()\n",
    "\n",
    "    # iterate over download_data dictionary and download the data\n",
    "    for export_status in results:\n",
    "        export_status = export_status._asdict()\n",
    "\n",
    "        file_path = os.path.join(\n",
    "            DOWNLOAD_FOLDER,\n",
    "            f\"{export_status['indicator_group']}+\"\n",
    "            f\"{export_status['start_date']}+\"\n",
    "            f\"{export_status['end_date']}.zip\",\n",
    "        )\n",
    "\n",
    "        # download the data\n",
    "        iot_wrapper.download_time_series_export_sequential(\n",
    "            request_id=export_status[\"request_id\"], file_path=file_path, log=log\n",
    "        )\n",
    "\n",
    "        # iot_wrapper.download_time_series_export(\n",
    "        #     request_id=export_status[\"request_id\"], file_path=file_path\n",
    "        # )\n",
    "\n",
    "        # update the status in the iot_export_status table\n",
    "        stmt = (\n",
    "            iot_export_status_table.update()\n",
    "            .values(status=\"Downloaded\")\n",
    "            .where(\n",
    "                and_(\n",
    "                    iot_export_status_table.c.tenant_id == config[\"config_id\"],\n",
    "                    iot_export_status_table.c.indicator_group\n",
    "                    == export_status[\"indicator_group\"],\n",
    "                    iot_export_status_table.c.start_date\n",
    "                    == time_slice[0].strftime(\"%Y-%m-%d\"),\n",
    "                    iot_export_status_table.c.end_date\n",
    "                    == time_slice[1].strftime(\"%Y-%m-%d\"),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        conn.execute(stmt)\n",
    "        conn.commit()\n",
    "\n",
    "log.info(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract downloaded data\n",
    "\n",
    "When downloading the time series data we'll get a compressed zip file back. For further\n",
    "processing in the next step we need to extract the files from the archive. This be done\n",
    "in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "DOWNLOAD_FOLDER = config[\"extract\"][\"time-series\"][\"directory\"]\n",
    "\n",
    "# read all files in the download folder\n",
    "files = []\n",
    "for root, dirs, filenames in os.walk(DOWNLOAD_FOLDER):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".zip\"):\n",
    "            files.append(os.path.join(root, filename))\n",
    "\n",
    "# iterate over all files\n",
    "for file in files:\n",
    "    # get the thing type, property set type, start date and end date from the file name\n",
    "    file_name = os.path.basename(file)\n",
    "    # the files needs to be unzipped first - crete a folder with the name of the file\n",
    "    file_folder = file.replace(\".zip\", \"\")\n",
    "    if not os.path.exists(file_folder):\n",
    "        os.makedirs(file_folder)\n",
    "    # unzip the file\n",
    "    with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "        log.info(f\"unzipping {file}\")\n",
    "        zip_ref.extractall(file_folder)\n",
    "    \n",
    "        # now read all files with ending .gz in the folder where the file was unzipped\n",
    "        for root, dirs, filenames in os.walk(file_folder):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(\".gz\"):\n",
    "                    log.info(f\"unzipping {filename}\")\n",
    "                    # unzip the file\n",
    "                    with gzip.open(os.path.join(root, filename), 'rb') as f_in:\n",
    "                        with open(os.path.join(root, filename.replace(\".gz\", \"\")),'wb') as f_out:\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "                        # we can delete the gz file after unzipping\n",
    "                        os.remove(os.path.join(root, filename))\n",
    "\n",
    "\n",
    "log.info(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

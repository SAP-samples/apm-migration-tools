{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisite Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following pre-requisites are performed before the extraction of data from relevant systems.\n",
    "\n",
    "1. **Set Configuration:**\n",
    "    * Set a `CONFIG_ID` for execution.\n",
    "    * Code picks & loads configuration file (yaml) from `config` folder based on the `config_id` maintained in the file(s). \n",
    "2. **Create directories:**\n",
    "    * Pick the extraction directory maintained in configuration for Indicators\n",
    "    * Extraction data such as CSV / parquet files are staged in this directory.\n",
    "    * This data is local.\n",
    "    * A `reports` directory is also created within, where the error reports are stored.\n",
    "3. **Create database tables:**\n",
    "    * The script creates necessary tables in the database specified in the configuration.\n",
    "    * Tables are only created if they do not already exist in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------ #\n",
    "# For testing\n",
    "# ------------------------------------------------------------------------------ #\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import concurrent.futures\n",
    "from modules.util.helpers import Logger\n",
    "from modules.util.config import get_config_by_id\n",
    "from modules.pai.alerts import AlertsAPIWrapper \n",
    "from modules.acf.alerttypes import AlertTypeAPIWrapper \n",
    "from modules.util.database import SQLAlchemyClient\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Configurations\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "CONFIG_ID = 'dca-test'\n",
    "\n",
    "EXTRACTION_DIR = get_config_by_id(CONFIG_ID)[\"extract\"][\"alerts\"][\"directory\"]\n",
    "DATA_DIR = f\"{EXTRACTION_DIR}/data\"\n",
    "ERROR_DIR = f\"{EXTRACTION_DIR}/errors\"\n",
    "UDR_DIR = f\"{EXTRACTION_DIR}/udr\"\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Directories\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "if not os.path.exists(ERROR_DIR):\n",
    "    os.makedirs(ERROR_DIR)\n",
    "\n",
    "if not os.path.exists(UDR_DIR):\n",
    "    os.makedirs(UDR_DIR)\n",
    "\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "log.info(\"** EXTRACT - ALERTS **\")\n",
    "Logger.blank_line(log)\n",
    "log.info(f\"Configuration ID: {CONFIG_ID}\")\n",
    "log.info(f\"Extraction directory: {EXTRACTION_DIR}\")\n",
    "log.info(f\"Data directory: {DATA_DIR}\")\n",
    "log.info(f\"Error directory: {ERROR_DIR}\")\n",
    "\n",
    "\n",
    "file_pai_alerts = rf'{DATA_DIR}/Alerts.parquet'\n",
    "file_alerttypes = rf'{ERROR_DIR}/Alerttypes.csv'\n",
    "file_alerts = rf'{ERROR_DIR}/Alerts.csv'\n",
    "UDR_Alerttypes = rf'{UDR_DIR}/UDR_Alerttypes.csv'\n",
    "UDR_Alerts = rf'{UDR_DIR}/UDR_Alerts.csv'\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Database\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "log.info(f\"Connecting to database\")\n",
    "db = SQLAlchemyClient(config_id=CONFIG_ID)\n",
    "log.info(f\"Connected to {db.get_database_url()}\")\n",
    "\n",
    "log.info(f\"Creating all tables\")\n",
    "db.table_create_all()\n",
    "log.info(f\"Tables created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Extract PAI Alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alerts for both Technical Objects (Equipments and Functional Locations) will be extracted.\n",
    "* Storing in a dataframe df_alerts for filtering/transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Importing AlertAPIWrapper from alerts.py\n",
    "alert_api = AlertsAPIWrapper(config_id=CONFIG_ID)\n",
    "alert_count = alert_api.getCount()\n",
    "log.info(f\"{alert_count} alerts found\")\n",
    "\n",
    "\n",
    "# Function to fetch alerts for a given skip value\n",
    "def fetch_alerts_chunk(skip):\n",
    "    return alert_api.getAlerts(skip=skip, top=100)\n",
    "\n",
    "# Fetch the alerts using threading\n",
    "alerts = []\n",
    "if alert_count > 0:\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(fetch_alerts_chunk, skip) for skip in range(0, alert_count + 100, 100)]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                alerts.extend(future.result())\n",
    "            except Exception as e:\n",
    "                log.error(f\"Failed to fetch alerts chunk: {e}\")\n",
    "\n",
    "# Convert alerts to DataFrame and save to parquet\n",
    "if alerts:\n",
    "    df_alerts = pd.DataFrame(alerts)\n",
    "    df_alerts.to_parquet(file_pai_alerts, index=False)\n",
    "    log.info(f\"Alerts saved to {file_pai_alerts}\")\n",
    "else:\n",
    "    log.error(\"No alerts to save\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\n",
    "     \"__metadata\"\n",
    "]\n",
    "\n",
    "df_alerts = df_alerts.drop(columns=columns_to_remove)\n",
    "df_alerts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staging the PAI system Alerts\n",
    "\n",
    "* Above data is staged in the coreresponding table in database: T_PAI_ALERTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Stage Alerts in DB\n",
    "from modules.util.database import SQLAlchemyClient, PaiAlerts\n",
    "\n",
    "log.info(f\"Connecting to database\")\n",
    "db = SQLAlchemyClient(config_id=CONFIG_ID)\n",
    "log.info(f\"Connected to {db.get_database_url()}\")\n",
    "\n",
    "log.info(f\"Creating {PaiAlerts.__tablename__} table\")\n",
    "db.table_create_all()\n",
    "\n",
    "log.info(f\"Updated {PaiAlerts.__tablename__} with {db.count(PaiAlerts)} records\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "log.info(f\"Updating {PaiAlerts.__tablename__}\")\n",
    "log.info(f\"Drop and Reload configuration is set to {db.drop_reload}\")\n",
    "if db.drop_reload:\n",
    "    log.info(f\"Clearing data from {PaiAlerts.__tablename__}\")\n",
    "    db.truncate(PaiAlerts)  # truncate all existing data (remove if needed)\n",
    "pai_alerts = SQLAlchemyClient.dataframe_to_object(df_alerts, PaiAlerts)\n",
    "db.insert_batches(data=pai_alerts)  # insert the new PAI alerts\n",
    "\n",
    "log.info(f\"Updated {PaiAlerts.__tablename__} with {db.count(PaiAlerts)} records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Filter PAI Alerts for Valid Technical Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Filter the equipments and functional locations, in the PAI Alerts, that are present in the system.\n",
    "* Using view V_Transform_Indicators that contains the valid TO's for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import V_Transform_Indicators, SQLAlchemyClient\n",
    "\n",
    "log.info(f\"Connecting to database\")\n",
    "db = SQLAlchemyClient(config_id=CONFIG_ID)\n",
    "log.info(f\"Connected to {db.get_database_url()}\")\n",
    "\n",
    "log.info(f\"Reading {V_Transform_Indicators.__tablename__}\")\n",
    "transform_indicators = db.select(\n",
    "    model=V_Transform_Indicators,\n",
    "    distinct=True\n",
    ")\n",
    "log.info(f\"{len(transform_indicators)} transform indicators fetched.\")\n",
    "df_transform_indicators = pd.DataFrame(transform_indicators)\n",
    "df_transform_indicators.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All possible equipment ids/ floc ids that are in final transformed list of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if EquipmentID in df_alerts are present in df_transform_indicators['id']\n",
    "equipment_ids_in_transform = df_alerts['EquipmentID'].isin(df_transform_indicators['id'])\n",
    "num_equipment_ids_in_transform = equipment_ids_in_transform.sum()\n",
    "log.info(f\"Number of EquipmentID in df_alerts present in df_transform_indicators: {num_equipment_ids_in_transform}\")\n",
    "\n",
    "# Check if FunctionalLocationID in df_alerts are present in df_transform_indicators['id']\n",
    "functional_location_ids_in_transform = df_alerts['FunctionalLocationID'].isin(df_transform_indicators['id'])\n",
    "num_functional_location_ids_in_transform = functional_location_ids_in_transform.sum()\n",
    "log.info(f\"Number of FunctionalLocationID in df_alerts present in df_transform_indicators: {num_functional_location_ids_in_transform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the df_transform_indicators dataframe to get the valid equipment IDs and functional location IDs\n",
    "filtered_df = df_transform_indicators[\n",
    "    (df_transform_indicators['id'].isin(df_alerts['EquipmentID'])) |\n",
    "    (df_transform_indicators['id'].isin(df_alerts['FunctionalLocationID']))\n",
    "]\n",
    "\n",
    "# Display the filtered dataframe\n",
    "filtered_df.head()\n",
    "\n",
    "# Create a new dataframe with the valid equipment IDs from df_alerts\n",
    "valid_alert_TO = df_alerts[\n",
    "    (df_alerts['EquipmentID'].isin(filtered_df['id'])) |\n",
    "    (df_alerts['FunctionalLocationID'].isin(filtered_df['id']))\n",
    "]\n",
    "\n",
    "# Display the new dataframe\n",
    "valid_alert_TO.reset_index(drop=True, inplace=True)\n",
    "\n",
    "valid_alert_TO.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the available alerttypes to migrate the alerttypes corresponding to the valid TO's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from modules.acf.alerttypes import AlertTypeAPIWrapper\n",
    "\n",
    "alert_api = AlertTypeAPIWrapper(config_id=CONFIG_ID)\n",
    "alerttypes = alert_api.getAlerttypes()\n",
    "log.info(f\"{len(alerttypes)} alert types fetched.\")\n",
    "df_alerttypes = pd.DataFrame(alerttypes)\n",
    "df_alerttypes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import SQLAlchemyClient,PaiAlertTypes\n",
    "\n",
    "# Step 2 : Stage Alerts in DB\n",
    "\n",
    "log.info(f\"Connecting to database\")\n",
    "db = SQLAlchemyClient(config_id=CONFIG_ID)\n",
    "log.info(f\"Connected to {db.get_database_url()}\")\n",
    "\n",
    "log.info(f\"Updated {PaiAlertTypes.__tablename__} with {db.count(PaiAlertTypes)} records\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "log.info(f\"tUpdating {PaiAlertTypes.__tablename__}\")\n",
    "log.info(f\"Drop and Reload configuration is set to {db.drop_reload}\")\n",
    "if db.drop_reload:\n",
    "    log.warning(f\"Clearing data from {PaiAlertTypes.__tablename__}\")\n",
    "    db.truncate(PaiAlertTypes)  # truncate all existing data (remove if needed)\n",
    "pai_alerttypes = SQLAlchemyClient.dataframe_to_object(df_alerttypes, PaiAlertTypes)\n",
    "db.insert_batches(data=pai_alerttypes)  # insert the new PAI alerts\n",
    "\n",
    "log.info(f\"Updated {PaiAlertTypes.__tablename__} with {db.count(PaiAlertTypes)} records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract the alerts whose TO id's are in the validated ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_at = valid_alert_TO['AlertType'].unique()\n",
    "df_alerttypes = df_alerttypes[df_alerttypes['alertTypeID'].isin(required_at)]\n",
    "df_alerttypes.reset_index(drop=True, inplace=True)\n",
    "df_alerttypes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3a - Staging the Alerttypes to APM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_alerttypes = df_alerttypes[['alertTypeID', 'category', 'alertDescription', 'severity', 'severityDescription']]\n",
    "df_post_alerttypes = df_post_alerttypes.rename(columns={\n",
    "    'alertTypeID': 'Name',\n",
    "    'category': 'Category',\n",
    "    'alertDescription': 'Description',\n",
    "    'severity': 'DefaultSeverity',\n",
    "    'severityDescription': 'DefaultSeverityDescription'\n",
    "})\n",
    "df_post_alerttypes['Source'] = 'API'\n",
    "\n",
    "\n",
    "\n",
    "# # Perform the mapping check\n",
    "# df_post_alerttypes['DefaultSeverity'] = df_post_alerttypes.apply(\n",
    "#     lambda row: 5 if row['DefaultSeverity'] == '2' else (10 if row['DefaultSeverity'] == '3' else row['DefaultSeverity']),\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# Perform the mapping check\n",
    "# Check for incorrect severity values\n",
    "incorrect_severity_values = df_post_alerttypes[~df_post_alerttypes['DefaultSeverity'].astype(str).isin(['1', '2', '3', '5', '10'])]\n",
    "\n",
    "if not incorrect_severity_values.empty:\n",
    "    incorrect_severity_file = os.path.join(ERROR_DIR, 'Incorrect_Alert_Types.csv')\n",
    "    incorrect_severity_values.to_csv(incorrect_severity_file, index=False)\n",
    "    log.error(f\"Incorrect severity values found. Details saved to {incorrect_severity_file}\")\n",
    "else:\n",
    "    df_post_alerttypes['DefaultSeverity'] = df_post_alerttypes.apply(\n",
    "        lambda row: 5 if row['DefaultSeverity'] == '2' else (10 if row['DefaultSeverity'] == '3' else row['DefaultSeverity']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "df_post_alerttypes['DefaultSeverity'] = df_post_alerttypes['DefaultSeverity'].astype('int32')\n",
    "\n",
    "# Enabling Deduplication\n",
    "\n",
    "df_post_alerttypes['DeduplicationPeriod'] = ''\n",
    "df_post_alerttypes['DeduplicationIsEnabled'] = ''\n",
    "\n",
    "# Save to CSV\n",
    "df_post_alerttypes.to_csv(UDR_Alerttypes, index=False)\n",
    "log.info(f\"Alert types saved as UDR in {UDR_Alerttypes}\")\n",
    "\n",
    "df_post_alerttypes.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the UDR after user has provided inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_alerttypes = pd.read_csv(UDR_Alerttypes)\n",
    "\n",
    "# Reorder columns to place 'Source' after 'Category'\n",
    "df_post_alerttypes = df_post_alerttypes[['Name', 'Category', 'Source', 'Description', 'DefaultSeverity', 'DefaultSeverityDescription', 'DeduplicationPeriod', 'DeduplicationIsEnabled']]\n",
    "df_post_alerttypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import SQLAlchemyClient, ApmAlertTypes\n",
    "\n",
    "log.info(f\"Connecting to database\")\n",
    "db = SQLAlchemyClient(config_id=CONFIG_ID)\n",
    "log.info(f\"Connected to {db.get_database_url()}\")\n",
    "\n",
    "log.info(f\"Updated {ApmAlertTypes.__tablename__} with {db.count(ApmAlertTypes)} records\")\n",
    "log.info(\"\")  # better readability\n",
    "\n",
    "log.info(f\"Updating {ApmAlertTypes.__tablename__}\")\n",
    "log.info(f\"Drop and Reload configuration is set to {db.drop_reload}\")\n",
    "if db.drop_reload:\n",
    "    log.warning(f\"Clearing data from {ApmAlertTypes.__tablename__}\")\n",
    "    db.truncate(ApmAlertTypes)  # truncate all existing data (remove if needed)\n",
    "pai_alerttypes = SQLAlchemyClient.dataframe_to_object(df_post_alerttypes, ApmAlertTypes)\n",
    "db.insert_batches(data=pai_alerttypes)  # insert the new PAI alerts\n",
    "\n",
    "log.info(f\"Updated {ApmAlertTypes.__tablename__} with {db.count(ApmAlertTypes)} records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post the alerttypes to APM\n",
    "\n",
    "* To keep track of the alert types migrated, stage the alert types that are migrated  in another table \"Post AlertTypes\"\n",
    "* If successful, the error message and error code will be None, else capture them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.apm.manage_alerts import APMAlertAPIWrapper\n",
    "from modules.util.database import SQLAlchemyClient, PostAlertTypes\n",
    "import json\n",
    "import requests\n",
    "\n",
    "alert_api = APMAlertAPIWrapper(config_id=CONFIG_ID)\n",
    "if db.drop_reload:\n",
    "    log.info(f\"Clearing data from {PostAlertTypes.__tablename__}\")\n",
    "    db.truncate(PostAlertTypes)  # truncate all existing data (remove if needed)\n",
    "for index, row in df_post_alerttypes.iterrows():\n",
    "    alert_type_data = {\n",
    "        \"name\": row['Name'],\n",
    "        \"category\": str(row['Category']),\n",
    "        \"source\": row['Source'],\n",
    "        \"description\": row['Description'],\n",
    "        \"default_severity\": int(row['DefaultSeverity']),\n",
    "        \"default_severity_description\": row['DefaultSeverityDescription']\n",
    "        }\n",
    "    print([alert_type_data['category']])\n",
    "    if row['DeduplicationPeriod'] and row['DeduplicationIsEnabled']:\n",
    "        \n",
    "        alert_type_data['deduplication_period'] = row['DeduplicationPeriod']\n",
    "        alert_type_data['deduplication_is_enabled'] = bool(row['DeduplicationIsEnabled'])\n",
    "        \n",
    "        try:\n",
    "            response = alert_api.postAlerttype(**alert_type_data)\n",
    "            current_row_df = row.to_frame().T\n",
    "            current_row_df['ErrorMessage'] = None\n",
    "            current_row_df['ErrorCode'] = None\n",
    "            posting_alerttype = SQLAlchemyClient.dataframe_to_object(current_row_df, PostAlertTypes)\n",
    "            db.insert_batches(data=posting_alerttype)\n",
    "            log.info(f\"Response: {response}\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            log.warning(f\"ERROR:\\tFailed to post alert type: {alert_type_data}\")\n",
    "            log.warning(f\"ERROR:\\t{e}\")\n",
    "            current_row_df = row.to_frame().T\n",
    "            current_row_df['ErrorMessage'] = e.response.text\n",
    "            current_row_df['ErrorCode'] = e.response.status_code\n",
    "            posting_alerttype = SQLAlchemyClient.dataframe_to_object(current_row_df, PostAlertTypes)\n",
    "            db.insert_batches(data=posting_alerttype)\n",
    "    else:\n",
    "        try:\n",
    "            response = alert_api.postAlerttype(**alert_type_data)\n",
    "            current_row_df = row.to_frame().T\n",
    "            current_row_df['ErrorMessage'] = None\n",
    "            current_row_df['ErrorCode'] = None\n",
    "            posting_alerttype = SQLAlchemyClient.dataframe_to_object(current_row_df, PostAlertTypes)\n",
    "            db.insert_batches(data=posting_alerttype)\n",
    "            log.info(f\"Response: {response}\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            log.warning(f\"ERROR:\\tFailed to post alert type: {alert_type_data}\")\n",
    "            log.warning(f\"ERROR:\\t{e}\")\n",
    "            current_row_df = row.to_frame().T\n",
    "            current_row_df['ErrorMessage'] = e.response.text\n",
    "            current_row_df['ErrorCode'] = e.response.status_code\n",
    "            posting_alerttype = SQLAlchemyClient.dataframe_to_object(current_row_df, PostAlertTypes)\n",
    "            db.insert_batches(data=posting_alerttype) \n",
    "      \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an error report for alerttypes from collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import SQLAlchemyClient, V_APM_AlertType\n",
    "valid_alerttype = db.select(model = V_APM_AlertType, distinct = True, where=[V_APM_AlertType.valid !='X'])\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_valid_alerttype = pd.DataFrame(valid_alerttype)\n",
    "\n",
    "if(len(df_valid_alerttype) == 0):\n",
    "    log.info(\"No invalid alert types found\")\n",
    "     # Display the resulting DataFrame\n",
    "    df_valid_alerttype.to_csv(file_alerttypes)\n",
    "    print(f\"INFO:\\tAlerttypes saved to {file_alerttypes}\")\n",
    "else:\n",
    "    # Drop the specified columns\n",
    "    df_valid_alerttype = df_valid_alerttype.drop(columns=['idx', 'tenantid', 'valid'])\n",
    "\n",
    "    # Display the resulting DataFrame\n",
    "    df_valid_alerttype.to_csv(file_alerttypes)\n",
    "    print(f\"INFO:\\tAlerttypes saved to {file_alerttypes}\")\n",
    "\n",
    "    df_valid_alerttype.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3b - Staging the Alerts to APM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Filter the alerts based on the valid alert types\n",
    "df_alerts_filtered = df_alerts[df_alerts['AlertType'].isin(df_alerttypes['alertTypeID'])]\n",
    "df_alerts_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 2: Filter the df_transform_indicators based on the condition\n",
    "filtered_TO_df = df_transform_indicators[\n",
    "    df_transform_indicators['id'].isin(df_alerts_filtered['EquipmentID']) |\n",
    "    df_transform_indicators['id'].isin(df_alerts_filtered['FunctionalLocationID'])\n",
    "]\n",
    "\n",
    "# Step 3: Merge the filtered DataFrame with testing_alerts on EquipmentID\n",
    "merged_df_equipment = filtered_TO_df.merge(\n",
    "    df_alerts_filtered,\n",
    "    left_on='id',\n",
    "    right_on='EquipmentID',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Step 4: Merge the filtered DataFrame with testing_alerts on FunctionalLocationID\n",
    "merged_df_functional = filtered_TO_df.merge(\n",
    "    df_alerts_filtered,\n",
    "    left_on='id',\n",
    "    right_on='FunctionalLocationID',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Concatenate the two merged DataFrames\n",
    "merged_df = pd.concat([merged_df_equipment, merged_df_functional], ignore_index=True)\n",
    "\n",
    "# Step 5: Convert the date columns to datetime\n",
    "\n",
    "#Function to convert the date string to the required format\n",
    "def convert_date(date_str):\n",
    "    # Extract the timestamp in milliseconds\n",
    "    timestamp_ms = int(date_str[6:-2])\n",
    "    # Convert to datetime, then format as required\n",
    "    datetime_obj = pd.to_datetime(timestamp_ms, unit='ms')\n",
    "    return datetime_obj.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-4] + 'Z'\n",
    "\n",
    "# Apply the function to the date_column\n",
    "merged_df['TriggeredOn'] = merged_df['TriggeredOn'].apply(convert_date)\n",
    "merged_df['LastOccuredOn'] = merged_df['LastOccuredOn'].apply(convert_date)\n",
    "\n",
    "# Step 6: Combine EquipmentID and FunctionalLocationID into a single column\n",
    "\n",
    "# Create the new column 'TechnicalObject'\n",
    "merged_df['TechnicalObject'] = np.where(\n",
    "    merged_df['EquipmentID'].notna() & (merged_df['EquipmentID'] != ''),\n",
    "    merged_df['EquipmentID'],\n",
    "    merged_df['FunctionalLocationID']\n",
    ")\n",
    "\n",
    "# Drop the original columns if they're no longer needed\n",
    "merged_df = merged_df.drop(columns=['EquipmentID', 'FunctionalLocationID'])\n",
    "\n",
    "# Step 7: Drop the columns which are not required\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['internalId', 'type', 'externalId','AlertType','TechnicalObject','TriggeredOn']\n",
    "\n",
    "# Select only these columns\n",
    "merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "merged_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data as per the structure required to post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Group and explode to get individual rows for each externalId\n",
    "grouped_alerts = merged_df.groupby(['TriggeredOn', 'AlertType', 'type']).agg({'externalId': list}).reset_index()\n",
    "expanded_alerts = grouped_alerts.explode('externalId').reset_index(drop=True)\n",
    "\n",
    "# Step 2: Create the TechnicalObject dictionary for each row\n",
    "expanded_alerts['TechnicalObject'] = expanded_alerts.apply(\n",
    "    lambda row: {\n",
    "        'Number': row['externalId'],\n",
    "        'SSID': 'QM7_910',\n",
    "        'Type': 'EQUI' if row['type'] == 'EQU' else 'FLOC'\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 3: Group by 'TriggeredOn' and 'AlertType' to aggregate distinct TechnicalObject dictionaries into a list\n",
    "grouped_df = expanded_alerts.groupby(['TriggeredOn', 'AlertType']).agg({\n",
    "    'TechnicalObject': lambda x: list({v['Number']: v for v in x}.values())  # Aggregates distinct dictionaries into a list for each unique (TriggeredOn, AlertType) pair\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "# Step 3: Select the columns you need in the final DataFrame\n",
    "final_df = grouped_df[['TriggeredOn', 'AlertType', 'TechnicalObject']].copy()\n",
    "\n",
    "# Step 4: Convert TechnicalObject column to JSON string if needed\n",
    "final_df['TechnicalObject'] = final_df['TechnicalObject'].apply(json.dumps)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage the data for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import SQLAlchemyClient, ApmAlerts\n",
    "\n",
    "# Step 2 : Stage Alerts in DB\n",
    "\n",
    "log.info(f\"Connecting to database\")\n",
    "db = SQLAlchemyClient(config_id=CONFIG_ID)\n",
    "log.info(f\"Connected to {db.get_database_url()}\")\n",
    "\n",
    "log.info(f\"Creating {ApmAlerts.__tablename__} table\")\n",
    "db.table_create_all()\n",
    "\n",
    "log.info(f\"Updated {ApmAlerts.__tablename__} with {db.count(ApmAlerts)} records\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "log.info(f\"Updating {ApmAlerts.__tablename__}\")\n",
    "log.info(f\"Drop and Reload configuration is set to {db.drop_reload}\")\n",
    "if db.drop_reload:\n",
    "    log.warning(f\"Clearing data from {ApmAlerts.__tablename__}\")\n",
    "    db.truncate(ApmAlerts)  # truncate all existing data (remove if needed)\n",
    "Apmalerts = SQLAlchemyClient.dataframe_to_object(final_df, ApmAlerts)\n",
    "db.insert_batches(data=Apmalerts)  # insert the new PAI alerts\n",
    "\n",
    "log.info(f\"Updated {ApmAlerts.__tablename__} with {db.count(ApmAlerts)} records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post the alerts to APM\n",
    "\n",
    "* To keep track of the alerts migrated, stage the alerts that are migrated  in another table \"Post Alerts\"\n",
    "* If successful, the error message and error code will be None, wlse capture them as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate a UDR for Alerts\n",
    "\n",
    "* A UDR will be published for the users to fill in the Deduplication count. This information can be added to the api call to keep track of the alerts of that corresponding Alerttype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_post_alerts = db.select(ApmAlerts)\n",
    "to_post_alerts_df = pd.DataFrame(to_post_alerts)\n",
    "to_post_alerts_df['DeduplicationCount'] = None\n",
    "to_post_alerts_df.to_csv(UDR_Alerts, index=False)\n",
    "\n",
    "log.info(f\"Posted Alerts saved as UDR in {UDR_Alerts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After the UDR is filled with required information, we post the data to APM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import SQLAlchemyClient, PostAlerts\n",
    "import json\n",
    "import requests\n",
    "\n",
    "post_alerts_api = APMAlertAPIWrapper(config_id=CONFIG_ID)\n",
    "if db.drop_reload:\n",
    "    log.warning(f\"Clearing data from {PostAlerts\n",
    "    .__tablename__}\")\n",
    "    db.truncate(PostAlerts)\n",
    "\n",
    "for index, row in final_df.iterrows():\n",
    "    alert_data = {\n",
    "        \"alert_type\": row['AlertType'],\n",
    "        \"triggered_on\": row['TriggeredOn'],\n",
    "        \"technical_objects\": json.loads(row['TechnicalObject'])\n",
    "    }\n",
    "    try:\n",
    "        response = post_alerts_api.postAlert(**alert_data)\n",
    "        current_row_df = row.to_frame().T\n",
    "        current_row_df['ErrorMessage'] = None\n",
    "        current_row_df['ErrorCode'] = None\n",
    "        posting_alert = SQLAlchemyClient.dataframe_to_object(current_row_df, PostAlerts)\n",
    "        db.insert_batches(data=posting_alert)\n",
    "        log.info(f\"Response: {response}\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        log.error(f\"ERROR:\\tFailed to post alert: {alert_data}\")\n",
    "        log.error(f\"ERROR:\\t{e}\")\n",
    "        current_row_df = row.to_frame().T\n",
    "        current_row_df['ErrorMessage'] = e.response.text\n",
    "        current_row_df['ErrorCode'] = e.response.status_code\n",
    "        posting_alert = SQLAlchemyClient.dataframe_to_object(current_row_df, PostAlerts)\n",
    "        db.insert_batches(data=posting_alert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an error report with collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util.database import SQLAlchemyClient, V_APM_Alerts\n",
    "\n",
    "error_alerts = db.select(model=V_APM_Alerts,where=[V_APM_Alerts.valid != 'X'])\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_error_alerts = pd.DataFrame(error_alerts)\n",
    "df_error_alerts.to_csv(file_alerts)\n",
    "log.info(f\"Alerts saved to {file_alerts}\")\n",
    "\n",
    "df_error_alerts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP : Extracting Alerts from APM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modules.apm.manage_alerts import APM_AlertAPIWrapper  # Importing AlertAPIWrapper from alerts.py\n",
    "# import pandas as pd\n",
    "\n",
    "# file_2 = r'../migration-data/alerts-download/2_apm_alerts.csv'\n",
    "# file_3 = r'../migration-data/alerts-download/3_alert_types.csv'\n",
    "\n",
    "# CONFIG_ID = 'aspm-test'\n",
    "# alert_api = APM_AlertAPIWrapper(config_id=CONFIG_ID)\n",
    "# print(f\"INFO:\\t{alert_api.get_apm_alert_count()} alerts found\")\n",
    "\n",
    "# alerts = alert_api.get_apm_alerts()\n",
    "\n",
    "# # Normalize the alerts data\n",
    "# df = pd.json_normalize(alerts, sep='_')\n",
    "\n",
    "# # Further normalize the 'value' column\n",
    "# value_normalized = pd.json_normalize(df['value'].explode().tolist(), sep='_')\n",
    "\n",
    "# # Concatenate the normalized 'value' data with the original dataframe\n",
    "# df = pd.concat([df.drop(columns=['value']).loc[df.index.repeat(df['value'].str.len())].reset_index(drop=True), value_normalized], axis=1)\n",
    "\n",
    "# # Further normalize 'Technical Objects' into 3 columns with placeholders\n",
    "# technical_objects_normalized = pd.json_normalize(df['TechnicalObject'].explode().dropna().tolist(), sep='_')\n",
    "# technical_objects_normalized.columns = ['TechnicalObject_' + col for col in technical_objects_normalized.columns]\n",
    "\n",
    "# # Concatenate the normalized 'Technical Objects' data with the original dataframe\n",
    "# df = pd.concat([df.drop(columns=['TechnicalObject']).loc[df.index.repeat(df['TechnicalObject'].str.len())].reset_index(drop=True), technical_objects_normalized], axis=1)\n",
    "\n",
    "# print(f\"INFO:\\t{len(df)} alerts extracted\")\n",
    "\n",
    "# df.to_csv(file_2, index=False)\n",
    "# print(f'INFO:\\t{file_2} generated')\n",
    "\n",
    "# # Drop off the columns that are not needed\n",
    "# df = df.drop(columns=['@context', '@metadataEtag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modules.util.sqlalchemy import SQLAlchemyClient, Alerts\n",
    "\n",
    "# # Step 2 : Stage Alerts in DB\n",
    "\n",
    "# print(f\"INFO:\\tConnecting to database\")\n",
    "# db = SQLAlchemyClient(config_id=CONFIG_ID)\n",
    "# print(f\"INFO:\\tConnected to {db.get_database_url()}\")\n",
    "# print(f\"INFO:\\tUpdating {Alerts.__tablename__}\")\n",
    "\n",
    "# print(f\"WARN:\\tClearing data from {Alerts.__tablename__}\")\n",
    "# db.table_create_all()\n",
    "# db.truncate(Alerts)  # truncate all existing data (remove if needed)\n",
    "\n",
    "# alerts_data = []\n",
    "# for index, row in df.iterrows():\n",
    "#     row_data = row.to_dict()\n",
    "#     alert = Alerts(**row_data)\n",
    "#     alerts_data.append(alert)\n",
    "\n",
    "# db.insert_batches(data=alerts_data)  # insert the new alerts\n",
    "# print(f\"INFO:\\tUpdated {Alerts.__tablename__} with {db.count(Alerts)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetch alert types\n",
    "# alert_types = alert_api.get_apm_alerttypes()\n",
    "\n",
    "# # Normalize the alert types data\n",
    "# df_alert_types = pd.json_normalize(alert_types, sep='_')\n",
    "\n",
    "# # Ensure the entire file is extracted and stored\n",
    "# df_alert_types.to_csv(file_3, index=False)\n",
    "# print(f'INFO:\\t{file_3} generated')\n",
    "\n",
    "# # Remove @context and @metadataEtag columns\n",
    "# df_alert_types = df_alert_types.drop(columns=['@context', '@metadataEtag'])\n",
    "\n",
    "# # Split the 'value' column into separate columns\n",
    "# value_normalized = pd.json_normalize(df_alert_types['value'].explode().tolist(), sep='_')\n",
    "\n",
    "# # Concatenate the normalized 'value' data with the original dataframe\n",
    "# df_alerttype = pd.concat([df_alert_types.drop(columns=['value']).loc[df_alert_types.index.repeat(df_alert_types['value'].str.len())].reset_index(drop=True), value_normalized], axis=1)\n",
    "\n",
    "# # Display the alert types dataframe\n",
    "# print(f\"INFO:\\t{len(df_alerttype)} alert types extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modules.util.sqlalchemy import  AlertTypes\n",
    "\n",
    "# # Step 3: Stage Alert Types in DB\n",
    "\n",
    "# print(f\"INFO:\\tUpdating {AlertTypes.__tablename__} with alert types\")\n",
    "\n",
    "# print(f\"WARN:\\tClearing data from {Alerts.__tablename__}\")\n",
    "# db.table_create_all()\n",
    "# db.truncate(AlertTypes)  # truncate all existing data (remove if needed)\n",
    "\n",
    "# alert_types_data = []\n",
    "# for index, row in df_alerttype.iterrows():\n",
    "#     row_data = row.to_dict()\n",
    "#     alert_type = AlertTypes(**row_data)\n",
    "#     alert_types_data.append(alert_type)\n",
    "\n",
    "# db.insert_batches(data=alert_types_data)  # insert the new alert types\n",
    "# print(f\"INFO:\\tUpdated {AlertTypes.__tablename__} with {db.count(AlertTypes)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group the dataframe by 'Id' and aggregate the technical objects\n",
    "# grouped_df = df.groupby('Id').agg({\n",
    "#     'AlertType': 'first',\n",
    "#     'TriggeredOn': 'first',\n",
    "#     'TechnicalObject_Number': list,\n",
    "#     'TechnicalObject_Type': list\n",
    "# }).reset_index()\n",
    "\n",
    "# # grouped_df[grouped_df['Id'] == '2d11181c-114a-4657-81b3-229f88425f6f']\n",
    "\n",
    "# ssid = \"QM7_910\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate through the alert types dataframe and post each alert type\n",
    "# for _, row in df_alerttype.iterrows():\n",
    "#     alert_type_data = {\n",
    "#         \"Name\": row['Name'],\n",
    "#         \"Category\": row['Category'],\n",
    "#         \"CategoryDescription\": row['CategoryDescription'],\n",
    "#         \"Source\": row['Source'],\n",
    "#         \"Description\": row['Description'],\n",
    "#         \"DefaultSeverity\": row['DefaultSeverity'],\n",
    "#         \"DefaultSeverityDescription\": row['DefaultSeverityDescription'],\n",
    "#         \"DeduplicationPeriod\": row['DeduplicationPeriod'],\n",
    "#         \"DeduplicationIsEnabled\": row['DeduplicationIsEnabled']\n",
    "#     }\n",
    "    \n",
    "#     # Call the post_alerttype method\n",
    "#     response = alert_api.post_alerttype(\n",
    "#         name=row['Name'],\n",
    "#         category=row['Category'],\n",
    "#         category_description=row['CategoryDescription'],\n",
    "#         source=row['Source'],\n",
    "#         description=row['Description'],\n",
    "#         default_severity=row['DefaultSeverity'],\n",
    "#         default_severity_description=row['DefaultSeverityDescription'],\n",
    "#         deduplication_period=row['DeduplicationPeriod'],\n",
    "#         deduplication_is_enabled=row['DeduplicationIsEnabled']\n",
    "#     )\n",
    "    \n",
    "#     # Print the response for debugging\n",
    "#     print(response)\n",
    "\n",
    "# print(f\"INFO:\\t{len(df_alerttype)} alert types posted in the system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group the dataframe by 'Id' and aggregate the technical objects\n",
    "# grouped_df = df.groupby('Id').agg({\n",
    "#     'AlertType': 'first',\n",
    "#     'TriggeredOn': 'first',\n",
    "#     'TechnicalObject_Number': list,\n",
    "#     'TechnicalObject_Type': list\n",
    "# }).reset_index()\n",
    "\n",
    "# # grouped_df[grouped_df['Id'] == '2d11181c-114a-4657-81b3-229f88425f6f']\n",
    "\n",
    "# ssid = \"QM7_910\"\n",
    "\n",
    "# # Iterate through the grouped dataframe and prepare the technical objects list\n",
    "# for _, row in grouped_df.iterrows():\n",
    "#     technical_objects = [\n",
    "#         {\n",
    "#             \"Number\": number,\n",
    "#             \"SSID\": ssid,\n",
    "#             \"Type\": type_\n",
    "#         }\n",
    "#         for number, type_ in zip(row['TechnicalObject_Number'], row['TechnicalObject_Type'])\n",
    "#     ]\n",
    "#     # Call the post_alert method\n",
    "#     response = alert_api.post_alert(\n",
    "#         alert_type=row['AlertType'],\n",
    "#         triggered_on=row['TriggeredOn'],\n",
    "#         technical_objects=technical_objects\n",
    "#     )\n",
    "    \n",
    "#     # Print the response for debugging\n",
    "#     print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

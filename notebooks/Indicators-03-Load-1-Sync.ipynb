{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-requisite Configuration\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# For testing\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "from modules.util.config import get_config_by_id, get_config_global\n",
    "from modules.util.database import SQLAlchemyClient\n",
    "from modules.util.helpers import Logger\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "CONFIG_ID = 'dca-test'\n",
    "TRANSFORM = get_config_by_id(CONFIG_ID)[\"load\"][\"indicator\"]\n",
    "EXTRACTION_DIR = TRANSFORM[\"directory\"]\n",
    "REPORTS_DIR = f\"{EXTRACTION_DIR}/reports\"\n",
    "CONFIG_GLOBAL = get_config_global().get('indicators').get('transform')\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Create directories\n",
    "# ------------------------------------------------------------------------------ #\n",
    "if not os.path.exists(REPORTS_DIR):\n",
    "    os.makedirs(REPORTS_DIR)\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Logger\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"** LOAD - INDICATORS **\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "log.info(f\"Extraction Directory: {EXTRACTION_DIR}\")\n",
    "log.info(f\"Reports Directory: {REPORTS_DIR}\")\n",
    "\n",
    "db = SQLAlchemyClient(CONFIG_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Sync Characteristics with ERP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ERP characteristics are derived from the *user decision report*.\n",
    "\n",
    "* The migration tool proposes the ERP characteristics, but user has an option to override the proposal by providing an input.\n",
    "\n",
    "* The view `V_ERPCharacteristics` contains the list of unique characteristics derived from above.\n",
    "\n",
    "* Characteristics are **mandatory** to be present (or) created in the ERP system before migrating the indicators.\n",
    "\n",
    "**Process:**\n",
    "1. We check the ERP system `[GET]` for characteristics, based on the characteristic name (upper case)\n",
    "\n",
    "2. If the value already exists in the ERP system, we derive the `CharacInternalID` of the characteristic from the system.\n",
    "\n",
    "3. In case if the record does not exist, a new characteristic is created `[POST]` in the ERP system \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERP Characteristics\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# standard imports\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# custom imports\n",
    "from modules.util.helpers import convert_dataframe\n",
    "from modules.erp.s4_clfn_characteristic_srv import ApiCharacteristicHeader\n",
    "from modules.util.api import APIException\n",
    "from modules.util.database import (\n",
    "    SQLAlchemyClient,\n",
    "    V_ERPCharacteristics,\n",
    "    ERPCharacteristics,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "file_tgt = rf'{EXTRACTION_DIR}/1_ERP_Characteristics.parquet'\n",
    "file_err = rf'{REPORTS_DIR}/1_ERP_Characteristics_Errors.csv'\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Load: ERP Characteristics\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "# get characteristics from view\n",
    "results = db.select(\n",
    "    model = V_ERPCharacteristics,\n",
    "    distinct=True,\n",
    "    orderby=['ERPCharacteristic'],\n",
    ")\n",
    "\n",
    "if len(results) > 0:\n",
    "    # check if characteristic already exist in the ERP system\n",
    "    df_chars = pd.DataFrame(results)\n",
    "    df_chars = convert_dataframe(df_chars)\n",
    "    \n",
    "    api = ApiCharacteristicHeader(CONFIG_ID)\n",
    "\n",
    "    response = []\n",
    "    error = []\n",
    "\n",
    "    log.info(f\"[GET] ERP characteristics based on Characteristic Name\")\n",
    "    # use Threading for parallel processing\n",
    "    def call_api(char):\n",
    "        return(api.search_characteristic(char))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        future_char = {executor.submit(call_api, char): char for char in df_chars['ERPCharacteristic']}\n",
    "\n",
    "        for future in as_completed(future_char):\n",
    "            char = future_char[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                if data:\n",
    "                    response.append(data)\n",
    "            except APIException as api_e:\n",
    "                log.error(f\"API Exception: {char} - {api_e.status_code} - {api_e.response} - {api_e.endpoint}\")\n",
    "                err = {\n",
    "                    'characteristic': char,\n",
    "                    'status_code': api_e.status_code,\n",
    "                    'response': api_e.response,\n",
    "                    'endpoint': api_e.endpoint\n",
    "                }\n",
    "                error.append(err)\n",
    "\n",
    "    log.info(f\"{len(response)} characteristics fetched.\")\n",
    "    if error:\n",
    "        log.error(f\"{len(error)} characteristics had errors.\")\n",
    "\n",
    "    df_chars = pd.json_normalize(response, sep=\"_\")\n",
    "    df_chars = convert_dataframe(df_chars)\n",
    "    df_chars.columns = [col.lstrip('__') if col.startswith('__') else col for col in df_chars.columns]\n",
    "    df_chars.to_parquet(file_tgt, index=False)\n",
    "    log.info(f\"{file_tgt} generated.\")\n",
    "\n",
    "    if error is not None and len(error)>0:\n",
    "        df_error = pd.DataFrame(error)\n",
    "        df_error.to_csv(file_err, index=False)\n",
    "        log.error(f\"{file_err} generated.\")\n",
    "    \n",
    "    if db.drop_reload:\n",
    "        db.truncate(ERPCharacteristics)  # truncate all existing data (remove if needed)\n",
    "\n",
    "    erp_chars = SQLAlchemyClient.dataframe_to_object(df_chars, ERPCharacteristics)\n",
    "    if erp_chars:\n",
    "        db.insert_batches(erp_chars)\n",
    "else:\n",
    "    log.warning(\"No characteristics found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Characteristiscs in ERP\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "from modules.util.database import (\n",
    "    SQLAlchemyClient,\n",
    "    V_ERPCharacteristics,\n",
    "    ERPCharacteristics,\n",
    ")\n",
    "from modules.erp.s4_clfn_characteristic_srv import ApiCharacteristicHeader\n",
    "\n",
    "file_err = rf\"{REPORTS_DIR}/2_ERP_Characteristics_Errors.csv\"\n",
    "\n",
    "missing_data = db.select(\n",
    "    model = V_ERPCharacteristics,\n",
    "    distinct=True,\n",
    "    orderby=['ERPCharacteristic'],\n",
    "    where = [V_ERPCharacteristics.CharcInternalID == None]\n",
    ")\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    log.info(f\"Missing Characteristics in ERP: {len(missing_data)}\")\n",
    "    df_missing = pd.DataFrame(missing_data)\n",
    "    df_missing = convert_dataframe(df_missing)\n",
    "\n",
    "    erp_mapping = CONFIG_GLOBAL.get(\"characteristic\").get(\"erp_mapping\")\n",
    "\n",
    "    df_missing['ERP_DataType'] = df_missing['indicators_datatype'].map(lambda x: erp_mapping.get(x, {}).get('datatype'))\n",
    "    df_missing['ERP_Length'] = df_missing.apply(\n",
    "        lambda row: row['indicators_precision'] if row['indicators_precision'] != '0' else erp_mapping.get(row['indicators_datatype'], {}).get('length'), \n",
    "        axis=1\n",
    "    )\n",
    "    df_missing['ERP_Decimals'] = df_missing.apply(\n",
    "        lambda row: row['indicators_scale'] if row['indicators_scale'] != '0' else erp_mapping.get(row['indicators_datatype'], {}).get('decimals'), \n",
    "        axis=1)\n",
    "\n",
    "    df_missing['ERP_NegativeFlag'] = df_missing['indicators_datatype'].map(lambda x: erp_mapping.get(x, {}).get('negative'))\n",
    "    df_missing['ERP_CaseSensitive'] = df_missing['indicators_datatype'].map(lambda x: erp_mapping.get(x, {}).get('case_sensitive'))\n",
    "\n",
    "    def call_api(char, datatype, length, decimals, description, negative_flag, case_sensitive):\n",
    "        return(api.create_characteristic(\n",
    "            char=char, \n",
    "            datatype=datatype, \n",
    "            length=length, \n",
    "            decimals=decimals, \n",
    "            description=description,\n",
    "            negative_flag=negative_flag,\n",
    "            case_sensitive_flag=case_sensitive))\n",
    "\n",
    "    api = ApiCharacteristicHeader(CONFIG_ID)\n",
    "    response = []\n",
    "    error = []\n",
    "    log.info(f\"[POST] ERP characteristics based on Characteristic Name\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        future_char = {executor.submit(call_api, char, dtype, length, decimals, description, negative_flag, case_sensitive): \n",
    "                    char for char, dtype, length, decimals, description, negative_flag, case_sensitive in \n",
    "                    zip(df_missing['ERPCharacteristic'], df_missing['ERP_DataType'], df_missing['ERP_Length'], df_missing['ERP_Decimals'], df_missing['ERPCharacteristic'], df_missing['ERP_NegativeFlag'], df_missing['ERP_CaseSensitive'])}\n",
    "\n",
    "        for future in as_completed(future_char):\n",
    "            char = future_char[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                if data:\n",
    "                    response.append(data)\n",
    "            except APIException as api_e:\n",
    "                log.error(f\"API Exception: {char} - {api_e.status_code} - {api_e.response} - {api_e.endpoint}\")\n",
    "                err = {\n",
    "                    'characteristic': char,\n",
    "                    'status_code': api_e.status_code,\n",
    "                    'response': api_e.response,\n",
    "                    'endpoint': api_e.endpoint\n",
    "                }\n",
    "                error.append(err)\n",
    "\n",
    "    log.info(f\"{len(response)} characteristics created.\")\n",
    "    if error:\n",
    "        log.info(f\"{len(error)} characteristics had errors.\")\n",
    "\n",
    "    if error is not None and len(error)>0:\n",
    "        df_error = pd.DataFrame(error)\n",
    "        df_error.to_csv(file_err, index=False)\n",
    "        log.error(f\"{file_err} generated.\")\n",
    "\n",
    "    df_chars = pd.json_normalize(response, sep=\"_\")\n",
    "    df_chars = convert_dataframe(df_chars)\n",
    "    df_chars.columns = [col.lstrip('__') if col.startswith('__') else col for col in df_chars.columns]\n",
    "    df_chars.drop(columns=['to_CharacteristicDesc_results'], inplace=True)\n",
    "\n",
    "    if not df_chars.empty:\n",
    "        erp_chars = SQLAlchemyClient.dataframe_to_object(df_chars, ERPCharacteristics)\n",
    "        db.insert_batches(erp_chars)\n",
    "else:\n",
    "    log.info(\"No characteristics created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Sync Indicator Positions with APM\n",
    "\n",
    "* Indicator positions are derived from the *APM indicator position*.\n",
    "\n",
    "* The migration tool proposes the APM indicator positions, but user has an option to override the proposal by providing an input.\n",
    "\n",
    "* The view `V_APMIndicatorPositions` contains the list of unique indicator positions derived from above.\n",
    "\n",
    "* Indicator positions are **mandatory** to be present (or) created in the APM system before migrating the indicators.\n",
    "\n",
    "**Process:**\n",
    "1. We check the APM system `[GET]` for indicator positions, based on the position name.\n",
    "\n",
    "2. If the value already exists in the APM system, we derive the `apm_guid` of the indicator position from the system.\n",
    "\n",
    "3. In case if the record does not exist, a new indicator position is created `[POST]` in the APM system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from modules.apm.manage_indicators import ApiIndicatorPosition\n",
    "from modules.util.database import APM_IndicatorPositions\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Load: APM Indicator Positions\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "file = rf'{EXTRACTION_DIR}/2_APM_Indicator_Positions.parquet'\n",
    "\n",
    "api_ind_pos = ApiIndicatorPosition(CONFIG_ID)\n",
    "\n",
    "indicator_positions = api_ind_pos.get_indicator_positions()\n",
    "df = pd.json_normalize(indicator_positions, sep='_')\n",
    "df.to_parquet(file, index=False)\n",
    "log.info(f'{file} generated')\n",
    "\n",
    "if db.drop_reload:\n",
    "    db.truncate(APM_IndicatorPositions) # truncate all existing data (remove if needed)\n",
    "\n",
    "positions = SQLAlchemyClient.dataframe_to_object(df, APM_IndicatorPositions)\n",
    "if positions:\n",
    "    db.insert_batches(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------ #\n",
    "# Create missing indicator positions\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# standard imports\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# custom imports\n",
    "from modules.util.api import APIException\n",
    "from modules.util.database import V_APMIndicatorPositions, APM_IndicatorPositions\n",
    "from modules.apm.manage_indicators import ApiIndicatorPosition\n",
    "from modules.util.helpers import convert_dataframe\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "api = ApiIndicatorPosition(CONFIG_ID)\n",
    "\n",
    "results = db.select(\n",
    "    model = V_APMIndicatorPositions,\n",
    "    where = [V_APMIndicatorPositions.apm_guid == None]\n",
    ")\n",
    "\n",
    "# process the missing indicator positions\n",
    "if len(results) > 0:\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # remove duplicates (there can be multiple indicators with the same position)\n",
    "    df_missing_position = df.drop_duplicates(subset=['APMIndicatorPosition'])\n",
    "    log.info(f\"{len(df_missing_position)} indicator positions to be created\")\n",
    "\n",
    "    if len(df_missing_position) > 0:\n",
    "        api = ApiIndicatorPosition(CONFIG_ID)\n",
    "        response = []\n",
    "        error = []\n",
    "\n",
    "        def call_api(id):\n",
    "            return(api.create_indicator_position(id))\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            future_char = {executor.submit(call_api, id): id for id in df_missing_position['APMIndicatorPosition']}\n",
    "\n",
    "            for future in as_completed(future_char):\n",
    "                id = future_char[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    response.append(data)\n",
    "                except APIException as api_e:\n",
    "                    \n",
    "                    # 409: indicator position already exists\n",
    "                    if api_e.status_code == 409:\n",
    "                        try:\n",
    "                            res = api.get_indicator_position_name(id)\n",
    "                            response.append(res)\n",
    "                        except APIException as api_e:\n",
    "                            log.error(f\"Exception for Indicator Position: {id}\")\n",
    "                            err = {\n",
    "                                'id': id,\n",
    "                                'status_code': api_e.status_code,\n",
    "                                'response': api_e.response,\n",
    "                                'endpoint': api_e.endpoint\n",
    "                            }\n",
    "                            error.append(err)\n",
    "                    else:\n",
    "                        log.error(f\"Exception for Indicator Position: {id}\")\n",
    "                        err = {\n",
    "                            'id': id,\n",
    "                            'status_code': api_e.status_code,\n",
    "                            'response': api_e.response,\n",
    "                            'endpoint': api_e.endpoint\n",
    "                        }\n",
    "                        error.append(err)\n",
    "        \n",
    "        log.info(f\"{len(response)} indicator positions created.\")\n",
    "        log.info(f\"{len(error)} indicator positions had errors.\")\n",
    "\n",
    "        if response:\n",
    "            df_ind_positions = pd.json_normalize(response, sep='_')\n",
    "            cols = ['ID', 'SSID', 'name']\n",
    "            df_ind_positions = df_ind_positions[cols]\n",
    "            df_ind_positions = convert_dataframe(df_ind_positions)\n",
    "\n",
    "            log.info(f\"Updating {APM_IndicatorPositions.__tablename__}\")\n",
    "            positions = SQLAlchemyClient.dataframe_to_object(df_ind_positions, APM_IndicatorPositions)\n",
    "            db.insert_batches(positions)\n",
    "            log.info(f\"Updated {APM_IndicatorPositions.__tablename__} with {db.count(APM_IndicatorPositions)} records\")\n",
    "        \n",
    "        if error:\n",
    "            df_error = pd.DataFrame(error)\n",
    "            file_err = rf\"{REPORTS_DIR}/3_APM_Indicator_Positions_Errors.csv\"\n",
    "            df_error.to_csv(file_err, index=False)\n",
    "            log.error(f\"{file_err} generated.\")\n",
    "else:\n",
    "    log.warning(f\"No indicator positions to be created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

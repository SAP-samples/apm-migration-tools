{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOM Enhancement\n",
    "\n",
    "As we have not the same equipment number for the equipments within PAI and APM, we need to adjust\n",
    "the load notebook to use a different equipment number when loading the data into APM.\n",
    "\n",
    "The equipment mapping are created from the already used `CUSTOM_MAPPING.csv` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-requisite Configuration\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# For testing\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"current work directory: {current_dir}\")\n",
    "\n",
    "# if the current working directory not ends with \"notebooks\", change it to the parent directory\n",
    "if not current_dir.endswith(\"notebooks\"):\n",
    "\n",
    "    workspace_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    os.chdir(workspace_root)\n",
    "\n",
    "    print(f\"changed root work directory to: {workspace_root}\")\n",
    "\n",
    "\n",
    "from modules.util.config import get_config_by_id, get_config_global # noqa: E402\n",
    "from modules.util.database import SQLAlchemyClient # noqa: E402\n",
    "from modules.util.helpers import Logger # noqa: E402\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "CONFIG_ID = 'CUSTOM_TEST'\n",
    "TRANSFORM = get_config_by_id(CONFIG_ID)[\"load\"][\"indicator\"]\n",
    "EXTRACTION_DIR = TRANSFORM[\"directory\"]\n",
    "REPORTS_DIR = f\"{EXTRACTION_DIR}/reports\"\n",
    "CONFIG_GLOBAL = get_config_global().get('indicators').get('transform')\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Create directories\n",
    "# ------------------------------------------------------------------------------ #\n",
    "if not os.path.exists(REPORTS_DIR):\n",
    "    os.makedirs(REPORTS_DIR)\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Logger\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"** LOAD - INDICATORS **\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "log.info(f\"Extraction Directory: {EXTRACTION_DIR}\")\n",
    "log.info(f\"Reports Directory: {REPORTS_DIR}\")\n",
    "\n",
    "db = SQLAlchemyClient(CONFIG_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Equi Mapping file\n",
    "\n",
    "In this step we will read the custom mapping file and create a separate mapping file for all equipments which are different \n",
    "from source and target system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import csv\n",
    "\n",
    "# files\n",
    "custom_mapping_path = \"./custom/CUSTUM_MAPPING.csv\"\n",
    "equipment_mapping_path = \"./custom/equipment_mapping.yaml\"\n",
    "\n",
    "def load_csv(file_path, delimiter=\",\"):\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file, delimiter=delimiter)\n",
    "        return list(reader)\n",
    "\n",
    "equipment_mapping = {}\n",
    "# for each row in the custom_mapping_data\n",
    "# we need to get the value of the 'PAI_Equipment' column and the 'Equipment' column\n",
    "# and add them to the equipment_mapping dictionary\n",
    "\n",
    "custom_mapping_data = load_csv(custom_mapping_path, delimiter=\";\")\n",
    "for custom_row in custom_mapping_data:\n",
    "    # check if 'PAI_Equipment' and 'Equipment' columns are not empty and if they are not equal\n",
    "    if custom_row.get('PAI_Equipment') and custom_row.get('Equipment') and custom_row['PAI_Equipment'] != custom_row['Equipment']:\n",
    "        # check if the value of 'PAI_Equipment' is not already in the equipment_mapping dictionary\n",
    "        if custom_row['PAI_Equipment'] not in equipment_mapping:\n",
    "            # add the value of 'PAI_Equipment' as a key and the value of 'Equipment' as a value\n",
    "            equipment_mapping[custom_row['PAI_Equipment']] = custom_row['Equipment']\n",
    "\n",
    "# save the equipment_mapping dictionary to a yaml file\n",
    "with open(equipment_mapping_path, \"w\") as file:\n",
    "    yaml.dump(equipment_mapping, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Technical Object Numbers from APM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get : Load Technical Object Numbers from APM for relevant technical objects\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# Standard Imports\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# custom imports\n",
    "from modules.util.database import (\n",
    "    V_Transform_Indicators,\n",
    "    ApmTechnicalObjects\n",
    ")\n",
    "from modules.apm.explore_technical_objects import ApiTechnicalObjects, APIException\n",
    "from modules.util.helpers import convert_dataframe\n",
    "from custom.tools import equipment_mapper\n",
    "\n",
    "file_tgt = f\"{REPORTS_DIR}/APM_Technical_Objects.csv\"\n",
    "file_err = f\"{REPORTS_DIR}/APM_Technical_Objects_Errors.csv\"\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"APM: Get Technical Object Numbers\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "log.info(f\"Reading data from {V_Transform_Indicators.__tablename__}\")\n",
    "df_objects = pd.DataFrame(db.select(model=V_Transform_Indicators, fields=['externalId'], distinct=True))\n",
    "\n",
    "api = ApiTechnicalObjects(CONFIG_ID)\n",
    "\n",
    "def call_api(id):\n",
    "    return(api.get_technical_object_number(id))\n",
    "\n",
    "response = []\n",
    "error = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    future_id = {executor.submit(call_api, equipment_mapper(id)): id for id in df_objects['externalId']}\n",
    "\n",
    "    for future in as_completed(future_id):\n",
    "        id = future_id[future]\n",
    "        try:\n",
    "            res = future.result()\n",
    "            if res:\n",
    "                response.append(res)\n",
    "        except APIException as api_e:\n",
    "            log.error(f\"API Exception: {id} - {api_e.status_code} - {api_e.response} - {api_e.endpoint}\")\n",
    "            err = {\n",
    "                'id': id,\n",
    "                'status_code': api_e.status_code,\n",
    "                'response': api_e.response,\n",
    "                'endpoint': api_e.endpoint\n",
    "            }\n",
    "            error.append(err)\n",
    "log.info(f\"Technical Object Numbers Fetched: {len(response)}\")\n",
    "if error:\n",
    "    log.error(f\"{len(error)} technical objects had errors.\")\n",
    "    df_err = pd.DataFrame(error)\n",
    "    df_err.to_csv(file_err,index=False)\n",
    "    log.error(f\"{file_err} generated.\")\n",
    "\n",
    "if response:\n",
    "    df_tech_objects = pd.json_normalize(response)\n",
    "    df_tech_objects = convert_dataframe(df_tech_objects)\n",
    "    if '@id' in df_tech_objects.columns:\n",
    "        df_tech_objects.drop(columns=['@id'], inplace=True)\n",
    "    \n",
    "    df_tech_objects.to_csv(file_tgt,index=False)\n",
    "    log.info(f\"{file_tgt} generated\")\n",
    "\n",
    "    if db.drop_reload:\n",
    "        db.truncate(ApmTechnicalObjects)\n",
    "    \n",
    "    technical_objects = SQLAlchemyClient.dataframe_to_object(df_tech_objects, ApmTechnicalObjects)\n",
    "    if technical_objects:\n",
    "        db.insert_batches(data=technical_objects)\n",
    "    else:\n",
    "        log.warning(\"Technical object numbers not found in APM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Indicators to APM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare: Load Indicator Data\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# standard imports\n",
    "import pandas as pd\n",
    "\n",
    "# custom imports\n",
    "from modules.util.database import (\n",
    "    V_Transform_Indicators,\n",
    "    T_UDR_Indicators,\n",
    "    V_ERPCharacteristics,\n",
    "    V_APMIndicatorPositions,\n",
    "    LoadIndicators,\n",
    "    PreLoadIndicators,\n",
    "    ApmTechnicalObjects\n",
    ")\n",
    "\n",
    "from custom.tools import equipment_mapper\n",
    "\n",
    "Logger.blank_line(log)\n",
    "log.info(\"Load: APM Indicators\")\n",
    "Logger.blank_line(log)\n",
    "\n",
    "log.info(f\"Reading data from {T_UDR_Indicators.__tablename__} / {V_Transform_Indicators.__tablename__} / {V_ERPCharacteristics.__tablename__} / {V_APMIndicatorPositions.__tablename__}\")\n",
    "df_udr = pd.DataFrame(db.select(model=T_UDR_Indicators))\n",
    "df_indicators = pd.DataFrame(db.select(model=V_Transform_Indicators))\n",
    "df_chars = pd.DataFrame(db.select(model=V_ERPCharacteristics))\n",
    "df_positions = pd.DataFrame(db.select(model=V_APMIndicatorPositions))\n",
    "df_technicalObjects = pd.DataFrame(db.select(model=ApmTechnicalObjects))\n",
    "\n",
    "# check for a mapping for all lines of df_technicalObjects for the column 'technicalObject' and 'number'\n",
    "if not df_technicalObjects.empty:\n",
    "    df_technicalObjects['technicalObject'] = df_technicalObjects['technicalObject'].apply(lambda x: equipment_mapper(x))\n",
    "    df_technicalObjects['number'] = df_technicalObjects['number'].apply(lambda x: equipment_mapper(x))\n",
    "\n",
    "if not df_udr.empty:\n",
    "    df_udr['externalId'] = df_udr['externalId'].apply(lambda x: equipment_mapper(x))\n",
    "    df_udr[\"internalId\"] = df_udr[\"internalId\"].apply(lambda x: equipment_mapper(x))\n",
    "    df_udr[\"name\"] = df_udr[\"name\"].apply(lambda x: equipment_mapper(x))\n",
    "\n",
    "if not df_indicators.empty:\n",
    "    df_indicators['externalId'] = df_indicators['externalId'].apply(lambda x: equipment_mapper(x))\n",
    "    df_indicators[\"internalId\"] = df_indicators[\"internalId\"].apply(lambda x: equipment_mapper(x))\n",
    "    df_indicators[\"name\"] = df_indicators[\"name\"].apply(lambda x: equipment_mapper(x))\n",
    "\n",
    "log.info(f\"Preparing Indicator Data for Migration\")\n",
    "df_final = pd.DataFrame()\n",
    "df_final = pd.merge(df_udr, df_indicators, on=['tenantid', 'id', 'templateId', 'indicatorGroups_id', 'indicators_id'], suffixes=('', '_udr'), how='left')\n",
    "df_final = pd.merge(df_final, df_chars, on=['tenantid', 'ERPCharacteristic'], suffixes=('', '_char'), how='left')\n",
    "df_final = pd.merge(df_final, df_positions, on=['tenantid', 'indicatorGroups_internalId', 'APMIndicatorPosition'], suffixes=('', '_pos'), how='left')\n",
    "\n",
    "df_final.drop(columns=['idx_pos', 'idx_char', 'idx'], inplace=True)\n",
    "df_final.drop_duplicates(inplace=True)\n",
    "\n",
    "df_final['technicalObject_type'] = df_final['objectType'].apply(lambda x: 'EQUI' if x == 'EQU' else 'FLOC' if x == 'FL' else None)\n",
    "df_final['valid'] = df_final.apply(lambda row: 'X' if row['APMIndicatorCategory'] and row['CharcInternalID'] and row['apm_guid'] and row['ssid'] else None, axis=1)\n",
    "\n",
    "# consider the technical object number from APM and not externalID for posting data\n",
    "df_final = pd.merge(df_final, df_technicalObjects[['technicalObject', 'number']], left_on='externalId', right_on='technicalObject', how='left')\n",
    "df_final.rename(columns={'number': 'APMTechnicalObjectNumber'}, inplace=True)\n",
    "df_final.drop(columns=['technicalObject'], inplace=True)\n",
    "\n",
    "\n",
    "if len(df_final) > 0:\n",
    "    df_final = df_final[['tenantid','internalId', 'name', 'externalId', 'objectType', 'indicatorGroups_internalId', \n",
    "                         'indicatorGroups_description_short', 'indicators_internalId', 'indicators_description_short', 'indicators_datatype',\n",
    "                         'indicators_scale','indicators_precision', 'id', 'templateId', 'indicatorGroups_id', 'indicators_id',\n",
    "                         'ERPCharacteristic', 'CharcInternalID','APMIndicatorCategory', 'apm_guid','ssid', 'technicalObject_type', 'APMTechnicalObjectNumber','valid']]\n",
    "    if db.drop_reload:\n",
    "        log.info(f\"Clearing data from {PreLoadIndicators.__tablename__}\")\n",
    "        db.truncate(PreLoadIndicators)\n",
    "    log.info(f\"Updating {PreLoadIndicators.__tablename__}\")    \n",
    "    indicators = SQLAlchemyClient.dataframe_to_object(df_final, PreLoadIndicators)\n",
    "    db.insert_batches(indicators)\n",
    "    log.info(f\"Updated {PreLoadIndicators.__tablename__} with {db.count(PreLoadIndicators)} records\")\n",
    "\n",
    "\n",
    "df_final = df_final[['tenantid', 'APMTechnicalObjectNumber', 'technicalObject_type', 'APMIndicatorCategory', 'CharcInternalID', 'apm_guid', 'ssid', 'valid']]\n",
    "df_final.columns = ['tenantid', 'technicalObject_number', 'technicalObject_type', 'category_name', 'characteristics_characteristicsInternalId', 'positionDetails_ID', 'technicalObject_SSID', 'valid']\n",
    "\n",
    "df_final['category_SSID'] = df_final['technicalObject_SSID']\n",
    "df_final['characteristics_SSID'] = df_final['technicalObject_SSID']\n",
    "\n",
    "df_final = df_final.drop_duplicates()\n",
    "\n",
    "if len(df_final) > 0:\n",
    "    if db.drop_reload:\n",
    "        log.info(f\"Clearing data from {LoadIndicators.__tablename__}\")\n",
    "        db.truncate(LoadIndicators)\n",
    "    \n",
    "    log.info(f\"Updating {LoadIndicators.__tablename__}\")\n",
    "    indicators = SQLAlchemyClient.dataframe_to_object(df_final, LoadIndicators)\n",
    "    db.insert_batches(indicators)\n",
    "    log.info(f\"Updated {LoadIndicators.__tablename__} with {db.count(LoadIndicators)} records\")\n",
    "else:\n",
    "    log.warning(\"No indicators found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load: Indicator Data to APM\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "# standard imports\n",
    "import pandas as pd\n",
    "\n",
    "# custom imports\n",
    "from modules.util.database import LoadIndicators, PostLoadIndicators\n",
    "from modules.apm.manage_indicators import ApiIndicators\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from modules.util.api import APIException\n",
    "from modules.util.helpers import convert_dataframe\n",
    "\n",
    "file_err = rf\"{REPORTS_DIR}/3_APM_Indicators_Errors.csv\"\n",
    "\n",
    "df_indicators = pd.DataFrame(db.select(model=LoadIndicators, where=[LoadIndicators.valid == 'X']))\n",
    "log.info(f\"{len(df_indicators)} valid indicators found\")\n",
    "\n",
    "api = ApiIndicators(CONFIG_ID)\n",
    "response = []\n",
    "error = []\n",
    "\n",
    "def call_api(row):\n",
    "    return(api.create_indicator(row))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    future_char = {executor.submit(call_api, row): row for _, row in df_indicators.iterrows()}\n",
    "\n",
    "    for future in as_completed(future_char):\n",
    "        row = future_char[future]\n",
    "        try:\n",
    "            data = future.result()\n",
    "            response.append(data)\n",
    "        except APIException as api_e:\n",
    "            if api_e.status_code == 409:\n",
    "                log.warning(f\"Indicator already exists: ({row['technicalObject_number']},{row['category_name']},{row['positionDetails_ID']},{row['characteristics_characteristicsInternalId']})\")\n",
    "                data = api.search_indicator(\n",
    "                    technicalObject_number=row['technicalObject_number'],\n",
    "                    technicalObject_type=row['technicalObject_type'],\n",
    "                    category_name=row['category_name'],\n",
    "                    positionDetails_ID=row['positionDetails_ID'],\n",
    "                    characteristics_characteristicsInternalId=row['characteristics_characteristicsInternalId'],\n",
    "                    technicalObject_SSID=row['technicalObject_SSID'],\n",
    "                    category_SSID=row['category_SSID'],\n",
    "                    characteristics_SSID=row['characteristics_SSID']\n",
    "                )\n",
    "                if \"value\" in data:\n",
    "                    response.append(data.get(\"value\")[0])\n",
    "            else:\n",
    "                log.error(f\"API Exception: ({row['technicalObject_number']},{row['category_name']},{row['positionDetails_ID']},{row['characteristics_characteristicsInternalId']}) - {api_e.status_code} - {api_e.response}\")\n",
    "                err = {\n",
    "                    'technicalObject_number': row['technicalObject_number'],\n",
    "                    'category_name': row['category_name'],\n",
    "                    'positionDetails_ID': row['positionDetails_ID'],\n",
    "                    'characteristics_characteristicsInternalId': row['characteristics_characteristicsInternalId'],\n",
    "                    'status_code': api_e.status_code,\n",
    "                    'response': api_e.response,\n",
    "                    'endpoint': api_e.endpoint\n",
    "                }\n",
    "                error.append(err)\n",
    "if response:\n",
    "    df_response = pd.json_normalize(response, sep='_')\n",
    "    df_response = convert_dataframe(df_response)\n",
    "    df_response = df_response[[col for col in PostLoadIndicators.__table__.columns.keys() if col in df_response.columns]]\n",
    "\n",
    "    if db.drop_reload:\n",
    "        log.info(f\"Clearing data from {PostLoadIndicators.__tablename__}\")\n",
    "        db.truncate(PostLoadIndicators)\n",
    "    \n",
    "    log.info(f\"Updating {PostLoadIndicators.__tablename__}\")\n",
    "    indicators = SQLAlchemyClient.dataframe_to_object(df_response, PostLoadIndicators)\n",
    "    db.insert_batches(indicators)\n",
    "    log.info(f\"Updated {PostLoadIndicators.__tablename__} with {db.count(PostLoadIndicators)} records\")\n",
    "\n",
    "if error:\n",
    "    df_error = pd.DataFrame(error)\n",
    "    df_error.to_csv(file_err, index=False)\n",
    "    log.error(f\"{file_err} generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data\n",
    "\n",
    "This notebook will transform the extracted data and store it after transformation in parquet files.\n",
    "\n",
    "For detailed description see [documentation](../docs/iot-time-series-data.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"current work directory: {current_dir}\")\n",
    "\n",
    "# if the current working directory not ends with \"notebooks\", change it to the parent directory\n",
    "if not current_dir.endswith(\"notebooks\"):\n",
    "\n",
    "    workspace_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    os.chdir(workspace_root)\n",
    "\n",
    "    print(f\"changed root work directory to: {workspace_root}\")\n",
    "\n",
    "from modules.util.database import SQLAlchemyClient  # noqa: E402\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "CONFIG_ID = \"CUSTOM_TEST\"\n",
    "\n",
    "# setup database\n",
    "\n",
    "db = SQLAlchemyClient(CONFIG_ID)\n",
    "db.table_create_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data\n",
    "\n",
    "We'll iterate over all csv files in the download folder and process each file after another.\n",
    "We expect that each file has a unique property set type or indicator group as this was the\n",
    "key when downloading the file. \n",
    "\n",
    "Next we need to determine the APM and eIOT ID's for each Thing / Indicator from PAI. To avoid\n",
    "unneccesary API calls we also store the mapping in an own DB table for later lookup. So, if the\n",
    "mapping was already determined before, we can return the values from the DB. Otherwise we need\n",
    "to do the following steps:\n",
    "- determine the modelId and modelType from PAI\n",
    "- from external id api we can now determine the technical object number for the source S4 system\n",
    "- after finally having the SSID, Number and Type of the technical object we can get the\n",
    "metadata for this TO from the eIOT Metadata API. This will return the needed information as\n",
    "managedObjectId and all the measuringNodeId's for the assigned indicators.\n",
    "- also important to check that the status of the metadata sync is done (\"synced\")\n",
    "- to map the \"old\" PAI indicator to the new indicator in APM we use the own database view\n",
    "V_POST_LOAD_INDICATORS which holds the information about the newly created indicators in APM.\n",
    "- if the PAI indicator was created in APM we save this indicator data from eIOT in our internal\n",
    "mapping table.\n",
    "\n",
    "After the mapping for the technical objects and indicators are derived, we are creating a dataset\n",
    "for each measurement with the assigned _time, managedObjectId, measuringNodeId, characteristic and\n",
    "value. As this dataset is loaded into a dataframe, we can easily pivot the data to have all\n",
    "characteristics in one line, based on the key fields of _time, managedObjectId, measuringNodeId.\n",
    "\n",
    "The schema of the dataset will be set afterwards. All indicators with numeric or numeric flexible\n",
    "get the datatype `float` and a date indicator get the datatype of `date`. Indicators with\n",
    "string can't be migrated.\n",
    "\n",
    "Finally the dataframe will be written as a parquet file to the location you have configured in the\n",
    "config file under `[\"transform\"][\"time-series\"][\"directory\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "\n",
    "from custom.tools import equipment_mapper # noqa: E402\n",
    "from modules.acf.external_id_api import ApiExternalId  # noqa: E402\n",
    "from modules.util.config import get_config_by_id  # noqa: E402\n",
    "from modules.util.helpers import (  # noqa: E402\n",
    "    Logger,  # noqa: E402\n",
    "    convert_unix_to_iso,  # noqa: E402\n",
    "    get_parquet_schema,  # noqa: E402\n",
    ")  # noqa: E402\n",
    "from modules.acf.model_api import ApiModel  # noqa: E402\n",
    "from modules.apm.eiot import EIoTApi  # noqa: E402\n",
    "\n",
    "from modules.util.database import (  # noqa: E402\n",
    "    EIotMapping,  # noqa: E402\n",
    "    EIotMappingIndicators,  # noqa: E402\n",
    "    SQLAlchemyClient,  # noqa: E402\n",
    "    V_PostLoad_Indicators,  # noqa: E402\n",
    ")  # noqa: E402\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "CONFIG_ID = \"CUSTOM_TEST\"\n",
    "log = Logger.get_logger(CONFIG_ID)\n",
    "config = get_config_by_id(CONFIG_ID)\n",
    "api_external_id = ApiExternalId(CONFIG_ID)\n",
    "api_eiot = EIoTApi(CONFIG_ID)\n",
    "api_model = ApiModel(CONFIG_ID)\n",
    "db = SQLAlchemyClient(CONFIG_ID)\n",
    "\n",
    "db.truncate(EIotMapping)\n",
    "db.truncate(model=EIotMappingIndicators)\n",
    "\n",
    "indicator_mapping = dict()\n",
    "\n",
    "# TO-DO: move to configuration and create folders if not exist  (use pathlib)\n",
    "DOWNLOAD_FOLDER = config[\"extract\"][\"time-series\"][\"directory\"]\n",
    "TRANSFORMED_FOLDER = config[\"transform\"][\"time-series\"][\"directory\"]\n",
    "COLUMN_IGNORE_LIST = [\"_TIME\", \"equipmentId\", \"indicatorGroupId\", \"modelId\", \"templateId\", \"nPST\"]\n",
    "\n",
    "# dictionary to store the external ids (technical objects)\n",
    "# external_ids = dict() # key: thing_id, value: { \"ain_id\": \"1234\", \"erp_id\": \"1234\" }\n",
    "SSID = api_eiot.get_ssid()\n",
    "\n",
    "def get_indicator_mapping(db_session: SQLAlchemyClient, indicator_id: str, acf_id:str, thing_id: str, indicator_mapping: dict) -> dict:\n",
    "    \"\"\" In order not to query the API from APM everytime we store the indicator mapping in a database\n",
    "    and query it from there.\n",
    "\n",
    "    Args:\n",
    "        indicator_id (str): _description_\n",
    "        acf_id (str): _description_\n",
    "        thing_id (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        dict: _description_\n",
    "    \"\"\"\n",
    "    # if the indicator_id starts with leading I_ we need to remove it\n",
    "    if indicator_id.startswith(\"I_\"):\n",
    "        indicator_id = indicator_id[2:]\n",
    "\n",
    "    if indicator_id in indicator_mapping:\n",
    "        return indicator_mapping[indicator_id]\n",
    "\n",
    "    # create where condition\n",
    "    if acf_id:\n",
    "        indicator_mapping_db = db_session.select(\n",
    "            model=EIotMapping,\n",
    "            where=[\n",
    "                EIotMapping.acfId == f\"{acf_id}\",\n",
    "            ],\n",
    "            return_dict=False,\n",
    "        )\n",
    "    elif thing_id:\n",
    "        indicator_mapping_db = db_session.select(\n",
    "            model=EIotMapping,\n",
    "            where=[\n",
    "                EIotMapping.acfId\n",
    "                == f\"{thing_id}\",\n",
    "            ],\n",
    "            return_dict=False,\n",
    "        )\n",
    "\n",
    "    if len(indicator_mapping_db) == 0:\n",
    "        log.error(f\"Could not find indicator mapping for indicator id {indicator_id}\")\n",
    "        return None\n",
    "    elif len(indicator_mapping_db) == 1:\n",
    "        # Ensure the parent instance is bound to a session\n",
    "        from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "        Session = sessionmaker(bind=db_session.engine)\n",
    "        with Session() as session:\n",
    "            parent_instance = session.query(EIotMapping).get(indicator_mapping_db[0].id)\n",
    "\n",
    "            # find the indicator mapping\n",
    "            for indi in parent_instance.indicators:\n",
    "                if indi.indicatorIdAcf == indicator_id:\n",
    "                    indi.managedObjectId = parent_instance.managedObjectId\n",
    "                    indicator_mapping[indicator_id] = indi\n",
    "                    return indi\n",
    "            else:\n",
    "                log.error(\n",
    "                    f\"Could not find indicator mapping for indicator id {indicator_id}\"\n",
    "                )\n",
    "                indicator_mapping[indicator_id] = None\n",
    "                return None\n",
    "\n",
    "\n",
    "def check_eiot_mapping(db_session: SQLAlchemyClient, acf_id:str, model_id: str) -> bool:\n",
    "    \n",
    "    eiot_mapping = db_session.select_one(\n",
    "        model=EIotMapping,\n",
    "        where=[EIotMapping.acfId == f\"{acf_id}\"],\n",
    "    )\n",
    "\n",
    "    if eiot_mapping is None:\n",
    "        # we need to add the mapping\n",
    "        return add_eiot_mapping(db_session=db, model_id=model_id, acf_id=acf_id)\n",
    "    else:\n",
    "        # we need to check if the mapping has indicators\n",
    "        eiot_mapping_count_indicators = db_session.select_count(\n",
    "            model=EIotMappingIndicators,\n",
    "            where=[EIotMappingIndicators.parent_id == f\"{eiot_mapping['id']}\"],\n",
    "        )\n",
    "        if eiot_mapping_count_indicators == 0:\n",
    "            return False\n",
    "        elif eiot_mapping_count_indicators > 0:\n",
    "            return True\n",
    "\n",
    "def get_eiot_mapping_count(db_session: SQLAlchemyClient, acf_id:str):\n",
    "    eiot_mapping = db_session.select(\n",
    "        model=EIotMapping,\n",
    "        where=[EIotMapping.acfId == f\"{acf_id}\"],\n",
    "    )\n",
    "\n",
    "    if len(eiot_mapping) == 0:\n",
    "        return 0\n",
    "    elif len(eiot_mapping) == 1:\n",
    "        eiot_mapping_count_indicators = db_session.select_count(\n",
    "            model=EIotMappingIndicators,\n",
    "            where=[EIotMappingIndicators.parent_id == f\"{eiot_mapping[0]['id']}\"],\n",
    "        )\n",
    "        if eiot_mapping_count_indicators == 0:\n",
    "            log.error(f\"No indicators found for {eiot_mapping[0]['number']}({eiot_mapping[0]['type']})\")\n",
    "            return -1\n",
    "        elif eiot_mapping_count_indicators > 0:\n",
    "            return 1\n",
    "    else:\n",
    "        log.error(f\"Unknown length when getting eiot mapping: EIOT_MAPPING {len(eiot_mapping)}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_output_file(indicator_group_id: str) -> str:\n",
    "    outout_folder = os.path.join(TRANSFORMED_FOLDER, indicator_group_id)\n",
    "    if not os.path.exists(outout_folder):\n",
    "        os.makedirs(outout_folder)\n",
    "    return os.path.join(outout_folder, file.replace(\".csv\", \".parquet\"))\n",
    "\n",
    "def add_eiot_mapping(db_session: SQLAlchemyClient, model_id: str, acf_id: str = None) -> bool:\n",
    "    \n",
    "    eiot_mapping_header = EIotMapping()\n",
    "\n",
    "    # first we need to get the model id to distinguish between equipment and functional location\n",
    "    model_data = api_model.get_model_header(model_id=model_id)\n",
    "\n",
    "    eiot_mapping_header.modelId = model_id\n",
    "    eiot_mapping_header.type = model_data[\"modelType\"]\n",
    "    eiot_mapping_header.acfId = acf_id\n",
    "    eiot_mapping_header.SSID = SSID\n",
    "\n",
    "    api_external_id_res = api_external_id.get_external_data(\n",
    "        filter_str=f\"objectType eq '{model_data[\"modelType\"]}' and systemType eq 'SAP ERP' and ainObjectId eq '{acf_id}'\"\n",
    "    )\n",
    "\n",
    "    if len(api_external_id_res) == 0:\n",
    "        log.error(f\"Could not find external id {acf_id} in ACF\")\n",
    "        # add without eiot mapping and without indicators, but return False\n",
    "        db_session.insert_one(obj=eiot_mapping_header, commit=True)\n",
    "        return False\n",
    "\n",
    "    # eiot_mapping_header.number = api_external_id_res[0][\"externalId\"]\n",
    "    eiot_mapping_header.number = equipment_mapper(api_external_id_res[0][\"externalId\"])\n",
    "\n",
    "    try:\n",
    "        eiot_sync_status = api_eiot.get_eiot_sync_status_by_to(\n",
    "            number=eiot_mapping_header.number,\n",
    "            ssid=eiot_mapping_header.SSID,\n",
    "            to_type=eiot_mapping_header.type,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log.error(\n",
    "            f\"Error when getting eiot sync status for {eiot_mapping_header.number}({eiot_mapping_header.type}): {e.status_code}:{e.response}\"\n",
    "        )\n",
    "        db_session.insert_one(obj=eiot_mapping_header, commit=True)\n",
    "        return False\n",
    "\n",
    "    if eiot_sync_status['technicalObjectSyncStatus'] == 'NOT_SYNCED' or eiot_sync_status['eIotSyncTime'] is None:\n",
    "        log.error(f\"Technical object {eiot_mapping_header.number}({eiot_mapping_header.type}) is not synced\")\n",
    "        db_session.insert_one(obj=eiot_mapping_header, commit=True)\n",
    "        return False\n",
    "\n",
    "    eiot_mapping_header.managedObjectId = eiot_sync_status[\n",
    "        \"managedObjectId\"\n",
    "    ]\n",
    "\n",
    "    for indicator in eiot_sync_status[\"indicators\"]:\n",
    "        # add indicators to mapping\n",
    "        log.debug(f\"check indicator {indicator[\"indicatorId\"]}\")\n",
    "        # read the matching indicator\n",
    "        apm_indicator = db.select(\n",
    "            model=V_PostLoad_Indicators,\n",
    "            distinct=True,\n",
    "            where=[\n",
    "                V_PostLoad_Indicators.apm_indicatorId\n",
    "                == f\"{indicator[\"indicatorId\"]}\",\n",
    "                V_PostLoad_Indicators.apm_positionId\n",
    "                == f\"{indicator[\"positionDetailsId\"]}\",\n",
    "                V_PostLoad_Indicators.APMIndicatorCategory\n",
    "                == f\"{indicator[\"categoryName\"]}\",\n",
    "                V_PostLoad_Indicators.CharcInternalID\n",
    "                == f\"{indicator[\"characteristicsInternalId\"]}\",\n",
    "                V_PostLoad_Indicators.externalId\n",
    "                == f\"{eiot_sync_status['number']}\",\n",
    "                V_PostLoad_Indicators.technicalObject_type\n",
    "                == f\"{eiot_sync_status['type']}\",\n",
    "                V_PostLoad_Indicators.ssid\n",
    "                == f\"{eiot_sync_status['SSID']}\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        if len(apm_indicator) == 0:\n",
    "            log.debug(\n",
    "                f\"no synced indicator {indicator[\"indicatorId\"]} posId:{indicator[\"positionDetailsId\"]}\"\n",
    "            )\n",
    "        elif len(apm_indicator) == 1:\n",
    "            # add an item to eiot_mapping_header\n",
    "            eiot_mapping_item = EIotMappingIndicators(\n",
    "                tenantid=apm_indicator[0][\"tenantid\"],\n",
    "                indicatorIdAcf=apm_indicator[0][\"indicators_id\"],\n",
    "                indicatorIdApm=apm_indicator[0][\"apm_indicatorId\"],\n",
    "                categoryName=apm_indicator[0][\"APMIndicatorCategory\"],\n",
    "                characteristicsInternalId=apm_indicator[0][\n",
    "                    \"CharcInternalID\"\n",
    "                ],\n",
    "                positionDetailsId=apm_indicator[0][\"apm_positionId\"],\n",
    "                dataType=indicator[\"dataType\"],\n",
    "                unitOfMeasure=indicator[\"unitOfMeasure\"],\n",
    "                charcLength=indicator[\"charcLength\"],\n",
    "                charcDecimals=indicator[\"charcDecimals\"],\n",
    "                measuringNodeId=indicator[\"measuringNodeId\"],\n",
    "                technicalGroupId=indicator[\"technicalGroupId\"],\n",
    "            )\n",
    "\n",
    "            eiot_mapping_header.indicators.append(eiot_mapping_item)\n",
    "        else:\n",
    "            log.error(\n",
    "                f\"Unknown length when getting indicator mapping: V_POST_LOAD_INDICATORS {len(apm_indicator)}\"\n",
    "            )\n",
    "\n",
    "        # if len(eiot_mapping_header.indicators) == 0:\n",
    "        #     log.error(f\"No indicators found for {eiot_mapping_header.number}({eiot_mapping_header.type})\")\n",
    "        #     db_session.insert_one(obj=eiot_mapping_header, commit=True)\n",
    "        #     return False\n",
    "\n",
    "    # due to lazy loading in sqlalchemy we need to save the indicator count\n",
    "    indicator_count = len(eiot_mapping_header.indicators)\n",
    "\n",
    "    # insert mapping to db\n",
    "    db_session.insert_one(obj=eiot_mapping_header, commit=True)\n",
    "\n",
    "    eiot_mapping_count = db_session.select(\n",
    "        model=EIotMapping,\n",
    "        where=[EIotMapping.acfId == f\"{acf_id}\"],\n",
    "    )\n",
    "\n",
    "    if indicator_count == 0:\n",
    "        log.error(f\"No indicators found for {eiot_mapping_count[0]['number']}({eiot_mapping_count[0]['type']})\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# if V_PostLoad_Indicators is empty, we need can skip the rest of the script\n",
    "if db.select_count(model=V_PostLoad_Indicators) == 0:\n",
    "    log.error(\"No indicators found in V_PostLoad_Indicators\")\n",
    "    raise Exception(\"No indicators found in V_PostLoad_Indicators\")\n",
    "\n",
    "# find all files with ending .csv in the download folder and their sub-folders\n",
    "for root, dirs, files in os.walk(DOWNLOAD_FOLDER):\n",
    "    for file in files:\n",
    "        start_time = time.time()\n",
    "        data = []\n",
    "        if file.endswith(\".csv\"):\n",
    "            log.debug(os.path.join(root, file))\n",
    "            # read the csv file into a pandas dataframe\n",
    "            df = pd.read_csv(os.path.join(root, file), sep=\",\", header=0)\n",
    "            log.debug(f\"Read CSV file in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "            # split root string by / and get the last element\n",
    "            # fallback if ind. group is not unique in the csv files\n",
    "            # current_ind_group = root.split(\"/\")[len(root.split(\"/\")) - 1].split(\"+\")[0]\n",
    "\n",
    "            # we have different flavors of csv files, so we need to check if the column names are the same\n",
    "            if \"THING_ID\" in df.columns:\n",
    "                if len(df[\"nPST\"].unique()) > 1:\n",
    "                    log.error(\"Indicator group id is not unique\")\n",
    "                    # TO-DO: if this becomes true we need to use the\n",
    "                    # indicator group extracted from the folder name\n",
    "                    raise Exception(\"Indicator group id is not unique\")\n",
    "                else:\n",
    "                    output_file = get_output_file(indicator_group_id=df[\"nPST\"].unique()[0])\n",
    "\n",
    "                log.debug(\"CSV data is based on thing model\")\n",
    "\n",
    "                thing_ids = df[\"THING_ID\"].unique()\n",
    "\n",
    "\n",
    "                for thing_id in thing_ids:\n",
    "                    thing_id_mapping = api_external_id.get_acf_object_by_thing_id(external_id=thing_id)\n",
    "\n",
    "                    model_mapping_iot = api_external_id.get_acf_model_id_by_thing_type(thing_type=df[\"_ThingType\"].unique()[0])\n",
    "                    \n",
    "                    if not thing_id_mapping:\n",
    "                        df = df[df[\"THING_ID\"] != thing_id]\n",
    "                        continue\n",
    "\n",
    "                    eiot_result = check_eiot_mapping(db_session=db, acf_id=thing_id_mapping[\"ainObjectID\"], model_id=model_mapping_iot[\"ainObjectID\"])\n",
    "                    if not eiot_result:\n",
    "                        df = df[df[\"THING_ID\"] != thing_id]\n",
    "\n",
    "            elif \"equipmentId\" in df.columns:\n",
    "                if len(df[\"indicatorGroupId\"].unique()) > 1:\n",
    "                    log.error(\"Indicator group id is not unique\")\n",
    "                    raise Exception(\"Indicator group id is not unique\")\n",
    "                else:\n",
    "                    output_file = get_output_file(indicator_group_id=df[\"indicatorGroupId\"].unique()[0])\n",
    "\n",
    "                log.debug(\"CSV data is based on abstract model\")\n",
    "                equi_ids = df[\"equipmentId\"].unique()\n",
    "\n",
    "                for equi_id in equi_ids:\n",
    "                    eiot_result = check_eiot_mapping(db_session=db, acf_id=equi_id, model_id=df.loc[df[\"equipmentId\"] == equi_id].iloc[0][\"modelId\"])\n",
    "                    if not eiot_result:\n",
    "                        df = df[df[\"equipmentId\"] != equi_id]\n",
    "\n",
    "            log.debug(f\"Processed equipment/thing IDs in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                iso_time = convert_unix_to_iso(row[\"_TIME\"])\n",
    "\n",
    "                if \"equipmentId\" in df.columns:\n",
    "                    acf_id = row[\"equipmentId\"]\n",
    "                    thing_id = None\n",
    "                elif \"THING_ID\" in df.columns:\n",
    "                    acf_id = None\n",
    "                    thing_id = row[\"THING_ID\"]\n",
    "\n",
    "                for key, value in row.items():\n",
    "                    if key in COLUMN_IGNORE_LIST:\n",
    "                        continue\n",
    "                    if pd.isna(value):\n",
    "                        continue\n",
    "                    ind_mapping = get_indicator_mapping(db_session=db, indicator_id=key, acf_id=acf_id, thing_id=thing_id, indicator_mapping=indicator_mapping)\n",
    "                    if ind_mapping is None:\n",
    "                        continue\n",
    "\n",
    "                    if isinstance(value, bool):\n",
    "                        value = 1 if value else 0\n",
    "                    dataset = {\n",
    "                        \"_time\": iso_time,\n",
    "                        \"value\": value,\n",
    "                        \"managedObjectId\": ind_mapping.managedObjectId,\n",
    "                        \"characteristic\": f\"C_{ind_mapping.characteristicsInternalId}\",\n",
    "                        \"measuringNodeId\": ind_mapping.measuringNodeId,\n",
    "                    }\n",
    "                    data.append(dataset)\n",
    "\n",
    "            log.debug(f\"Processed rows in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "            # store data as parquet file\n",
    "            if len(data) == 0:\n",
    "                log.debug(\"No data to store\")\n",
    "                continue\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            df_pivot = df.pivot_table(\n",
    "                index=[\"managedObjectId\", \"_time\", \"measuringNodeId\"],\n",
    "                columns=\"characteristic\",\n",
    "                values=\"value\",\n",
    "                aggfunc=\"first\",\n",
    "            ).reset_index()\n",
    "\n",
    "            # Ensure _time column is in datetime format\n",
    "            df_pivot[\"_time\"] = pd.to_datetime(df_pivot[\"_time\"], errors='coerce')\n",
    "            if df_pivot[\"_time\"].dt.tz is None:\n",
    "                df_pivot[\"_time\"] = df_pivot[\"_time\"].dt.tz_localize('UTC')\n",
    "            else:\n",
    "                df_pivot[\"_time\"] = df_pivot[\"_time\"].dt.tz_convert('UTC')\n",
    "\n",
    "            # Ensure the schema correctly reflects the data types of the columns\n",
    "            schema = get_parquet_schema(df_pivot, indicator_mapping, log)\n",
    "\n",
    "            table = pa.Table.from_pandas(df_pivot, schema=schema)\n",
    "            pq.write_table(table, output_file, compression='GZIP')\n",
    "            log.debug(f\"Stored parquet file in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"start upload\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
